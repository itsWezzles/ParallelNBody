
Due to MODULEPATH changes the following have been reloaded:
  1) mvapich2/1.9a2

[cli_32]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffc9845530) failed
PMPI_Comm_rank(66).: Null communicator

[cli_48]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff15d135a0) failed
PMPI_Comm_rank(66).: Null communicator

[c472-401.stampede.tacc.utexas.edu:mpispawn_2][readline] Unexpected End-Of-File on file descriptor 5. MPI process died?
[c472-401.stampede.tacc.utexas.edu:mpispawn_2][mtpmi_processops] Error while reading PMI socket. MPI process died?
[cli_64]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff5ea645d0) failed
PMPI_Comm_rank(66).: Null communicator

[c472-402.stampede.tacc.utexas.edu:mpispawn_3][readline] Unexpected End-Of-File on file descriptor 5. MPI process died?
[c472-402.stampede.tacc.utexas.edu:mpispawn_3][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c472-401.stampede.tacc.utexas.edu:mpispawn_2][child_handler] MPI process (rank: 32, pid: 105331) exited with status 1
[cli_16]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff4aa1f3b0) failed
PMPI_Comm_rank(66).: Null communicator

[c473-303.stampede.tacc.utexas.edu:mpispawn_4][readline] Unexpected End-Of-File on file descriptor 18. MPI process died?
[c473-303.stampede.tacc.utexas.edu:mpispawn_4][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c472-402.stampede.tacc.utexas.edu:mpispawn_3][child_handler] MPI process (rank: 48, pid: 95838) exited with status 1
[c471-503.stampede.tacc.utexas.edu:mpispawn_1][readline] Unexpected End-Of-File on file descriptor 17. MPI process died?
[c471-503.stampede.tacc.utexas.edu:mpispawn_1][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c473-303.stampede.tacc.utexas.edu:mpispawn_4][child_handler] MPI process (rank: 64, pid: 8578) exited with status 1
[c471-503.stampede.tacc.utexas.edu:mpispawn_1][child_handler] MPI process (rank: 16, pid: 25740) exited with status 1
[cli_128]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff12af3300) failed
PMPI_Comm_rank(66).: Null communicator

[c473-901.stampede.tacc.utexas.edu:mpispawn_8][readline] Unexpected End-Of-File on file descriptor 19. MPI process died?
[c473-901.stampede.tacc.utexas.edu:mpispawn_8][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c473-901.stampede.tacc.utexas.edu:mpispawn_8][child_handler] MPI process (rank: 128, pid: 12818) exited with status 1
[cli_192]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff594d8170) failed
PMPI_Comm_rank(66).: Null communicator

[c474-401.stampede.tacc.utexas.edu:mpispawn_12][readline] Unexpected End-Of-File on file descriptor 5. MPI process died?
[c474-401.stampede.tacc.utexas.edu:mpispawn_12][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c474-401.stampede.tacc.utexas.edu:mpispawn_12][child_handler] MPI process (rank: 192, pid: 49662) exited with status 1
[cli_256]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff37075350) failed
PMPI_Comm_rank(66).: Null communicator

[c482-404.stampede.tacc.utexas.edu:mpispawn_16][readline] Unexpected End-Of-File on file descriptor 19. MPI process died?
[c482-404.stampede.tacc.utexas.edu:mpispawn_16][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c482-404.stampede.tacc.utexas.edu:mpispawn_16][child_handler] MPI process (rank: 256, pid: 24659) exited with status 1
[cli_512]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff59bf2150) failed
PMPI_Comm_rank(66).: Null communicator

[c485-502.stampede.tacc.utexas.edu:mpispawn_32][readline] Unexpected End-Of-File on file descriptor 6. MPI process died?
[c485-502.stampede.tacc.utexas.edu:mpispawn_32][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c485-502.stampede.tacc.utexas.edu:mpispawn_32][child_handler] MPI process (rank: 512, pid: 6586) exited with status 1
[cli_768]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffbd993350) failed
PMPI_Comm_rank(66).: Null communicator

[c495-001.stampede.tacc.utexas.edu:mpispawn_48][readline] Unexpected End-Of-File on file descriptor 6. MPI process died?
[c495-001.stampede.tacc.utexas.edu:mpispawn_48][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c495-001.stampede.tacc.utexas.edu:mpispawn_48][child_handler] MPI process (rank: 768, pid: 62421) exited with status 1
[cli_6]: [cli_14]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffc4f92980) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff92bd6940) failed
PMPI_Comm_rank(66).: Null communicator

[cli_52]: [cli_50]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffb3413920) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffde09bed0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_18]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff3accdc10) failed
PMPI_Comm_rank(66).: Null communicator

[cli_26]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffed093eb0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_24]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff92e47f70) failed
PMPI_Comm_rank(66).: Null communicator

[cli_66]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff616b3bf0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_22]: [cli_20]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff64637cf0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff86c95b50) failed
PMPI_Comm_rank(66).: Null communicator

[cli_28]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffed347100) failed
PMPI_Comm_rank(66).: Null communicator

[cli_54]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffc0a42360) failed
PMPI_Comm_rank(66).: Null communicator

[cli_68]: [cli_70]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffe5a3a880) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff291b8df0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_74]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff810a09b0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_76]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff35a9cdb0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_58]: [cli_56]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff8c0b7bd0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_62]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff434b2970) failed
PMPI_Comm_rank(66).: Null communicator

[cli_60]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff7c68e9f0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff2b1aa470) failed
PMPI_Comm_rank(66).: Null communicator

[cli_30]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff00e2eb20) failed
PMPI_Comm_rank(66).: Null communicator

[cli_78]: [cli_72]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffae815a70) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffe11f0070) failed
PMPI_Comm_rank(66).: Null communicator

[cli_34]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7ffff63073f0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_42]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff11880db0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_44]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff1df2a840) failed
PMPI_Comm_rank(66).: Null communicator

[cli_36]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff3379c5a0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_38]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7ffff9a16b60) failed
PMPI_Comm_rank(66).: Null communicator

[cli_40]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffd83cbec0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_46]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7ffffcf65320) failed
PMPI_Comm_rank(66).: Null communicator

[cli_102]: [cli_104]: [cli_100]: [cli_106]: [cli_108]: [cli_98]: [cli_96]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffe5d29f90) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff1f9ac3d0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff87be4f20) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffa4b208d0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffa1f52d30) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffa5b08220) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff7114a1f0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_90]: [cli_88]: [cli_94]: [cli_92]: [cli_80]: [cli_86]: [cli_84]: [cli_82]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffe3ecca40) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffb72ef5e0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff93fdaf20) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffb72bcf00) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff13e02060) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7ffffafc5560) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff8aff96f0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffce5a31f0) failed
PMPI_Comm_rank(66).: Null communicator

[c468-601.stampede.tacc.utexas.edu:mpispawn_0][readline] Unexpected End-Of-File on file descriptor 11. MPI process died?
[c468-601.stampede.tacc.utexas.edu:mpispawn_0][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c473-401.stampede.tacc.utexas.edu:mpispawn_6][readline] Unexpected End-Of-File on file descriptor 11. MPI process died?
[c473-401.stampede.tacc.utexas.edu:mpispawn_6][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c473-304.stampede.tacc.utexas.edu:mpispawn_5][readline] Unexpected End-Of-File on file descriptor 9. MPI process died?
[c473-304.stampede.tacc.utexas.edu:mpispawn_5][mtpmi_processops] Error while reading PMI socket. MPI process died?
[cli_4]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff55858300) failed
PMPI_Comm_rank(66).: Null communicator

[c471-503.stampede.tacc.utexas.edu:mpispawn_1][child_handler] MPI process (rank: 18, pid: 25742) exited with status 1
[c473-303.stampede.tacc.utexas.edu:mpispawn_4][child_handler] MPI process (rank: 66, pid: 8580) exited with status 1
[c473-304.stampede.tacc.utexas.edu:mpispawn_5][child_handler] MPI process (rank: 88, pid: 8343) exited with status 1
[cli_12]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffcde57a60) failed
PMPI_Comm_rank(66).: Null communicator

[c473-401.stampede.tacc.utexas.edu:mpispawn_6][child_handler] MPI process (rank: 96, pid: 36156) exited with status 1
[c472-401.stampede.tacc.utexas.edu:mpispawn_2][child_handler] MPI process (rank: 44, pid: 105343) exited with status 1
[c471-503.stampede.tacc.utexas.edu:mpispawn_1][child_handler] MPI process (rank: 20, pid: 25744) exited with status 1
[cli_8]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffccb3e320) failed
PMPI_Comm_rank(66).: Null communicator

[c472-401.stampede.tacc.utexas.edu:mpispawn_2][child_handler] MPI process (rank: 34, pid: 105333) exited with status 1
[c473-303.stampede.tacc.utexas.edu:mpispawn_4][child_handler] MPI process (rank: 68, pid: 8582) exited with status 1
[c472-402.stampede.tacc.utexas.edu:mpispawn_3][child_handler] MPI process (rank: 58, pid: 95848) exited with status 1
[c472-402.stampede.tacc.utexas.edu:mpispawn_3][child_handler] MPI process (rank: 50, pid: 95840) exited with status 1
[c473-304.stampede.tacc.utexas.edu:mpispawn_5][child_handler] MPI process (rank: 80, pid: 8335) exited with status 1
[cli_2]: [c473-401.stampede.tacc.utexas.edu:mpispawn_6][child_handler] MPI process (rank: 98, pid: 36158) exited with status 1
[cli_10]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffc9fc0f00) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff48444140) failed
PMPI_Comm_rank(66).: Null communicator

[c471-503.stampede.tacc.utexas.edu:mpispawn_1][child_handler] MPI process (rank: 22, pid: 25746) exited with status 1
[c472-401.stampede.tacc.utexas.edu:mpispawn_2][child_handler] MPI process (rank: 36, pid: 105335) exited with status 1
[c473-303.stampede.tacc.utexas.edu:mpispawn_4][child_handler] MPI process (rank: 70, pid: 8584) exited with status 1
[c472-402.stampede.tacc.utexas.edu:mpispawn_3][child_handler] MPI process (rank: 52, pid: 95842) exited with status 1
[c473-304.stampede.tacc.utexas.edu:mpispawn_5][child_handler] MPI process (rank: 82, pid: 8337) exited with status 1
[c473-401.stampede.tacc.utexas.edu:mpispawn_6][child_handler] MPI process (rank: 100, pid: 36160) exited with status 1
[c468-601.stampede.tacc.utexas.edu:mpispawn_0][child_handler] MPI process (rank: 4, pid: 44418) exited with status 1
[c473-303.stampede.tacc.utexas.edu:mpispawn_4][child_handler] MPI process (rank: 72, pid: 8586) exited with status 1
[c468-601.stampede.tacc.utexas.edu:mpispawn_0][child_handler] MPI process (rank: 2, pid: 44416) exited with status 1
[c472-402.stampede.tacc.utexas.edu:mpispawn_3][child_handler] MPI process (rank: 54, pid: 95844) exited with status 1
[c473-304.stampede.tacc.utexas.edu:mpispawn_5][child_handler] MPI process (rank: 84, pid: 8339) exited with status 1
[c473-401.stampede.tacc.utexas.edu:mpispawn_6][child_handler] MPI process (rank: 102, pid: 36162) exited with status 1
[c471-503.stampede.tacc.utexas.edu:mpispawn_1][child_handler] MPI process (rank: 24, pid: 25748) exited with status 1
[c472-401.stampede.tacc.utexas.edu:mpispawn_2][child_handler] MPI process (rank: 38, pid: 105337) exited with status 1
[c471-503.stampede.tacc.utexas.edu:mpispawn_1][child_handler] MPI process (rank: 26, pid: 25750) exited with status 1
[c472-401.stampede.tacc.utexas.edu:mpispawn_2][child_handler] MPI process (rank: 40, pid: 105339) exited with status 1
[c468-601.stampede.tacc.utexas.edu:mpispawn_0][child_handler] MPI process (rank: 6, pid: 44420) exited with status 1
[c473-303.stampede.tacc.utexas.edu:mpispawn_4][child_handler] MPI process (rank: 74, pid: 8588) exited with status 1
[c472-402.stampede.tacc.utexas.edu:mpispawn_3][child_handler] MPI process (rank: 56, pid: 95846) exited with status 1
[c473-304.stampede.tacc.utexas.edu:mpispawn_5][child_handler] MPI process (rank: 86, pid: 8341) exited with status 1
[c473-401.stampede.tacc.utexas.edu:mpispawn_6][child_handler] MPI process (rank: 104, pid: 36164) exited with status 1
[cli_110]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff63236260) failed
PMPI_Comm_rank(66).: Null communicator

[cli_114]: [cli_120]: [cli_118]: [cli_116]: [cli_124]: [cli_122]: [cli_112]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7ffff948c950) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff7880ecb0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffd5e675b0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffe723ac70) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffaa0f2100) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff14dca8a0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_126]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff429d3380) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff9ebf6b30) failed
PMPI_Comm_rank(66).: Null communicator

[cli_130]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffd5781eb0) failed
PMPI_Comm_rank(66).: Null communicator

[c471-503.stampede.tacc.utexas.edu:mpispawn_1][child_handler] MPI process (rank: 28, pid: 25752) exited with status 1
[cli_132]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffe0643db0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_194]: [cli_196]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffa5cd1070) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff463f12f0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_198]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffcfa5fc10) failed
PMPI_Comm_rank(66).: Null communicator

[cli_258]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffe3ebc8f0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_260]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff74e4f430) failed
PMPI_Comm_rank(66).: Null communicator

[cli_152]: [cli_156]: [cli_158]: [cli_144]: [cli_154]: [cli_148]: [cli_146]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffae29ee90) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff92d9dda0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_150]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff309b2ec0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff29752330) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffa16525c0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff26a6e4c0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7ffff6bf4fc0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff6458cf30) failed
PMPI_Comm_rank(66).: Null communicator

[cli_262]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff1761d7f0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_268]: [cli_204]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff0ce465c0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_200]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7ffff75339a0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_202]: [cli_206]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffa6aaf9a0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff142143c0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_134]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff30202c20) failed
PMPI_Comm_rank(66).: Null communicator

[cli_270]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff9f9e5e70) failed
PMPI_Comm_rank(66).: Null communicator

[cli_266]: [cli_264]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff2b530940) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff82c32ad0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff626e8c40) failed
PMPI_Comm_rank(66).: Null communicator

[c472-401.stampede.tacc.utexas.edu:mpispawn_2][child_handler] MPI process (rank: 42, pid: 105341) exited with status 1
[cli_166]: [cli_164]: [cli_160]: [cli_162]: [cli_174]: [cli_170]: [cli_172]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff4260a7f0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff277bf0f0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_168]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffad376140) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffa2a72b30) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff43903ac0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff7c8cd4f0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff01616d40) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffcf073040) failed
PMPI_Comm_rank(66).: Null communicator

[cli_136]: [cli_186]: [cli_190]: [cli_178]: [cli_188]: [cli_182]: [cli_176]: [cli_180]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffb02440c0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff7d83de50) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff494d4ce0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff24b290c0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff0abcbdd0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffbb1d6ed0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7ffff9f49ee0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_184]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff875feb60) failed
PMPI_Comm_rank(66).: Null communicator

[cli_214]: [cli_210]: [cli_218]: [cli_208]: [cli_220]: [cli_216]: [cli_222]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7ffff4d9cda0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff81a7c5e0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffbacdcda0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff9c2000d0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffbcf68150) failed
PMPI_Comm_rank(66).: Null communicator

[cli_212]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffa2a839b0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffb94add30) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffb577c5f0) failed
PMPI_Comm_rank(66).: Null communicator

[c473-804.stampede.tacc.utexas.edu:mpispawn_7][readline] Unexpected End-Of-File on file descriptor 12. MPI process died?
[c473-804.stampede.tacc.utexas.edu:mpispawn_7][mtpmi_processops] Error while reading PMI socket. MPI process died?
[cli_238]: [cli_232]: [cli_234]: [cli_236]: [cli_224]: [cli_226]: [cli_230]: [cli_228]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffc7b61580) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffd09cdc00) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff22117900) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffb686b280) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff9d80c1a0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffa5930fe0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff83b68840) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffb0351f40) failed
PMPI_Comm_rank(66).: Null communicator

[cli_310]: [cli_306]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff423c27e0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffabda1670) failed
PMPI_Comm_rank(66).: Null communicator

[cli_316]: [cli_312]: [cli_314]: [cli_318]: [cli_308]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffd3d03370) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7ffffb297250) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff22570850) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffb6e799c0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffab879c00) failed
PMPI_Comm_rank(66).: Null communicator

[cli_304]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffd2120680) failed
PMPI_Comm_rank(66).: Null communicator

[cli_514]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff760e58b0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_330]: [cli_320]: [cli_332]: [cli_328]: [cli_322]: [cli_326]: [cli_324]: [cli_334]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffe9b0c360) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff82844360) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff35207750) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff92291660) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff3c6fc9e0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff64430d10) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff8a4a0bb0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff7bd083e0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_516]: [cli_518]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff06c48060) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffab3c48b0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_522]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffd5ae0d40) failed
PMPI_Comm_rank(66).: Null communicator

[cli_524]: [cli_350]: [cli_338]: [cli_336]: [cli_348]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff180c6c80) failed
PMPI_Comm_rank(66).: Null communicator

[cli_346]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffc549b2a0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_344]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff5b1c7930) failed
PMPI_Comm_rank(66).: Null communicator

[cli_342]: [cli_340]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffc50781c0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffc5b90ae0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffbc8abde0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff57b2c4e0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffef0854d0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff0cec1110) failed
PMPI_Comm_rank(66).: Null communicator

[cli_526]: [cli_382]: [cli_380]: [cli_374]: [cli_372]: [cli_370]: [cli_368]: [cli_376]: [cli_378]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffd8c83710) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff062de570) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffdc014070) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff35f3bca0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffab8f4540) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff875b22d0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff3717ddf0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff2fcc3660) failed
PMPI_Comm_rank(66).: Null communicator

[cli_520]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffa2160540) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffa8c02c60) failed
PMPI_Comm_rank(66).: Null communicator

[cli_410]: [cli_414]: [cli_408]: [cli_406]: [cli_400]: [cli_412]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffcc4c98b0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff18ea8dc0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_402]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff3ad69510) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff2b5fe690) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff7b9dbd50) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff416f0ab0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_404]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff8d468370) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff6a54f650) failed
PMPI_Comm_rank(66).: Null communicator

[c468-601.stampede.tacc.utexas.edu:mpispawn_0][child_handler] MPI process (rank: 8, pid: 44422) exited with status 1
[cli_426]: [cli_428]: [cli_430]: [cli_418]: [cli_424]: [cli_416]: [cli_422]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff407aa300) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff04e1fec0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_420]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff588188d0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff9c8ef5a0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff97123070) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff1231e2b0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff7ab22760) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff80fc1440) failed
PMPI_Comm_rank(66).: Null communicator

[cli_138]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff2c77dc00) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff64d9bd90) failed
PMPI_Comm_rank(66).: Null communicator

[cli_448]: [cli_460]: [cli_456]: [cli_452]: [cli_450]: [cli_462]: [cli_458]: [cli_454]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff918a58f0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffd580dcb0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffb6855da0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffb93b0760) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff56e1d6f0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff67075b60) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffa8d21da0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff6ec2f220) failed
PMPI_Comm_rank(66).: Null communicator

[cli_436]: [cli_432]: [cli_440]: [cli_438]: [cli_442]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff1afe93f0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_434]: [cli_446]: [cli_444]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff9b0edcb0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffc4f85300) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff61d90b10) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffe70e2e40) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff1b50dea0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffdb41ce60) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff9a9e8b10) failed
PMPI_Comm_rank(66).: Null communicator

[cli_140]: [cli_480]: [cli_486]: [cli_490]: [cli_484]: [cli_494]: [cli_488]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffe5d58950) failed
PMPI_Comm_rank(66).: Null communicator

[cli_492]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff4f434170) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff49c123a0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff400e8460) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffd8019c70) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffe814d2a0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_482]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff1b5597b0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff8b612050) failed
PMPI_Comm_rank(66).: Null communicator

[cli_498]: [cli_496]: [cli_510]: [cli_506]: [cli_504]: [cli_502]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff678b3870) failed
PMPI_Comm_rank(66).: Null communicator

[cli_508]: [cli_500]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffe518bea0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffa0528650) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffee660820) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffb77f75d0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff67bd7b90) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff8e16edd0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff9fcc5fe0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff5dbefdc0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_550]: [cli_558]: [cli_554]: [cli_552]: [cli_544]: [cli_546]: [cli_548]: [cli_556]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffc8f71c80) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffe7d39720) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff60838460) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7ffff19d4360) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff860895f0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffbbb8bba0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff9912b220) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffe9125c70) failed
PMPI_Comm_rank(66).: Null communicator

[cli_602]: [cli_604]: [cli_596]: [cli_592]: [cli_598]: [cli_606]: [cli_600]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff3b173460) failed
PMPI_Comm_rank(66).: Null communicator

[cli_594]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff33cf59e0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff94dba4a0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffe8306f00) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff012f5c10) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff67022a70) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff8bd43990) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffadab0660) failed
PMPI_Comm_rank(66).: Null communicator

[cli_0]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff334e26e0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_616]: [cli_618]: [cli_620]: [cli_610]: [cli_608]: [cli_614]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffdf50c0d0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_622]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff6e43a490) failed
PMPI_Comm_rank(66).: Null communicator

[cli_612]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffcfab5d90) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff38628130) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffb1245710) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff0a2bc930) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff1f0735f0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff2d8f9560) failed
PMPI_Comm_rank(66).: Null communicator

[c474-203.stampede.tacc.utexas.edu:mpispawn_9][readline] Unexpected End-Of-File on file descriptor 9. MPI process died?
[c474-203.stampede.tacc.utexas.edu:mpispawn_9][mtpmi_processops] Error while reading PMI socket. MPI process died?
[cli_652]: [cli_650]: [cli_654]: [cli_642]: [cli_646]: [cli_648]: [cli_640]: [cli_644]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7ffff102c150) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff9806adc0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffaa22d2c0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff4f66b040) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7ffffdb0b3f0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff193b6450) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffb97d3620) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff4cfda1e0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_142]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffb909cbb0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_666]: [cli_670]: [cli_660]: [cli_662]: [cli_664]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff1b0b60f0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_656]: [cli_658]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff9db9acf0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffce213080) failed
PMPI_Comm_rank(66).: Null communicator

[cli_668]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff62ce8760) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff6e36eef0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7ffff04099e0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff800c4930) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff4e0d1f40) failed
PMPI_Comm_rank(66).: Null communicator

[cli_566]: [cli_560]: [cli_562]: [cli_572]: [cli_574]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff418ee110) failed
PMPI_Comm_rank(66).: Null communicator

[cli_568]: [cli_570]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff7c18eae0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff694ce880) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7ffff381cf50) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffa75ffb50) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7ffff99053d0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff5929dc50) failed
PMPI_Comm_rank(66).: Null communicator

[cli_564]: [cli_694]: [cli_690]: [cli_698]: [cli_696]: [cli_700]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff5b75c5e0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7ffff6ddb7d0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_688]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff43f75380) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff1bb09d10) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffbe3deaf0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_702]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffb79ace30) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff247146f0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_692]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffa20b74c0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff2a1ffa90) failed
PMPI_Comm_rank(66).: Null communicator

[c474-204.stampede.tacc.utexas.edu:mpispawn_10][readline] Unexpected End-Of-File on file descriptor 9. MPI process died?
[c474-204.stampede.tacc.utexas.edu:mpispawn_10][mtpmi_processops] Error while reading PMI socket. MPI process died?
[cli_630]: [cli_626]: [cli_638]: [cli_632]: [cli_634]: [cli_636]: [cli_624]: [cli_628]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff45d43670) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff3dd2fe40) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff67a28cb0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff5d62f150) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff9d3bf930) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff0d440ca0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7ffff32e0ae0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff2b7b92e0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_674]: [cli_678]: [cli_684]: [cli_676]: [cli_686]: [cli_682]: [cli_680]: [cli_672]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffbbaad9e0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff97dcdca0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffae5aaaf0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffdb5d0ce0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff150e78a0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffad8729d0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffc5b148d0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffe90637b0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_590]: [cli_588]: [cli_586]: [cli_584]: [cli_576]: [cli_580]: [cli_578]: [cli_582]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffa1f34c00) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff1de12190) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff787cecc0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffdde4df10) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff78ddcdd0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffda29e520) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffaf629760) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff471979a0) failed
PMPI_Comm_rank(66).: Null communicator

[c474-403.stampede.tacc.utexas.edu:mpispawn_13][readline] Unexpected End-Of-File on file descriptor 8. MPI process died?
[c474-403.stampede.tacc.utexas.edu:mpispawn_13][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c474-404.stampede.tacc.utexas.edu:mpispawn_14][readline] Unexpected End-Of-File on file descriptor 10. MPI process died?
[c474-404.stampede.tacc.utexas.edu:mpispawn_14][mtpmi_processops] Error while reading PMI socket. MPI process died?
[cli_534]: [cli_528]: [cli_538]: [cli_536]: [cli_530]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff13133f60) failed
PMPI_Comm_rank(66).: Null communicator

[cli_542]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffb74dbcb0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff1464f330) failed
PMPI_Comm_rank(66).: Null communicator

[cli_532]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff5c69eb50) failed
PMPI_Comm_rank(66).: Null communicator

[cli_540]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff438c17e0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7ffffc1481f0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff9c537da0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffd23a0e50) failed
PMPI_Comm_rank(66).: Null communicator

[cli_476]: [cli_474]: [cli_472]: [cli_468]: [cli_466]: [cli_478]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff93c5b1e0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_470]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffdce3c6c0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_464]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff63f9d960) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffd28cde50) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff7c9b8f20) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff2d927330) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7ffff78e1970) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff6de870d0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_772]: [cli_770]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff4eedc5f0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffc9cf9910) failed
PMPI_Comm_rank(66).: Null communicator

[cli_394]: [cli_392]: [cli_386]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffdd44e4d0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_398]: [cli_396]: [cli_384]: [cli_390]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff5e387120) failed
PMPI_Comm_rank(66).: Null communicator

[cli_388]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffc1b47a70) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff906b4390) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff81158440) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7ffff25ba9b0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7ffff7e614a0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff165612a0) failed
PMPI_Comm_rank(66).: Null communicator

[c483-101.stampede.tacc.utexas.edu:mpispawn_21][readline] Unexpected End-Of-File on file descriptor 10. MPI process died?
[c483-101.stampede.tacc.utexas.edu:mpispawn_21][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c483-601.stampede.tacc.utexas.edu:mpispawn_23][readline] Unexpected End-Of-File on file descriptor 20. MPI process died?
[c483-601.stampede.tacc.utexas.edu:mpispawn_23][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c483-003.stampede.tacc.utexas.edu:mpispawn_19][readline] Unexpected End-Of-File on file descriptor 13. MPI process died?
[c483-003.stampede.tacc.utexas.edu:mpispawn_19][mtpmi_processops] Error while reading PMI socket. MPI process died?
[cli_366]: [cli_364]: [cli_360]: [cli_362]: [cli_354]: [cli_352]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff00357300) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff4bb1a7f0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_356]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff192b9670) failed
PMPI_Comm_rank(66).: Null communicator

[cli_358]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffca38ebf0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffc2a7bb00) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff3c6d92e0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff9dff7a60) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff25176490) failed
PMPI_Comm_rank(66).: Null communicator

[c483-504.stampede.tacc.utexas.edu:mpispawn_22][readline] Unexpected End-Of-File on file descriptor 18. MPI process died?
[c483-504.stampede.tacc.utexas.edu:mpispawn_22][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c474-301.stampede.tacc.utexas.edu:mpispawn_11][readline] Unexpected End-Of-File on file descriptor 10. MPI process died?
[c474-301.stampede.tacc.utexas.edu:mpispawn_11][mtpmi_processops] Error while reading PMI socket. MPI process died?
[cli_778]: [cli_774]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff0e4f1850) failed
PMPI_Comm_rank(66).: Null communicator

[cli_780]: [c473-303.stampede.tacc.utexas.edu:mpispawn_4][child_handler] MPI process (rank: 76, pid: 8590) exited with status 1
aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffd18d70e0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_782]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7ffff4cfad40) failed
PMPI_Comm_rank(66).: Null communicator

[cli_776]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff91271c90) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff2ad4d1c0) failed
PMPI_Comm_rank(66).: Null communicator

[c483-804.stampede.tacc.utexas.edu:mpispawn_24][readline] Unexpected End-Of-File on file descriptor 19. MPI process died?
[c483-804.stampede.tacc.utexas.edu:mpispawn_24][mtpmi_processops] Error while reading PMI socket. MPI process died?
[cli_730]: [cli_734]: [cli_732]: [cli_722]: [cli_728]: [cli_724]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff9e74cda0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff49b08c60) failed
PMPI_Comm_rank(66).: Null communicator

[cli_726]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffb2ea03f0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_720]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffa93f23a0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff31e3a080) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffa808e890) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff5809bd00) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff9be244e0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_300]: [cli_288]: [cli_296]: [cli_290]: [cli_292]: [cli_298]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffa1c05ca0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffb946cbf0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff9fee2040) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff544cb970) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff9ce68490) failed
PMPI_Comm_rank(66).: Null communicator

[cli_294]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffd89d2370) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7ffff613d700) failed
PMPI_Comm_rank(66).: Null communicator

[cli_302]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffe56ce1e0) failed
PMPI_Comm_rank(66).: Null communicator

[c483-002.stampede.tacc.utexas.edu:mpispawn_18][readline] Unexpected End-Of-File on file descriptor 13. MPI process died?
[c483-002.stampede.tacc.utexas.edu:mpispawn_18][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c484-101.stampede.tacc.utexas.edu:mpispawn_26][readline] Unexpected End-Of-File on file descriptor 6. MPI process died?
[c484-101.stampede.tacc.utexas.edu:mpispawn_26][mtpmi_processops] Error while reading PMI socket. MPI process died?
[cli_286]: [cli_280]: [cli_282]: [cli_272]: [cli_278]: [cli_276]: [cli_274]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff7d964350) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffbfc33110) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff7c9f4890) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffba918c40) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff0d2807d0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_284]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7ffff2ef2b00) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffd835cd40) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff08b759d0) failed
PMPI_Comm_rank(66).: Null communicator

[c483-001.stampede.tacc.utexas.edu:mpispawn_17][readline] Unexpected End-Of-File on file descriptor 15. MPI process died?
[c483-001.stampede.tacc.utexas.edu:mpispawn_17][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c483-004.stampede.tacc.utexas.edu:mpispawn_20][readline] Unexpected End-Of-File on file descriptor 6. MPI process died?
[c483-004.stampede.tacc.utexas.edu:mpispawn_20][mtpmi_processops] Error while reading PMI socket. MPI process died?
[cli_744]: [cli_748]: [cli_750]: [cli_740]: [cli_738]: [cli_742]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffa4ba70e0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff382f32d0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_746]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff4c42e2d0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff27643400) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff35efe110) failed
PMPI_Comm_rank(66).: Null communicator

[cli_736]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7ffffb6a7050) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffb43e9ae0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff4f5a25d0) failed
PMPI_Comm_rank(66).: Null communicator

[c484-602.stampede.tacc.utexas.edu:mpispawn_30][readline] Unexpected End-Of-File on file descriptor 11. MPI process died?
[c484-602.stampede.tacc.utexas.edu:mpispawn_30][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c472-402.stampede.tacc.utexas.edu:mpispawn_3][child_handler] MPI process (rank: 60, pid: 95850) exited with status 1
[cli_254]: [cli_246]: [cli_244]: [cli_240]: [cli_242]: [cli_248]: [cli_252]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffee392cd0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_250]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff5d1fe0b0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff5a83d270) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffae24c700) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff1c13cbb0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffbd5d0ce0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffea7728e0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff1f1afb10) failed
PMPI_Comm_rank(66).: Null communicator

[c478-303.stampede.tacc.utexas.edu:mpispawn_15][readline] Unexpected End-Of-File on file descriptor 17. MPI process died?
[c478-303.stampede.tacc.utexas.edu:mpispawn_15][mtpmi_processops] Error while reading PMI socket. MPI process died?
[cli_788]: [cli_784]: [cli_798]: [cli_792]: [cli_790]: [cli_796]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff62776300) failed
PMPI_Comm_rank(66).: Null communicator

[cli_794]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff8c48c3e0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff51f338e0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff7f4e1d20) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff203406a0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff5b8419d0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff18fcb9c0) failed
PMPI_Comm_rank(66).: Null communicator

[c484-104.stampede.tacc.utexas.edu:mpispawn_28][readline] Unexpected End-Of-File on file descriptor 15. MPI process died?
[c484-104.stampede.tacc.utexas.edu:mpispawn_28][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c484-201.stampede.tacc.utexas.edu:mpispawn_29][readline] Unexpected End-Of-File on file descriptor 8. MPI process died?
[c484-201.stampede.tacc.utexas.edu:mpispawn_29][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c484-701.stampede.tacc.utexas.edu:mpispawn_31][readline] Unexpected End-Of-File on file descriptor 15. MPI process died?
[c484-701.stampede.tacc.utexas.edu:mpispawn_31][mtpmi_processops] Error while reading PMI socket. MPI process died?
[cli_842]: [cli_844]: [cli_832]: [cli_834]: [cli_840]: [cli_846]: [cli_838]: [cli_836]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff2081e680) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff47dd5cf0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff8819f570) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffed0c0f20) failed
PMPI_Comm_rank(66).: Null communicator

[cli_826]: [cli_820]: [cli_822]: [cli_824]: [cli_830]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff3151b260) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff665389f0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff1790f450) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff9736c620) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff9fc15220) failed
PMPI_Comm_rank(66).: Null communicator

[cli_816]: [cli_818]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffb8a53eb0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffb53e5690) failed
PMPI_Comm_rank(66).: Null communicator

[cli_828]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff4dbc7a30) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff54ef4250) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffea99f880) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffce6474b0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff9c5a20a0) failed
PMPI_Comm_rank(66).: Null communicator

[c491-004.stampede.tacc.utexas.edu:mpispawn_36][readline] Unexpected End-Of-File on file descriptor 19. MPI process died?
[c491-004.stampede.tacc.utexas.edu:mpispawn_36][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c484-103.stampede.tacc.utexas.edu:mpispawn_27][readline] Unexpected End-Of-File on file descriptor 15. MPI process died?
[c484-103.stampede.tacc.utexas.edu:mpispawn_27][mtpmi_processops] Error while reading PMI socket. MPI process died?
[cli_814]: [cli_812]: [cli_810]: [cli_800]: [cli_804]: [cli_806]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff696a2a90) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffabeb9e70) failed
PMPI_Comm_rank(66).: Null communicator

[cli_802]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffb175c180) failed
PMPI_Comm_rank(66).: Null communicator

[cli_808]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff8dfb0ba0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff81465aa0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff9947cee0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff0ae6aab0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffcc941c80) failed
PMPI_Comm_rank(66).: Null communicator

[cli_904]: [cli_908]: [cli_902]: [cli_896]: [cli_910]: [cli_906]: [cli_900]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff374b94b0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff0fcfbfb0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff93f25610) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff1ac859b0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff68e29860) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffe98267b0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffd7cd4f70) failed
PMPI_Comm_rank(66).: Null communicator

[cli_894]: [cli_892]: [cli_890]: [cli_888]: [cli_886]: [cli_880]: [cli_884]: [cli_882]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff52a1ec30) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff61c349b0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff57acf000) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffff378cf0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff317d79a0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff85059100) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff13a34620) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff55aa6420) failed
PMPI_Comm_rank(66).: Null communicator

[c483-901.stampede.tacc.utexas.edu:mpispawn_25][readline] Unexpected End-Of-File on file descriptor 8. MPI process died?
[c483-901.stampede.tacc.utexas.edu:mpispawn_25][mtpmi_processops] Error while reading PMI socket. MPI process died?
[cli_944]: [cli_952]: [cli_958]: [cli_946]: [cli_954]: [cli_950]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffeca89940) failed
PMPI_Comm_rank(66).: Null communicator

[cli_948]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffd70a5a30) failed
PMPI_Comm_rank(66).: Null communicator

[cli_922]: [cli_920]: [cli_924]: [cli_914]: [cli_916]: [cli_918]: [cli_926]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff491464b0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff580ed310) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff1c011a90) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff84ec4900) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff6af564f0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffa6e7ed30) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7ffffa3613e0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_940]: [cli_938]: [cli_932]: [cli_928]: [cli_936]: [cli_934]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffde7cf890) failed
PMPI_Comm_rank(66).: Null communicator

[cli_942]: [cli_930]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffa0dca560) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffa4c9e240) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffb2d3f950) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff4a3c7280) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffd9d19ff0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff20be3b80) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffe32fddb0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff15a8c0b0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffe8ef7140) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff1a886eb0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_956]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffe310ff60) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff0ddb2ba0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffb369e520) failed
PMPI_Comm_rank(66).: Null communicator

[cli_786]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff5625cd10) failed
PMPI_Comm_rank(66).: Null communicator

[c492-701.stampede.tacc.utexas.edu:mpispawn_37][readline] Unexpected End-Of-File on file descriptor 9. MPI process died?
[c492-701.stampede.tacc.utexas.edu:mpispawn_37][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c493-102.stampede.tacc.utexas.edu:mpispawn_39][readline] Unexpected End-Of-File on file descriptor 17. MPI process died?
[c493-102.stampede.tacc.utexas.edu:mpispawn_39][mtpmi_processops] Error while reading PMI socket. MPI process died?
[cli_860]: [cli_856]: [cli_862]: [cli_858]: [cli_848]: [cli_854]: [cli_852]: [cli_850]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff63eab4e0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7ffffdbb9280) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffceccb830) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7ffffdd8b270) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff621a4a80) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7ffff7cf2e00) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffa74ae6d0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff500ff5d0) failed
PMPI_Comm_rank(66).: Null communicator

[c486-102.stampede.tacc.utexas.edu:mpispawn_33][readline] Unexpected End-Of-File on file descriptor 16. MPI process died?
[c486-102.stampede.tacc.utexas.edu:mpispawn_33][mtpmi_processops] Error while reading PMI socket. MPI process died?
[cli_986]: [cli_990]: [cli_984]: [cli_988]: [cli_976]: [cli_978]: [cli_980]: [cli_982]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff42490af0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff732a7550) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff1869c830) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff494237e0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7ffffc1b1690) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffa5b3d100) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7ffff31113f0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff5dc5e430) failed
PMPI_Comm_rank(66).: Null communicator

[cli_912]: [c490-302.stampede.tacc.utexas.edu:mpispawn_34][readline] Unexpected End-Of-File on file descriptor 11. MPI process died?
[c490-302.stampede.tacc.utexas.edu:mpispawn_34][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c494-402.stampede.tacc.utexas.edu:mpispawn_42][readline] Unexpected End-Of-File on file descriptor 15. MPI process died?
[c494-402.stampede.tacc.utexas.edu:mpispawn_42][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c494-101.stampede.tacc.utexas.edu:mpispawn_41][readline] Unexpected End-Of-File on file descriptor 16. MPI process died?
[c494-101.stampede.tacc.utexas.edu:mpispawn_41][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c490-904.stampede.tacc.utexas.edu:mpispawn_35][readline] Unexpected End-Of-File on file descriptor 5. MPI process died?
[c490-904.stampede.tacc.utexas.edu:mpispawn_35][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c494-403.stampede.tacc.utexas.edu:mpispawn_43][readline] Unexpected End-Of-File on file descriptor 14. MPI process died?
[c494-403.stampede.tacc.utexas.edu:mpispawn_43][mtpmi_processops] Error while reading PMI socket. MPI process died?
[cli_960]: [cli_966]: [cli_974]: [cli_962]: [cli_964]: [cli_972]: [cli_970]: [cli_968]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff6e2c4720) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffbccc9e40) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7ffff73974d0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff16f6f100) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffbecd5fe0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff1bfa7a40) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7ffffec51670) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff0534c620) failed
PMPI_Comm_rank(66).: Null communicator

[cli_898]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff5d7ef750) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff74e57a80) failed
PMPI_Comm_rank(66).: Null communicator

[cli_1018]: [cli_1022]: [cli_1016]: [cli_1020]: [cli_1008]: [cli_1012]: [cli_1010]: [cli_1014]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff20ff1a00) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff6a6d8010) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7ffffc8f3380) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffd59f9040) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff54c8a560) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7ffff7f22020) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffedd93760) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff489e73d0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_1004]: [cli_998]: [cli_992]: [cli_1000]: [cli_996]: [cli_1002]: [cli_1006]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff10c8bfa0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff7ed3a460) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffdf3edf00) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffa355ed20) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff5e183d60) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff8d20dbf0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffe725f500) failed
PMPI_Comm_rank(66).: Null communicator

[c493-001.stampede.tacc.utexas.edu:mpispawn_38][readline] Unexpected End-Of-File on file descriptor 7. MPI process died?
[c493-001.stampede.tacc.utexas.edu:mpispawn_38][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c493-201.stampede.tacc.utexas.edu:mpispawn_40][readline] Unexpected End-Of-File on file descriptor 15. MPI process died?
[c493-201.stampede.tacc.utexas.edu:mpispawn_40][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c473-304.stampede.tacc.utexas.edu:mpispawn_5][child_handler] MPI process (rank: 90, pid: 8345) exited with status 1
[cli_874]: [cli_872]: [cli_878]: [cli_868]: [cli_864]: [cli_866]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff13e7fcb0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffa2fd9dc0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_876]: [cli_870]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffc4f3e230) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff50c26370) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffb993f220) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7ffffc1eca70) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff3174fc90) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff2e3e1f40) failed
PMPI_Comm_rank(66).: Null communicator

[cli_994]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7ffffbb24a20) failed
PMPI_Comm_rank(66).: Null communicator

[c494-702.stampede.tacc.utexas.edu:mpispawn_45][readline] Unexpected End-Of-File on file descriptor 15. MPI process died?
[c494-702.stampede.tacc.utexas.edu:mpispawn_45][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c473-401.stampede.tacc.utexas.edu:mpispawn_6][child_handler] MPI process (rank: 106, pid: 36166) exited with status 1
[c494-703.stampede.tacc.utexas.edu:mpispawn_46][readline] Unexpected End-Of-File on file descriptor 9. MPI process died?
[c494-703.stampede.tacc.utexas.edu:mpispawn_46][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c497-202.stampede.tacc.utexas.edu:mpispawn_52][readline] Unexpected End-Of-File on file descriptor 15. MPI process died?
[c497-202.stampede.tacc.utexas.edu:mpispawn_52][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c497-102.stampede.tacc.utexas.edu:mpispawn_51][readline] Unexpected End-Of-File on file descriptor 13. MPI process died?
[c497-102.stampede.tacc.utexas.edu:mpispawn_51][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c497-602.stampede.tacc.utexas.edu:mpispawn_55][readline] Unexpected End-Of-File on file descriptor 15. MPI process died?
[c497-602.stampede.tacc.utexas.edu:mpispawn_55][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c497-603.stampede.tacc.utexas.edu:mpispawn_56][readline] Unexpected End-Of-File on file descriptor 16. MPI process died?
[c497-603.stampede.tacc.utexas.edu:mpispawn_56][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c497-601.stampede.tacc.utexas.edu:mpispawn_54][readline] Unexpected End-Of-File on file descriptor 8. MPI process died?
[c497-601.stampede.tacc.utexas.edu:mpispawn_54][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c498-402.stampede.tacc.utexas.edu:mpispawn_58][readline] Unexpected End-Of-File on file descriptor 14. MPI process died?
[c498-402.stampede.tacc.utexas.edu:mpispawn_58][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c501-301.stampede.tacc.utexas.edu:mpispawn_61][readline] Unexpected End-Of-File on file descriptor 10. MPI process died?
[c501-301.stampede.tacc.utexas.edu:mpispawn_61][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c498-304.stampede.tacc.utexas.edu:mpispawn_57][readline] Unexpected End-Of-File on file descriptor 15. MPI process died?
[c498-304.stampede.tacc.utexas.edu:mpispawn_57][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c501-801.stampede.tacc.utexas.edu:mpispawn_63][readline] Unexpected End-Of-File on file descriptor 16. MPI process died?
[c501-801.stampede.tacc.utexas.edu:mpispawn_63][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c495-002.stampede.tacc.utexas.edu:mpispawn_49][readline] Unexpected End-Of-File on file descriptor 14. MPI process died?
[c495-002.stampede.tacc.utexas.edu:mpispawn_49][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c497-504.stampede.tacc.utexas.edu:mpispawn_53][readline] Unexpected End-Of-File on file descriptor 11. MPI process died?
[c497-504.stampede.tacc.utexas.edu:mpispawn_53][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c471-503.stampede.tacc.utexas.edu:mpispawn_1][child_handler] MPI process (rank: 30, pid: 25754) exited with status 1
[c472-401.stampede.tacc.utexas.edu:mpispawn_2][child_handler] MPI process (rank: 46, pid: 105345) exited with status 1
[c473-303.stampede.tacc.utexas.edu:mpispawn_4][child_handler] MPI process (rank: 78, pid: 8592) exited with status 1
[c473-804.stampede.tacc.utexas.edu:mpispawn_7][child_handler] MPI process (rank: 126, pid: 28230) exited with status 1
[c468-601.stampede.tacc.utexas.edu:mpispawn_0][child_handler] MPI process (rank: 10, pid: 44424) exited with status 1
[c483-504.stampede.tacc.utexas.edu:mpispawn_22][child_handler] MPI process (rank: 356, pid: 12099) exited with status 1
[c495-201.stampede.tacc.utexas.edu:mpispawn_50][readline] Unexpected End-Of-File on file descriptor 12. MPI process died?
[c495-201.stampede.tacc.utexas.edu:mpispawn_50][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c498-603.stampede.tacc.utexas.edu:mpispawn_59][readline] Unexpected End-Of-File on file descriptor 14. MPI process died?
[c498-603.stampede.tacc.utexas.edu:mpispawn_59][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c501-204.stampede.tacc.utexas.edu:mpispawn_60][readline] Unexpected End-Of-File on file descriptor 20. MPI process died?
[c501-204.stampede.tacc.utexas.edu:mpispawn_60][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c501-604.stampede.tacc.utexas.edu:mpispawn_62][readline] Unexpected End-Of-File on file descriptor 9. MPI process died?
[c501-604.stampede.tacc.utexas.edu:mpispawn_62][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c474-204.stampede.tacc.utexas.edu:mpispawn_10][child_handler] MPI process (rank: 164, pid: 9309) exited with status 1
[c472-402.stampede.tacc.utexas.edu:mpispawn_3][child_handler] MPI process (rank: 62, pid: 95852) exited with status 1
[c491-004.stampede.tacc.utexas.edu:mpispawn_36][child_handler] MPI process (rank: 580, pid: 116483) exited with status 1
[c483-101.stampede.tacc.utexas.edu:mpispawn_21][child_handler] MPI process (rank: 342, pid: 80025) exited with status 1
[c492-701.stampede.tacc.utexas.edu:mpispawn_37][child_handler] MPI process (rank: 596, pid: 118548) exited with status 1
[c483-601.stampede.tacc.utexas.edu:mpispawn_23][child_handler] MPI process (rank: 372, pid: 15079) exited with status 1
[c483-001.stampede.tacc.utexas.edu:mpispawn_17][child_handler] MPI process (rank: 280, pid: 18909) exited with status 1
[c493-201.stampede.tacc.utexas.edu:mpispawn_40][child_handler] MPI process (rank: 642, pid: 58508) exited with status 1
[c473-304.stampede.tacc.utexas.edu:mpispawn_5][child_handler] MPI process (rank: 92, pid: 8347) exited with status 1
[c474-203.stampede.tacc.utexas.edu:mpispawn_9][child_handler] MPI process (rank: 154, pid: 40945) exited with status 1
[c490-904.stampede.tacc.utexas.edu:mpispawn_35][child_handler] MPI process (rank: 572, pid: 35366) exited with status 1
[c478-303.stampede.tacc.utexas.edu:mpispawn_15][child_handler] MPI process (rank: 252, pid: 26675) exited with status 1
[c483-804.stampede.tacc.utexas.edu:mpispawn_24][child_handler] MPI process (rank: 386, pid: 32920) exited with status 1
[c486-102.stampede.tacc.utexas.edu:mpispawn_33][child_handler] MPI process (rank: 542, pid: 2022) exited with status 1
[c484-103.stampede.tacc.utexas.edu:mpispawn_27][child_handler] MPI process (rank: 438, pid: 35485) exited with status 1
[c495-001.stampede.tacc.utexas.edu:mpispawn_48][child_handler] MPI process (rank: 772, pid: 62425) exited with status 1
[c493-001.stampede.tacc.utexas.edu:mpispawn_38][child_handler] MPI process (rank: 610, pid: 23511) exited with status 1
[c473-401.stampede.tacc.utexas.edu:mpispawn_6][child_handler] MPI process (rank: 108, pid: 36168) exited with status 1
[c490-302.stampede.tacc.utexas.edu:mpispawn_34][child_handler] MPI process (rank: 556, pid: 56706) exited with status 1
[c494-403.stampede.tacc.utexas.edu:mpispawn_43][child_handler] MPI process (rank: 690, pid: 58294) exited with status 1
[c484-602.stampede.tacc.utexas.edu:mpispawn_30][child_handler] MPI process (rank: 492, pid: 99904) exited with status 1
[c474-404.stampede.tacc.utexas.edu:mpispawn_14][child_handler] MPI process (rank: 234, pid: 19421) exited with status 1
[c474-403.stampede.tacc.utexas.edu:mpispawn_13][child_handler] MPI process (rank: 210, pid: 101186) exited with status 1
[c483-002.stampede.tacc.utexas.edu:mpispawn_18][child_handler] MPI process (rank: 300, pid: 103817) exited with status 1
[c474-301.stampede.tacc.utexas.edu:mpispawn_11][child_handler] MPI process (rank: 190, pid: 12535) exited with status 1
[c485-502.stampede.tacc.utexas.edu:mpispawn_32][child_handler] MPI process (rank: 524, pid: 6598) exited with status 1
[c483-003.stampede.tacc.utexas.edu:mpispawn_19][child_handler] MPI process (rank: 306, pid: 99029) exited with status 1
[c493-102.stampede.tacc.utexas.edu:mpispawn_39][child_handler] MPI process (rank: 638, pid: 73256) exited with status 1
[c494-402.stampede.tacc.utexas.edu:mpispawn_42][child_handler] MPI process (rank: 680, pid: 77275) exited with status 1
[c484-701.stampede.tacc.utexas.edu:mpispawn_31][child_handler] MPI process (rank: 498, pid: 42136) exited with status 1
[c484-201.stampede.tacc.utexas.edu:mpispawn_29][child_handler] MPI process (rank: 478, pid: 90395) exited with status 1
[c497-504.stampede.tacc.utexas.edu:mpispawn_53][child_handler] MPI process (rank: 854, pid: 37836) exited with status 1
[c483-901.stampede.tacc.utexas.edu:mpispawn_25][child_handler] MPI process (rank: 412, pid: 14337) exited with status 1
[c497-603.stampede.tacc.utexas.edu:mpispawn_56][child_handler] MPI process (rank: 910, pid: 20662) exited with status 1
[c501-301.stampede.tacc.utexas.edu:mpispawn_61][child_handler] MPI process (rank: 982, pid: 55067) exited with status 1
[c484-101.stampede.tacc.utexas.edu:mpispawn_26][child_handler] MPI process (rank: 430, pid: 24639) exited with status 1
[c498-304.stampede.tacc.utexas.edu:mpispawn_57][child_handler] MPI process (rank: 924, pid: 36371) exited with status 1
[c484-104.stampede.tacc.utexas.edu:mpispawn_28][child_handler] MPI process (rank: 460, pid: 87890) exited with status 1
[c494-101.stampede.tacc.utexas.edu:mpispawn_41][child_handler] MPI process (rank: 664, pid: 15718) exited with status 1
[c501-204.stampede.tacc.utexas.edu:mpispawn_60][child_handler] MPI process (rank: 974, pid: 75676) exited with status 1
[c495-201.stampede.tacc.utexas.edu:mpispawn_50][child_handler] MPI process (rank: 812, pid: 34106) exited with status 1
[c497-102.stampede.tacc.utexas.edu:mpispawn_51][child_handler] MPI process (rank: 828, pid: 18570) exited with status 1
[c483-004.stampede.tacc.utexas.edu:mpispawn_20][child_handler] MPI process (rank: 330, pid: 9750) exited with status 1
[c494-702.stampede.tacc.utexas.edu:mpispawn_45][child_handler] MPI process (rank: 730, pid: 56982) exited with status 1
[c498-603.stampede.tacc.utexas.edu:mpispawn_59][child_handler] MPI process (rank: 958, pid: 116501) exited with status 1
[c501-604.stampede.tacc.utexas.edu:mpispawn_62][child_handler] MPI process (rank: 1004, pid: 36461) exited with status 1
[c497-601.stampede.tacc.utexas.edu:mpispawn_54][child_handler] MPI process (rank: 878, pid: 85272) exited with status 1
[c501-801.stampede.tacc.utexas.edu:mpispawn_63][child_handler] MPI process (rank: 1022, pid: 61231) exited with status 1
[c495-002.stampede.tacc.utexas.edu:mpispawn_49][child_handler] MPI process (rank: 792, pid: 123830) exited with status 1
[c468-601.stampede.tacc.utexas.edu:mpispawn_0][child_handler] MPI process (rank: 12, pid: 44426) exited with status 1
[c473-804.stampede.tacc.utexas.edu:mpispawn_7][child_handler] MPI process (rank: 112, pid: 28216) exited with status 1
[c497-602.stampede.tacc.utexas.edu:mpispawn_55][child_handler] MPI process (rank: 880, pid: 83444) exited with status 1
[c482-404.stampede.tacc.utexas.edu:mpispawn_16][child_handler] MPI process (rank: 266, pid: 24669) exited with status 1
[c473-901.stampede.tacc.utexas.edu:mpispawn_8][child_handler] MPI process (rank: 140, pid: 12830) exited with status 1
[c474-401.stampede.tacc.utexas.edu:mpispawn_12][child_handler] MPI process (rank: 204, pid: 49674) exited with status 1
[c483-504.stampede.tacc.utexas.edu:mpispawn_22][child_handler] MPI process (rank: 354, pid: 12097) exited with status 1
[c474-401.stampede.tacc.utexas.edu:mpispawn_12][child_handler] MPI process (rank: 194, pid: 49664) exited with status 1
[c474-204.stampede.tacc.utexas.edu:mpispawn_10][child_handler] MPI process (rank: 162, pid: 9307) exited with status 1
[c491-004.stampede.tacc.utexas.edu:mpispawn_36][child_handler] MPI process (rank: 578, pid: 116481) exited with status 1
[cli_764]: [cli_762]: [cli_760]: [cli_756]: [cli_758]: [cli_766]: [cli_752]: [cli_754]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffc1fc2e50) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff990fd780) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffe305dad0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff4cd0b570) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff6fc03330) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff58d47910) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7ffffb121d50) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff94589710) failed
PMPI_Comm_rank(66).: Null communicator

[c483-101.stampede.tacc.utexas.edu:mpispawn_21][child_handler] MPI process (rank: 336, pid: 80019) exited with status 1
[c498-402.stampede.tacc.utexas.edu:mpispawn_58][child_handler] MPI process (rank: 928, pid: 15996) exited with status 1
[c492-701.stampede.tacc.utexas.edu:mpispawn_37][child_handler] MPI process (rank: 594, pid: 118546) exited with status 1
[c473-901.stampede.tacc.utexas.edu:mpispawn_8][child_handler] MPI process (rank: 130, pid: 12820) exited with status 1
[c483-601.stampede.tacc.utexas.edu:mpispawn_23][child_handler] MPI process (rank: 368, pid: 15075) exited with status 1
[c493-201.stampede.tacc.utexas.edu:mpispawn_40][child_handler] MPI process (rank: 644, pid: 58510) exited with status 1
[c483-001.stampede.tacc.utexas.edu:mpispawn_17][child_handler] MPI process (rank: 272, pid: 18901) exited with status 1
[c473-304.stampede.tacc.utexas.edu:mpispawn_5][child_handler] MPI process (rank: 94, pid: 8349) exited with status 1
[c490-904.stampede.tacc.utexas.edu:mpispawn_35][child_handler] MPI process (rank: 562, pid: 35356) exited with status 1
[c474-203.stampede.tacc.utexas.edu:mpispawn_9][child_handler] MPI process (rank: 144, pid: 40935) exited with status 1
[c478-303.stampede.tacc.utexas.edu:mpispawn_15][child_handler] MPI process (rank: 242, pid: 26665) exited with status 1
[c483-804.stampede.tacc.utexas.edu:mpispawn_24][child_handler] MPI process (rank: 384, pid: 32918) exited with status 1
[c484-103.stampede.tacc.utexas.edu:mpispawn_27][child_handler] MPI process (rank: 432, pid: 35479) exited with status 1
[c495-001.stampede.tacc.utexas.edu:mpispawn_48][child_handler] MPI process (rank: 770, pid: 62423) exited with status 1
[c473-401.stampede.tacc.utexas.edu:mpispawn_6][child_handler] MPI process (rank: 110, pid: 36170) exited with status 1
[c486-102.stampede.tacc.utexas.edu:mpispawn_33][child_handler] MPI process (rank: 528, pid: 2006) exited with status 1
[c493-001.stampede.tacc.utexas.edu:mpispawn_38][child_handler] MPI process (rank: 608, pid: 23509) exited with status 1
[c490-302.stampede.tacc.utexas.edu:mpispawn_34][child_handler] MPI process (rank: 544, pid: 56694) exited with status 1
[c494-403.stampede.tacc.utexas.edu:mpispawn_43][child_handler] MPI process (rank: 688, pid: 58292) exited with status 1
[c484-602.stampede.tacc.utexas.edu:mpispawn_30][child_handler] MPI process (rank: 480, pid: 99892) exited with status 1
[c485-502.stampede.tacc.utexas.edu:mpispawn_32][child_handler] MPI process (rank: 514, pid: 6588) exited with status 1
[c494-904.stampede.tacc.utexas.edu:mpispawn_47][readline] Unexpected End-Of-File on file descriptor 9. MPI process died?
[c494-904.stampede.tacc.utexas.edu:mpispawn_47][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c474-301.stampede.tacc.utexas.edu:mpispawn_11][child_handler] MPI process (rank: 178, pid: 12523) exited with status 1
[c474-403.stampede.tacc.utexas.edu:mpispawn_13][child_handler] MPI process (rank: 208, pid: 101184) exited with status 1
[c474-404.stampede.tacc.utexas.edu:mpispawn_14][child_handler] MPI process (rank: 224, pid: 19411) exited with status 1
[c483-003.stampede.tacc.utexas.edu:mpispawn_19][child_handler] MPI process (rank: 304, pid: 99027) exited with status 1
[c484-701.stampede.tacc.utexas.edu:mpispawn_31][child_handler] MPI process (rank: 500, pid: 42138) exited with status 1
[c493-102.stampede.tacc.utexas.edu:mpispawn_39][child_handler] MPI process (rank: 624, pid: 73242) exited with status 1
[c497-504.stampede.tacc.utexas.edu:mpispawn_53][child_handler] MPI process (rank: 848, pid: 37830) exited with status 1
[c494-402.stampede.tacc.utexas.edu:mpispawn_42][child_handler] MPI process (rank: 672, pid: 77267) exited with status 1
[c484-104.stampede.tacc.utexas.edu:mpispawn_28][child_handler] MPI process (rank: 450, pid: 87880) exited with status 1
[c483-002.stampede.tacc.utexas.edu:mpispawn_18][child_handler] MPI process (rank: 290, pid: 103807) exited with status 1
[c484-201.stampede.tacc.utexas.edu:mpispawn_29][child_handler] MPI process (rank: 466, pid: 90383) exited with status 1
[c497-603.stampede.tacc.utexas.edu:mpispawn_56][child_handler] MPI process (rank: 896, pid: 20648) exited with status 1
[c483-901.stampede.tacc.utexas.edu:mpispawn_25][child_handler] MPI process (rank: 400, pid: 14325) exited with status 1
[c484-101.stampede.tacc.utexas.edu:mpispawn_26][child_handler] MPI process (rank: 416, pid: 24625) exited with status 1
[c501-301.stampede.tacc.utexas.edu:mpispawn_61][child_handler] MPI process (rank: 976, pid: 55061) exited with status 1
[c498-304.stampede.tacc.utexas.edu:mpispawn_57][child_handler] MPI process (rank: 912, pid: 36359) exited with status 1
[c494-101.stampede.tacc.utexas.edu:mpispawn_41][child_handler] MPI process (rank: 656, pid: 15710) exited with status 1
[c501-204.stampede.tacc.utexas.edu:mpispawn_60][child_handler] MPI process (rank: 960, pid: 75662) exited with status 1
[c482-404.stampede.tacc.utexas.edu:mpispawn_16][child_handler] MPI process (rank: 258, pid: 24661) exited with status 1
[c497-102.stampede.tacc.utexas.edu:mpispawn_51][child_handler] MPI process (rank: 816, pid: 18558) exited with status 1
[c483-004.stampede.tacc.utexas.edu:mpispawn_20][child_handler] MPI process (rank: 320, pid: 9740) exited with status 1
[c494-702.stampede.tacc.utexas.edu:mpispawn_45][child_handler] MPI process (rank: 720, pid: 56972) exited with status 1
[c498-603.stampede.tacc.utexas.edu:mpispawn_59][child_handler] MPI process (rank: 944, pid: 116487) exited with status 1
[c497-601.stampede.tacc.utexas.edu:mpispawn_54][child_handler] MPI process (rank: 866, pid: 85260) exited with status 1
[c501-801.stampede.tacc.utexas.edu:mpispawn_63][child_handler] MPI process (rank: 1008, pid: 61217) exited with status 1
[c497-202.stampede.tacc.utexas.edu:mpispawn_52][child_handler] MPI process (rank: 844, pid: 9021) exited with status 1
[c495-002.stampede.tacc.utexas.edu:mpispawn_49][child_handler] MPI process (rank: 784, pid: 123822) exited with status 1
[c468-601.stampede.tacc.utexas.edu:mpispawn_0][child_handler] MPI process (rank: 14, pid: 44428) exited with status 1
[c473-804.stampede.tacc.utexas.edu:mpispawn_7][child_handler] MPI process (rank: 114, pid: 28218) exited with status 1
[c497-602.stampede.tacc.utexas.edu:mpispawn_55][child_handler] MPI process (rank: 882, pid: 83446) exited with status 1
[c474-401.stampede.tacc.utexas.edu:mpispawn_12][child_handler] MPI process (rank: 196, pid: 49666) exited with status 1
[c474-204.stampede.tacc.utexas.edu:mpispawn_10][child_handler] MPI process (rank: 166, pid: 9311) exited with status 1
[c491-004.stampede.tacc.utexas.edu:mpispawn_36][child_handler] MPI process (rank: 576, pid: 116479) exited with status 1
[c483-101.stampede.tacc.utexas.edu:mpispawn_21][child_handler] MPI process (rank: 338, pid: 80021) exited with status 1
[c492-701.stampede.tacc.utexas.edu:mpispawn_37][child_handler] MPI process (rank: 592, pid: 118544) exited with status 1
[c483-601.stampede.tacc.utexas.edu:mpispawn_23][child_handler] MPI process (rank: 370, pid: 15077) exited with status 1
[c483-001.stampede.tacc.utexas.edu:mpispawn_17][child_handler] MPI process (rank: 274, pid: 18903) exited with status 1
[c493-201.stampede.tacc.utexas.edu:mpispawn_40][child_handler] MPI process (rank: 640, pid: 58506) exited with status 1
[c498-402.stampede.tacc.utexas.edu:mpispawn_58][child_handler] MPI process (rank: 930, pid: 15998) exited with status 1
[c483-504.stampede.tacc.utexas.edu:mpispawn_22][child_handler] MPI process (rank: 352, pid: 12095) exited with status 1
[c490-904.stampede.tacc.utexas.edu:mpispawn_35][child_handler] MPI process (rank: 560, pid: 35354) exited with status 1
[c501-604.stampede.tacc.utexas.edu:mpispawn_62][child_handler] MPI process (rank: 992, pid: 36449) exited with status 1
[c473-901.stampede.tacc.utexas.edu:mpispawn_8][child_handler] MPI process (rank: 132, pid: 12822) exited with status 1
[c483-804.stampede.tacc.utexas.edu:mpispawn_24][child_handler] MPI process (rank: 388, pid: 32922) exited with status 1
[c478-303.stampede.tacc.utexas.edu:mpispawn_15][child_handler] MPI process (rank: 240, pid: 26663) exited with status 1
[c494-703.stampede.tacc.utexas.edu:mpispawn_46][child_handler] MPI process (rank: 746, pid: 58768) exited with status 1
[c495-001.stampede.tacc.utexas.edu:mpispawn_48][child_handler] MPI process (rank: 774, pid: 62427) exited with status 1
[c495-201.stampede.tacc.utexas.edu:mpispawn_50][child_handler] MPI process (rank: 800, pid: 34094) exited with status 1
[c484-103.stampede.tacc.utexas.edu:mpispawn_27][child_handler] MPI process (rank: 434, pid: 35481) exited with status 1
[c486-102.stampede.tacc.utexas.edu:mpispawn_33][child_handler] MPI process (rank: 530, pid: 2008) exited with status 1
[c493-001.stampede.tacc.utexas.edu:mpispawn_38][child_handler] MPI process (rank: 612, pid: 23513) exited with status 1
[c490-302.stampede.tacc.utexas.edu:mpispawn_34][child_handler] MPI process (rank: 546, pid: 56696) exited with status 1
[c494-904.stampede.tacc.utexas.edu:mpispawn_47][child_handler] MPI process (rank: 766, pid: 87195) exited with status 1
[c474-203.stampede.tacc.utexas.edu:mpispawn_9][child_handler] MPI process (rank: 146, pid: 40937) exited with status 1
[cli_718]: [cli_710]: [cli_708]: [cli_704]: [cli_712]: [cli_716]: [cli_706]: [cli_714]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff91a43b80) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffef7a5560) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff05605440) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffa854bd00) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff198a00d0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff5d5869a0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffddb7c3e0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffb5f2b690) failed
PMPI_Comm_rank(66).: Null communicator

[c494-602.stampede.tacc.utexas.edu:mpispawn_44][readline] Unexpected End-Of-File on file descriptor 6. MPI process died?
[c494-602.stampede.tacc.utexas.edu:mpispawn_44][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c494-602.stampede.tacc.utexas.edu:mpispawn_44][child_handler] MPI process (rank: 718, pid: 13732) exited with status 1
[c494-403.stampede.tacc.utexas.edu:mpispawn_43][error_sighandler] Caught error: Segmentation fault (signal 11)
[c484-602.stampede.tacc.utexas.edu:mpispawn_30][error_sighandler] Caught error: Segmentation fault (signal 11)
bash: line 1: 58290 Segmentation fault      /usr/bin/env LD_LIBRARY_PATH=/opt/apps/gcc4_7/mvapich2/1.9/lib:/opt/apps/gcc4_7/mvapich2/1.9/lib/shared:/opt/apps/gcc/4.7.1/lib64:/opt/apps/gcc/4.7.1/lib:/opt/apps/xsede/gsi-openssh-5.7/lib64:/opt/apps/xsede/gsi-openssh-5.7/lib64 MPISPAWN_MPIRUN_MPD=0 USE_LINEAR_SSH=1 MPISPAWN_MPIRUN_HOST=c468-601.stampede.tacc.utexas.edu MPISPAWN_MPIRUN_HOSTIP=129.114.76.165 MPIRUN_RSH_LAUNCH=1 MPISPAWN_CHECKIN_PORT=36101 MPISPAWN_MPIRUN_PORT=36101 MPISPAWN_NNODES=64 MPISPAWN_GLOBAL_NPROCS=1024 MPISPAWN_MPIRUN_ID=44295 MPISPAWN_ARGC=4 MPDMAN_KVS_TEMPLATE=kvs_513_c468-601.stampede.tacc.utexas.edu_44295 MPISPAWN_LOCAL_NPROCS=16 MPISPAWN_ARGV_0='/home1/03222/tg825207/parallelnbody/teamscatter' MPISPAWN_ARGV_1='256000' MPISPAWN_ARGV_2='-c' MPISPAWN_ARGV_3='2' MPISPAWN_ARGC=4 MPISPAWN_GENERIC_ENV_COUNT=106 MPISPAWN_GENERIC_NAME_0=TACC_ENVLEN MPISPAWN_GENERIC_VALUE_0=6578 MPISPAWN_GENERIC_NAME_1=SLURM_JOB_ID MPISPAWN_GENERIC_VALUE_1="4306264" MPISPAWN_GENERIC_NAME_2=SLURMD_NODENAME MPISPAWN_GENERIC_VALUE_2="c468-601" MPISPAWN_GENERIC_NAME_3=STOCKYARD MPISPAWN_GENERIC_VALUE_3="/work/03222/tg825207" MPISPAWN_GENERIC_NAME_4=LC_ALL MPISPAWN_GENERIC_VALUE_4="en_US.UTF-8" MPISPAWN_GENERIC_NAME_5=SLURM_CHECKPOINT_IMAGE_DIR MPISPAWN_GENERIC_VALUE_5="/home1/03222/tg825207/parallelnbody" MPISPAWN_GENERIC_NAME_6=LMOD_FAMILY_MPI MPISPAWN_GENERIC_VALUE_6="mvapich2" MPISPAWN_GENERIC_NAME_7=SLURM_JOBID MPISPAWN_GENERIC_VALUE_7="4306264" MPISPAWN_GENERIC_NAME_8=_ModuleTable001_ MPISPAWN_GENERIC_VALUE_8="X01vZHVsZVRhYmxlXz17WyJhY3RpdmVTaXplIl09OCxiYXNlTXBhdGhBPXsiL29wdC9hcHBzL3hzZWRlL21vZHVsZWZpbGVzIiwiL29wdC9hcHBzL21vZHVsZWZpbGVzIiwiL29wdC9tb2R1bGVmaWxlcyIsfSxbImNfcmVidWlsZFRpbWUiXT1mYWxzZSxbImNfc2hvcnRUaW1lIl09ZmFsc2UsZmFtaWx5PXtbIk1QSSJdPSJtdmFwaWNoMiIsWyJjb21waWxlciJdPSJnY2MiLH0saW5hY3RpdmU9e30sbVQ9e0xpbnV4PXtbIkZOIl09Ii9vcHQvbW9kdWxlZmlsZXMvTGludXgiLFsiZGVmYXVsdCJdPTAsWyJmdWxsTmFtZSJdPSJMaW51eCIsWyJsb2FkT3JkZXIiXT0yLHByb3BUPXt9LFsic2hvcnQiXT0iTGludXgiLFsic3RhdHVzIl09ImFjdGl2ZSIsfSxUQUNDPXtbIkZOIl09Ii9v" MPISPAWN_GENERIC_NAME_9=TACC_SYSTEM MPISPAWN_GENERIC_VALUE_9="stampede" MPISPAWN_GENERIC_NAME_10=WORK MPISPAWN_GENERIC_VALUE_10="/work/03222/tg825207" MPISPAWN_GENERIC_NAME_11=SLURM_GTIDS MPISPAWN_GENERIC_VALUE_11="0" MPISPAWN_GENERIC_NAME_12=GCC_LIB MPISPAWN_GENERIC_VALUE_12="/opt/apps/gcc/4.7.1/lib64" MPISPAWN_GENERIC_NAME_13=SLURM_PRIO_PROCESS MPISPAWN_GENERIC_VALUE_13="0" MPISPAWN_GENERIC_NAME_14=SCRATCH MPISPAWN_GENERIC_VALUE_14="/scratch/03222/tg825207" MPISPAWN_GENERIC_NAME_15=LOGNAME MPISPAWN_GENERIC_VALUE_15="tg825207" MPISPAWN_GENERIC_NAME_16=_ModuleTable002_ MPISPAWN_GENERIC_VALUE_16="cHQvbW9kdWxlZmlsZXMvVEFDQyIsWyJkZWZhdWx0Il09MCxbImZ1bGxOYW1lIl09IlRBQ0MiLFsibG9hZE9yZGVyIl09Nixwcm9wVD17fSxbInNob3J0Il09IlRBQ0MiLFsic3RhdHVzIl09ImFjdGl2ZSIsfSxbIlRBQ0MtcGF0aHMiXT17WyJGTiJdPSIvb3B0L21vZHVsZWZpbGVzL1RBQ0MtcGF0aHMiLFsiZGVmYXVsdCJdPTAsWyJmdWxsTmFtZSJdPSJUQUNDLXBhdGhzIixbImxvYWRPcmRlciJdPTEscHJvcFQ9e30sWyJzaG9ydCJdPSJUQUNDLXBhdGhzIixbInN0YXR1cyJdPSJhY3RpdmUiLH0sY2x1c3Rlcj17WyJGTiJdPSIvb3B0L21vZHVsZWZpbGVzL2NsdXN0ZXIiLFsiZGVmYXVsdCJdPTAsWyJmdWxsTmFtZSJdPSJjbHVzdGVyIixbImxvYWRPcmRlciJdPTUscHJvcFQ9" MPISPAWN_GENERIC_NAME_17=INPUTRC MPISPAWN_GENERIC_VALUE_17="/etc/inputrc" MPISPAWN_GENERIC_NAME_18=LMOD_CMD MPISPAWN_GENERIC_VALUE_18="/opt/apps/lmod/lmod/libexec/lmod" MPISPAWN_GENERIC_NAME_19=I_MPI_OFA_ADAPTER_NAME MPISPAWN_GENERIC_VALUE_19="mlx4_0" MPISPAWN_GENERIC_NAME_20=QTLIB MPISPAWN_GENERIC_VALUE_20="/usr/lib64/qt-3.3/lib" MPISPAWN_GENERIC_NAME_21=NODE_TASKS_PPN_INFO MPISPAWN_GENERIC_VALUE_21=""16,0_"" MPISPAWN_GENERIC_NAME_22=SLURM_CPUS_ON_NODE MPISPAWN_GENERIC_VALUE_22="16" MPISPAWN_GENERIC_NAME_23=PATH MPISPAWN_GENERIC_VALUE_23="/opt/apps/gcc4_7/mvapich2/1.9/bin:/opt/apps/gcc/4.7.1/bin:/opt/apps/xalt/0.4.4/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/opt/apps/xsede/gsi-openssh-5.7/bin:/usr/X11R6/bin:/opt/ofed/bin:/opt/ofed/sbin:." MPISPAWN_GENERIC_NAME_24=SLURM_TACC_JOBNAME MPISPAWN_GENERIC_VALUE_24="stampedeLargeScaleAllTS.sh" MPISPAWN_GENERIC_NAME_25=SLURM_NODEID MPISPAWN_GENERIC_VALUE_25="0" MPISPAWN_GENERIC_NAME_26=LMOD_PKG MPISPAWN_GENERIC_VALUE_26="/opt/apps/lmod/lmod" MPISPAWN_GENERIC_NAME_27=MIC_I_MPI_OFA_ADAPTER_NAME MPISPAWN_GENERIC_VALUE_27="mlx4_0" MPISPAWN_GENERIC_NAME_28=HISTSIZE MPISPAWN_GENERIC_VALUE_28="1000" MPISPAWN_GENERIC_NAME_29=SLURM_NTASKS MPISPAWN_GENERIC_VALUE_29="2048" MPISPAWN_GENERIC_NAME_30=SHLIB_PATH MPISPAWN_GENERIC_VALUE_30="/opt/apps/xsede/gsi-openssh-5.7/lib64" MPISPAWN_GENERIC_NAME_31=TACC_OVERRIDE_PROJECT MPISPAWN_GENERIC_VALUE_31="TG-ASC140020" MPISPAWN_GENERIC_NAME_32=_ModuleTable005_ MPISPAWN_GENERIC_VALUE_32="Z2NjNF83L21vZHVsZWZpbGVzIiwiL29wdC9hcHBzL3hzZWRlL21vZHVsZWZpbGVzIiwiL29wdC9hcHBzL21vZHVsZWZpbGVzIiwiL29wdC9tb2R1bGVmaWxlcyIsfSxbInN5c3RlbUJhc2VNUEFUSCJdPSIvb3B0L2FwcHMveHNlZGUvbW9kdWxlZmlsZXM6L29wdC9hcHBzL21vZHVsZWZpbGVzOi9vcHQvbW9kdWxlZmlsZXMiLFsidmVyc2lvbiJdPTIsfQ==" MPISPAWN_GENERIC_NAME_33=_ModuleTable_Sz_ MPISPAWN_GENERIC_VALUE_33="5" MPISPAWN_GENERIC_NAME_34=MAIL MPISPAWN_GENERIC_VALUE_34="/var/spool/mail/tg825207" MPISPAWN_GENERIC_NAME_35=MV2_USE_HUGEPAGES MPISPAWN_GENERIC_VALUE_35="0" MPISPAWN_GENERIC_NAME_36=SLURM_TOPOLOGY_ADDR MPISPAWN_GENERIC_VALUE_36="c468-601" MPISPAWN_GENERIC_NAME_37=TACC_XALT_DIR MPISPAWN_GENERIC_VALUE_37="/opt/apps/xalt/0.4.4/" MPISPAWN_GENERIC_NAME_38=QTDIR MPISPAWN_GENERIC_VALUE_38="/usr/lib64/qt-3.3" MPISPAWN_GENERIC_NAME_39=SYSTEM MPISPAWN_GENERIC_VALUE_39="linux" MPISPAWN_GENERIC_NAME_40=SLURM_TACC_RUNLIMIT_MINS MPISPAWN_GENERIC_VALUE_40="180" MPISPAWN_GENERIC_NAME_41=SLURM_SUBMIT_DIR MPISPAWN_GENERIC_VALUE_41="/home1/03222/tg825207/parallelnbody" MPISPAWN_GENERIC_NAME_42=LIBPATH MPISPAWN_GENERIC_VALUE_42="/opt/apps/xsede/gsi-openssh-5.7/lib64:/usr/lib:/lib" MPISPAWN_GENERIC_NAME_43=HOSTNAME MPISPAWN_GENERIC_VALUE_43="c468-601" MPISPAWN_GENERIC_NAME_44=LMOD_DEFAULT_MODULEPATH MPISPAWN_GENERIC_VALUE_44="/opt/apps/xsede/modulefiles:/opt/apps/modulefiles:/opt/modulefiles" MPISPAWN_GENERIC_NAME_45=ARCHIVE MPISPAWN_GENERIC_VALUE_45="/home/03222/tg825207" MPISPAWN_GENERIC_NAME_46=_ MPISPAWN_GENERIC_VALUE_46="/usr/local/bin/build_env.pl" MPISPAWN_GENERIC_NAME_47=__BASHRC_SOURCED__ MPISPAWN_GENERIC_VALUE_47="1" MPISPAWN_GENERIC_NAME_48=APPS MPISPAWN_GENERIC_VALUE_48="/opt/apps" MPISPAWN_GENERIC_NAME_49=SHELL MPISPAWN_GENERIC_VALUE_49="/bin/bash" MPISPAWN_GENERIC_NAME_50=ARMCI_OPENIB_DEVICE MPISPAWN_GENERIC_VALUE_50="mlx4_0" MPISPAWN_GENERIC_NAME_51=ENVIRONMENT MPISPAWN_GENERIC_VALUE_51="BATCH" MPISPAWN_GENERIC_NAME_52=TACC_FAMILY_MPI MPISPAWN_GENERIC_VALUE_52="mvapich2" MPISPAWN_GENERIC_NAME_53=MV2_IBA_HCA MPISPAWN_GENERIC_VALUE_53="mlx4_0" MPISPAWN_GENERIC_NAME_54=TACC_PUBLIC_MACHINE MPISPAWN_GENERIC_VALUE_54="1" MPISPAWN_GENERIC_NAME_55=SLURM_TACC_NODES MPISPAWN_GENERIC_VALUE_55="1" MPISPAWN_GENERIC_NAME_56=_ModuleTable003_ MPISPAWN_GENERIC_VALUE_56="e30sWyJzaG9ydCJdPSJjbHVzdGVyIixbInN0YXR1cyJdPSJhY3RpdmUiLH0sWyJjbHVzdGVyLXBhdGhzIl09e1siRk4iXT0iL29wdC9tb2R1bGVmaWxlcy9jbHVzdGVyLXBhdGhzIixbImRlZmF1bHQiXT0wLFsiZnVsbE5hbWUiXT0iY2x1c3Rlci1wYXRocyIsWyJsb2FkT3JkZXIiXT0zLHByb3BUPXt9LFsic2hvcnQiXT0iY2x1c3Rlci1wYXRocyIsWyJzdGF0dXMiXT0iYWN0aXZlIix9LGdjYz17WyJGTiJdPSIvb3B0L2FwcHMvbW9kdWxlZmlsZXMvZ2NjLzQuNy4xLmx1YSIsWyJkZWZhdWx0Il09MCxbImZ1bGxOYW1lIl09ImdjYy80LjcuMSIsWyJsb2FkT3JkZXIiXT03LHByb3BUPXt9LFsic2hvcnQiXT0iZ2NjIixbInN0YXR1cyJdPSJhY3RpdmUiLH0sbXZhcGljaDI9e1si" MPISPAWN_GENERIC_NAME_57=QTINC MPISPAWN_GENERIC_VALUE_57="/usr/lib64/qt-3.3/include" MPISPAWN_GENERIC_NAME_58=CVS_RSH MPISPAWN_GENERIC_VALUE_58="ssh" MPISPAWN_GENERIC_NAME_59=MV2_USE_OLD_BCAST MPISPAWN_GENERIC_VALUE_59="1" MPISPAWN_GENERIC_NAME_60=SLURM_JOB_NUM_NODES MPISPAWN_GENERIC_VALUE_60="128" MPISPAWN_GENERIC_NAME_61=SLURM_TASK_PID MPISPAWN_GENERIC_VALUE_61="31479" MPISPAWN_GENERIC_NAME_62=TACC_MPI_GETMODE MPISPAWN_GENERIC_VALUE_62="mvapich2_ssh" MPISPAWN_GENERIC_NAME_63=LANG MPISPAWN_GENERIC_VALUE_63="en_US.UTF-8" MPISPAWN_GENERIC_NAME_64=USER MPISPAWN_GENERIC_VALUE_64="tg825207" MPISPAWN_GENERIC_NAME_65=MIC_I_MPI_DAPL_PROVIDER MPISPAWN_GENERIC_VALUE_65="ofa-v2-mlx4_0-1u" MPISPAWN_GENERIC_NAME_66=G_BROKEN_FILENAMES MPISPAWN_GENERIC_VALUE_66="1" MPISPAWN_GENERIC_NAME_67=SLURM_LOCALID MPISPAWN_GENERIC_VALUE_67="0" MPISPAWN_GENERIC_NAME_68=SLURM_NPROCS MPISPAWN_GENERIC_VALUE_68="2048" MPISPAWN_GENERIC_NAME_69=SLURM_TACC_NNODES_SET MPISPAWN_GENERIC_VALUE_69="0" MPISPAWN_GENERIC_NAME_70=SLURM_TACC_CORES MPISPAWN_GENERIC_VALUE_70="2048" MPISPAWN_GENERIC_NAME_71=LMOD_DIR MPISPAWN_GENERIC_VALUE_71="/opt/apps/lmod/lmod/libexec" MPISPAWN_GENERIC_NAME_72=LMOD_SETTARG_CMD MPISPAWN_GENERIC_VALUE_72=":" MPISPAWN_GENERIC_NAME_73=OMP_NUM_THREADS MPISPAWN_GENERIC_VALUE_73="1" MPISPAWN_GENERIC_NAME_74=_ModuleTable004_ MPISPAWN_GENERIC_VALUE_74="Rk4iXT0iL29wdC9hcHBzL2djYzRfNy9tb2R1bGVmaWxlcy9tdmFwaWNoMi8xLjlhMiIsWyJkZWZhdWx0Il09MSxbImZ1bGxOYW1lIl09Im12YXBpY2gyLzEuOWEyIixbImxvYWRPcmRlciJdPTgscHJvcFQ9e30sWyJzaG9ydCJdPSJtdmFwaWNoMiIsWyJzdGF0dXMiXT0iYWN0aXZlIix9LHhhbHQ9e1siRk4iXT0iL29wdC9hcHBzL21vZHVsZWZpbGVzL3hhbHQvMC40LjQubHVhIixbImRlZmF1bHQiXT0xLFsiZnVsbE5hbWUiXT0ieGFsdC8wLjQuNCIsWyJsb2FkT3JkZXIiXT00LHByb3BUPXt9LFsic2hvcnQiXT0ieGFsdCIsWyJzdGF0dXMiXT0iYWN0aXZlIix9LH0sbXBhdGhBPXsiL29wdC9hcHBzL2djYzRfNy9tdmFwaWNoMl8xXzkvbW9kdWxlZmlsZXMiLCIvb3B0L2FwcHMv" MPISPAWN_GENERIC_NAME_75=__PERSONAL_PATH__ MPISPAWN_GENERIC_VALUE_75="1" MPISPAWN_GENERIC_NAME_76=LMOD_SYSTEM_DEFAULT_MODULES MPISPAWN_GENERIC_VALUE_76="TACC" MPISPAWN_GENERIC_NAME_77=SLURM_JOB_NAME MPISPAWN_GENERIC_VALUE_77="stampedeLargeScaleAllTS.sh" MPISPAWN_GENERIC_NAME_78=SLURM_SUBMIT_HOST MPISPAWN_GENERIC_VALUE_78="login4.stampede.tacc.utexas.edu" MPISPAWN_GENERIC_NAME_79=LD_LIBRARY_PATH MPISPAWN_GENERIC_VALUE_79="/opt/apps/gcc4_7/mvapich2/1.9/lib:/opt/apps/gcc4_7/mvapich2/1.9/lib/shared:/opt/apps/gcc/4.7.1/lib64:/opt/apps/gcc/4.7.1/lib:/opt/apps/xsede/gsi-openssh-5.7/lib64:/opt/apps/xsede/gsi-openssh-5.7/lib64" MPISPAWN_GENERIC_NAME_80=SLURM_QUEUE MPISPAWN_GENERIC_VALUE_80="normal" MPISPAWN_GENERIC_NAME_81=PKG_CONFIG_PATH MPISPAWN_GENERIC_VALUE_81="/opt/apps/gcc4_7/mvapich2/1.9/lib/pkgconfig" MPISPAWN_GENERIC_NAME_82=LMOD_PREPEND_BLOCK MPISPAWN_GENERIC_VALUE_82="normal" MPISPAWN_GENERIC_NAME_83=TACC_FAMILY_COMPILER MPISPAWN_GENERIC_VALUE_83="gcc" MPISPAWN_GENERIC_NAME_84=TACC_DOMAIN MPISPAWN_GENERIC_VALUE_84="stampede" MPISPAWN_GENERIC_NAME_85=MV2_HOMOGENEOUS_CLUSTER MPISPAWN_GENERIC_VALUE_85="1" MPISPAWN_GENERIC_NAME_86=LMOD_FAMILY_COMPILER MPISPAWN_GENERIC_VALUE_86="gcc" MPISPAWN_GENERIC_NAME_87=MV2_USE_UD_HYBRID MPISPAWN_GENERIC_VALUE_87="0" MPISPAWN_GENERIC_NAME_88=MPICH_HOME MPISPAWN_GENERIC_VALUE_88="/opt/apps/gcc4_7/mvapich2/1.9" MPISPAWN_GENERIC_NAME_89=SLURM_JOB_NODELIST MPISPAWN_GENERIC_VALUE_89="c468-601,c471-503,c472-[401-402],c473-[303-304,401,804,901],c474-[203-204,301,401,403-404],c478-303,c482-404,c483-[001-004,101,504,601,804,901],c484-[101,103-104,201,602,701],c485-502,c486-102,c490-[302,904],c491-004,c492-701,c493-[001,102,201],c494-[101,402-403,602,702-703,904],c495-[001-002,201],c497-[102,202,504,601-603],c498-[304,402,603],c501-[204,301,604,801],c502-104,c503-901,c504-[602,604,702,801],c505-[001,202-204,401,502-504,802,901-904],c506-[302,704],c507-[102,202,503,601,904],c508-[101,204,601,702-703,802,903-904],c509-[002,201,301,303,401,501-502],c510-[003,401,404,501,503],c511-[601-602,902-903],c536-[801,902],c537-902,c538-[001,003,104],c540-[104,201-202,804,901-904]" MPISPAWN_GENERIC_NAME_90=SLURM_PROCID MPISPAWN_GENERIC_VALUE_90="0" MPISPAWN_GENERIC_NAME_91=I_MPI_DAPL_PROVIDER MPISPAWN_GENERIC_VALUE_91="ofa-v2-mlx4_0-1u" MPISPAWN_GENERIC_NAME_92=GLOBUS_LOCATION MPISPAWN_GENERIC_VALUE_92="/opt/apps/xsede/gsi-openssh-5.7" MPISPAWN_GENERIC_NAME_93=LMOD_COLORIZE MPISPAWN_GENERIC_VALUE_93="yes" MPISPAWN_GENERIC_NAME_94=I_MPI_HYDRA_BRANCH_COUNT MPISPAWN_GENERIC_VALUE_94="6000" MPISPAWN_GENERIC_NAME_95=SLURM_NNODES MPISPAWN_GENERIC_VALUE_95="128" MPISPAWN_GENERIC_NAME_96=MV2_DEFAULT_TIME_OUT MPISPAWN_GENERIC_VALUE_96="23" MPISPAWN_GENERIC_NAME_97=SLURM_TACC_NCORES_SET MPISPAWN_GENERIC_VALUE_97="1" MPISPAWN_GENERIC_NAME_98=__Init_Default_Modules MPISPAWN_GENERIC_VALUE_98="1" MPISPAWN_GENERIC_NAME_99=BASH_ENV MPISPAWN_GENERIC_VALUE_99="/etc/tacc/tacc_functions" MPISPAWN_GENERIC_NAME_100=SLURM_TOPOLOGY_ADDR_PATTERN MPISPAWN_GENERIC_VALUE_100="node" MPISPAWN_GENERIC_NAME_101=MV2_USE_RING_STARTUP MPISPAWN_GENERIC_VALUE_101="0" MPISPAWN_GENERIC_NAME_102=TACC_XALT_BIN MPISPAWN_GENERIC_VALUE_102="/opt/apps/xalt/0.4.4/bin" MPISPAWN_GENERIC_NAME_103=X509_USER_PROXY MPISPAWN_GENERIC_VALUE_103="/tmp/x509up_p3084.file9zM3fu.1" MPISPAWN_GENERIC_NAME_104=TMPDIR MPISPAWN_GENERIC_VALUE_104="/tmp" MPISPAWN_GENERIC_NAME_105=LMOD_FULL_SETTARG_SUPPORT MPISPAWN_GENERIC_VALUE_105="no" MPISPAWN_ID=43 MPISPAWN_WORKING_DIR=/home1/03222/tg825207/parallelnbody MPISPAWN_MPIRUN_RANK_0=688 MPISPAWN_MPIRUN_RANK_1=689 MPISPAWN_MPIRUN_RANK_2=690 MPISPAWN_MPIRUN_RANK_3=691 MPISPAWN_MPIRUN_RANK_4=692 MPISPAWN_MPIRUN_RANK_5=693 MPISPAWN_MPIRUN_RANK_6=694 MPISPAWN_MPIRUN_RANK_7=695 MPISPAWN_MPIRUN_RANK_8=696 MPISPAWN_MPIRUN_RANK_9=697 MPISPAWN_MPIRUN_RANK_10=698 MPISPAWN_MPIRUN_RANK_11=699 MPISPAWN_MPIRUN_RANK_12=700 MPISPAWN_MPIRUN_RANK_13=701 MPISPAWN_MPIRUN_RANK_14=702 MPISPAWN_MPIRUN_RANK_15=703 /opt/apps/gcc4_7/mvapich2/1.9/bin/mpispawn 0
bash: line 1: 99890 Segmentation fault      /usr/bin/env LD_LIBRARY_PATH=/opt/apps/gcc4_7/mvapich2/1.9/lib:/opt/apps/gcc4_7/mvapich2/1.9/lib/shared:/opt/apps/gcc/4.7.1/lib64:/opt/apps/gcc/4.7.1/lib:/opt/apps/xsede/gsi-openssh-5.7/lib64:/opt/apps/xsede/gsi-openssh-5.7/lib64 MPISPAWN_MPIRUN_MPD=0 USE_LINEAR_SSH=1 MPISPAWN_MPIRUN_HOST=c468-601.stampede.tacc.utexas.edu MPISPAWN_MPIRUN_HOSTIP=129.114.76.165 MPIRUN_RSH_LAUNCH=1 MPISPAWN_CHECKIN_PORT=36101 MPISPAWN_MPIRUN_PORT=36101 MPISPAWN_NNODES=64 MPISPAWN_GLOBAL_NPROCS=1024 MPISPAWN_MPIRUN_ID=44295 MPISPAWN_ARGC=4 MPDMAN_KVS_TEMPLATE=kvs_513_c468-601.stampede.tacc.utexas.edu_44295 MPISPAWN_LOCAL_NPROCS=16 MPISPAWN_ARGV_0='/home1/03222/tg825207/parallelnbody/teamscatter' MPISPAWN_ARGV_1='256000' MPISPAWN_ARGV_2='-c' MPISPAWN_ARGV_3='2' MPISPAWN_ARGC=4 MPISPAWN_GENERIC_ENV_COUNT=106 MPISPAWN_GENERIC_NAME_0=TACC_ENVLEN MPISPAWN_GENERIC_VALUE_0=6578 MPISPAWN_GENERIC_NAME_1=SLURM_JOB_ID MPISPAWN_GENERIC_VALUE_1="4306264" MPISPAWN_GENERIC_NAME_2=SLURMD_NODENAME MPISPAWN_GENERIC_VALUE_2="c468-601" MPISPAWN_GENERIC_NAME_3=STOCKYARD MPISPAWN_GENERIC_VALUE_3="/work/03222/tg825207" MPISPAWN_GENERIC_NAME_4=LC_ALL MPISPAWN_GENERIC_VALUE_4="en_US.UTF-8" MPISPAWN_GENERIC_NAME_5=SLURM_CHECKPOINT_IMAGE_DIR MPISPAWN_GENERIC_VALUE_5="/home1/03222/tg825207/parallelnbody" MPISPAWN_GENERIC_NAME_6=LMOD_FAMILY_MPI MPISPAWN_GENERIC_VALUE_6="mvapich2" MPISPAWN_GENERIC_NAME_7=SLURM_JOBID MPISPAWN_GENERIC_VALUE_7="4306264" MPISPAWN_GENERIC_NAME_8=_ModuleTable001_ MPISPAWN_GENERIC_VALUE_8="X01vZHVsZVRhYmxlXz17WyJhY3RpdmVTaXplIl09OCxiYXNlTXBhdGhBPXsiL29wdC9hcHBzL3hzZWRlL21vZHVsZWZpbGVzIiwiL29wdC9hcHBzL21vZHVsZWZpbGVzIiwiL29wdC9tb2R1bGVmaWxlcyIsfSxbImNfcmVidWlsZFRpbWUiXT1mYWxzZSxbImNfc2hvcnRUaW1lIl09ZmFsc2UsZmFtaWx5PXtbIk1QSSJdPSJtdmFwaWNoMiIsWyJjb21waWxlciJdPSJnY2MiLH0saW5hY3RpdmU9e30sbVQ9e0xpbnV4PXtbIkZOIl09Ii9vcHQvbW9kdWxlZmlsZXMvTGludXgiLFsiZGVmYXVsdCJdPTAsWyJmdWxsTmFtZSJdPSJMaW51eCIsWyJsb2FkT3JkZXIiXT0yLHByb3BUPXt9LFsic2hvcnQiXT0iTGludXgiLFsic3RhdHVzIl09ImFjdGl2ZSIsfSxUQUNDPXtbIkZOIl09Ii9v" MPISPAWN_GENERIC_NAME_9=TACC_SYSTEM MPISPAWN_GENERIC_VALUE_9="stampede" MPISPAWN_GENERIC_NAME_10=WORK MPISPAWN_GENERIC_VALUE_10="/work/03222/tg825207" MPISPAWN_GENERIC_NAME_11=SLURM_GTIDS MPISPAWN_GENERIC_VALUE_11="0" MPISPAWN_GENERIC_NAME_12=GCC_LIB MPISPAWN_GENERIC_VALUE_12="/opt/apps/gcc/4.7.1/lib64" MPISPAWN_GENERIC_NAME_13=SLURM_PRIO_PROCESS MPISPAWN_GENERIC_VALUE_13="0" MPISPAWN_GENERIC_NAME_14=SCRATCH MPISPAWN_GENERIC_VALUE_14="/scratch/03222/tg825207" MPISPAWN_GENERIC_NAME_15=LOGNAME MPISPAWN_GENERIC_VALUE_15="tg825207" MPISPAWN_GENERIC_NAME_16=_ModuleTable002_ MPISPAWN_GENERIC_VALUE_16="cHQvbW9kdWxlZmlsZXMvVEFDQyIsWyJkZWZhdWx0Il09MCxbImZ1bGxOYW1lIl09IlRBQ0MiLFsibG9hZE9yZGVyIl09Nixwcm9wVD17fSxbInNob3J0Il09IlRBQ0MiLFsic3RhdHVzIl09ImFjdGl2ZSIsfSxbIlRBQ0MtcGF0aHMiXT17WyJGTiJdPSIvb3B0L21vZHVsZWZpbGVzL1RBQ0MtcGF0aHMiLFsiZGVmYXVsdCJdPTAsWyJmdWxsTmFtZSJdPSJUQUNDLXBhdGhzIixbImxvYWRPcmRlciJdPTEscHJvcFQ9e30sWyJzaG9ydCJdPSJUQUNDLXBhdGhzIixbInN0YXR1cyJdPSJhY3RpdmUiLH0sY2x1c3Rlcj17WyJGTiJdPSIvb3B0L21vZHVsZWZpbGVzL2NsdXN0ZXIiLFsiZGVmYXVsdCJdPTAsWyJmdWxsTmFtZSJdPSJjbHVzdGVyIixbImxvYWRPcmRlciJdPTUscHJvcFQ9" MPISPAWN_GENERIC_NAME_17=INPUTRC MPISPAWN_GENERIC_VALUE_17="/etc/inputrc" MPISPAWN_GENERIC_NAME_18=LMOD_CMD MPISPAWN_GENERIC_VALUE_18="/opt/apps/lmod/lmod/libexec/lmod" MPISPAWN_GENERIC_NAME_19=I_MPI_OFA_ADAPTER_NAME MPISPAWN_GENERIC_VALUE_19="mlx4_0" MPISPAWN_GENERIC_NAME_20=QTLIB MPISPAWN_GENERIC_VALUE_20="/usr/lib64/qt-3.3/lib" MPISPAWN_GENERIC_NAME_21=NODE_TASKS_PPN_INFO MPISPAWN_GENERIC_VALUE_21=""16,0_"" MPISPAWN_GENERIC_NAME_22=SLURM_CPUS_ON_NODE MPISPAWN_GENERIC_VALUE_22="16" MPISPAWN_GENERIC_NAME_23=PATH MPISPAWN_GENERIC_VALUE_23="/opt/apps/gcc4_7/mvapich2/1.9/bin:/opt/apps/gcc/4.7.1/bin:/opt/apps/xalt/0.4.4/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/opt/apps/xsede/gsi-openssh-5.7/bin:/usr/X11R6/bin:/opt/ofed/bin:/opt/ofed/sbin:." MPISPAWN_GENERIC_NAME_24=SLURM_TACC_JOBNAME MPISPAWN_GENERIC_VALUE_24="stampedeLargeScaleAllTS.sh" MPISPAWN_GENERIC_NAME_25=SLURM_NODEID MPISPAWN_GENERIC_VALUE_25="0" MPISPAWN_GENERIC_NAME_26=LMOD_PKG MPISPAWN_GENERIC_VALUE_26="/opt/apps/lmod/lmod" MPISPAWN_GENERIC_NAME_27=MIC_I_MPI_OFA_ADAPTER_NAME MPISPAWN_GENERIC_VALUE_27="mlx4_0" MPISPAWN_GENERIC_NAME_28=HISTSIZE MPISPAWN_GENERIC_VALUE_28="1000" MPISPAWN_GENERIC_NAME_29=SLURM_NTASKS MPISPAWN_GENERIC_VALUE_29="2048" MPISPAWN_GENERIC_NAME_30=SHLIB_PATH MPISPAWN_GENERIC_VALUE_30="/opt/apps/xsede/gsi-openssh-5.7/lib64" MPISPAWN_GENERIC_NAME_31=TACC_OVERRIDE_PROJECT MPISPAWN_GENERIC_VALUE_31="TG-ASC140020" MPISPAWN_GENERIC_NAME_32=_ModuleTable005_ MPISPAWN_GENERIC_VALUE_32="Z2NjNF83L21vZHVsZWZpbGVzIiwiL29wdC9hcHBzL3hzZWRlL21vZHVsZWZpbGVzIiwiL29wdC9hcHBzL21vZHVsZWZpbGVzIiwiL29wdC9tb2R1bGVmaWxlcyIsfSxbInN5c3RlbUJhc2VNUEFUSCJdPSIvb3B0L2FwcHMveHNlZGUvbW9kdWxlZmlsZXM6L29wdC9hcHBzL21vZHVsZWZpbGVzOi9vcHQvbW9kdWxlZmlsZXMiLFsidmVyc2lvbiJdPTIsfQ==" MPISPAWN_GENERIC_NAME_33=_ModuleTable_Sz_ MPISPAWN_GENERIC_VALUE_33="5" MPISPAWN_GENERIC_NAME_34=MAIL MPISPAWN_GENERIC_VALUE_34="/var/spool/mail/tg825207" MPISPAWN_GENERIC_NAME_35=MV2_USE_HUGEPAGES MPISPAWN_GENERIC_VALUE_35="0" MPISPAWN_GENERIC_NAME_36=SLURM_TOPOLOGY_ADDR MPISPAWN_GENERIC_VALUE_36="c468-601" MPISPAWN_GENERIC_NAME_37=TACC_XALT_DIR MPISPAWN_GENERIC_VALUE_37="/opt/apps/xalt/0.4.4/" MPISPAWN_GENERIC_NAME_38=QTDIR MPISPAWN_GENERIC_VALUE_38="/usr/lib64/qt-3.3" MPISPAWN_GENERIC_NAME_39=SYSTEM MPISPAWN_GENERIC_VALUE_39="linux" MPISPAWN_GENERIC_NAME_40=SLURM_TACC_RUNLIMIT_MINS MPISPAWN_GENERIC_VALUE_40="180" MPISPAWN_GENERIC_NAME_41=SLURM_SUBMIT_DIR MPISPAWN_GENERIC_VALUE_41="/home1/03222/tg825207/parallelnbody" MPISPAWN_GENERIC_NAME_42=LIBPATH MPISPAWN_GENERIC_VALUE_42="/opt/apps/xsede/gsi-openssh-5.7/lib64:/usr/lib:/lib" MPISPAWN_GENERIC_NAME_43=HOSTNAME MPISPAWN_GENERIC_VALUE_43="c468-601" MPISPAWN_GENERIC_NAME_44=LMOD_DEFAULT_MODULEPATH MPISPAWN_GENERIC_VALUE_44="/opt/apps/xsede/modulefiles:/opt/apps/modulefiles:/opt/modulefiles" MPISPAWN_GENERIC_NAME_45=ARCHIVE MPISPAWN_GENERIC_VALUE_45="/home/03222/tg825207" MPISPAWN_GENERIC_NAME_46=_ MPISPAWN_GENERIC_VALUE_46="/usr/local/bin/build_env.pl" MPISPAWN_GENERIC_NAME_47=__BASHRC_SOURCED__ MPISPAWN_GENERIC_VALUE_47="1" MPISPAWN_GENERIC_NAME_48=APPS MPISPAWN_GENERIC_VALUE_48="/opt/apps" MPISPAWN_GENERIC_NAME_49=SHELL MPISPAWN_GENERIC_VALUE_49="/bin/bash" MPISPAWN_GENERIC_NAME_50=ARMCI_OPENIB_DEVICE MPISPAWN_GENERIC_VALUE_50="mlx4_0" MPISPAWN_GENERIC_NAME_51=ENVIRONMENT MPISPAWN_GENERIC_VALUE_51="BATCH" MPISPAWN_GENERIC_NAME_52=TACC_FAMILY_MPI MPISPAWN_GENERIC_VALUE_52="mvapich2" MPISPAWN_GENERIC_NAME_53=MV2_IBA_HCA MPISPAWN_GENERIC_VALUE_53="mlx4_0" MPISPAWN_GENERIC_NAME_54=TACC_PUBLIC_MACHINE MPISPAWN_GENERIC_VALUE_54="1" MPISPAWN_GENERIC_NAME_55=SLURM_TACC_NODES MPISPAWN_GENERIC_VALUE_55="1" MPISPAWN_GENERIC_NAME_56=_ModuleTable003_ MPISPAWN_GENERIC_VALUE_56="e30sWyJzaG9ydCJdPSJjbHVzdGVyIixbInN0YXR1cyJdPSJhY3RpdmUiLH0sWyJjbHVzdGVyLXBhdGhzIl09e1siRk4iXT0iL29wdC9tb2R1bGVmaWxlcy9jbHVzdGVyLXBhdGhzIixbImRlZmF1bHQiXT0wLFsiZnVsbE5hbWUiXT0iY2x1c3Rlci1wYXRocyIsWyJsb2FkT3JkZXIiXT0zLHByb3BUPXt9LFsic2hvcnQiXT0iY2x1c3Rlci1wYXRocyIsWyJzdGF0dXMiXT0iYWN0aXZlIix9LGdjYz17WyJGTiJdPSIvb3B0L2FwcHMvbW9kdWxlZmlsZXMvZ2NjLzQuNy4xLmx1YSIsWyJkZWZhdWx0Il09MCxbImZ1bGxOYW1lIl09ImdjYy80LjcuMSIsWyJsb2FkT3JkZXIiXT03LHByb3BUPXt9LFsic2hvcnQiXT0iZ2NjIixbInN0YXR1cyJdPSJhY3RpdmUiLH0sbXZhcGljaDI9e1si" MPISPAWN_GENERIC_NAME_57=QTINC MPISPAWN_GENERIC_VALUE_57="/usr/lib64/qt-3.3/include" MPISPAWN_GENERIC_NAME_58=CVS_RSH MPISPAWN_GENERIC_VALUE_58="ssh" MPISPAWN_GENERIC_NAME_59=MV2_USE_OLD_BCAST MPISPAWN_GENERIC_VALUE_59="1" MPISPAWN_GENERIC_NAME_60=SLURM_JOB_NUM_NODES MPISPAWN_GENERIC_VALUE_60="128" MPISPAWN_GENERIC_NAME_61=SLURM_TASK_PID MPISPAWN_GENERIC_VALUE_61="31479" MPISPAWN_GENERIC_NAME_62=TACC_MPI_GETMODE MPISPAWN_GENERIC_VALUE_62="mvapich2_ssh" MPISPAWN_GENERIC_NAME_63=LANG MPISPAWN_GENERIC_VALUE_63="en_US.UTF-8" MPISPAWN_GENERIC_NAME_64=USER MPISPAWN_GENERIC_VALUE_64="tg825207" MPISPAWN_GENERIC_NAME_65=MIC_I_MPI_DAPL_PROVIDER MPISPAWN_GENERIC_VALUE_65="ofa-v2-mlx4_0-1u" MPISPAWN_GENERIC_NAME_66=G_BROKEN_FILENAMES MPISPAWN_GENERIC_VALUE_66="1" MPISPAWN_GENERIC_NAME_67=SLURM_LOCALID MPISPAWN_GENERIC_VALUE_67="0" MPISPAWN_GENERIC_NAME_68=SLURM_NPROCS MPISPAWN_GENERIC_VALUE_68="2048" MPISPAWN_GENERIC_NAME_69=SLURM_TACC_NNODES_SET MPISPAWN_GENERIC_VALUE_69="0" MPISPAWN_GENERIC_NAME_70=SLURM_TACC_CORES MPISPAWN_GENERIC_VALUE_70="2048" MPISPAWN_GENERIC_NAME_71=LMOD_DIR MPISPAWN_GENERIC_VALUE_71="/opt/apps/lmod/lmod/libexec" MPISPAWN_GENERIC_NAME_72=LMOD_SETTARG_CMD MPISPAWN_GENERIC_VALUE_72=":" MPISPAWN_GENERIC_NAME_73=OMP_NUM_THREADS MPISPAWN_GENERIC_VALUE_73="1" MPISPAWN_GENERIC_NAME_74=_ModuleTable004_ MPISPAWN_GENERIC_VALUE_74="Rk4iXT0iL29wdC9hcHBzL2djYzRfNy9tb2R1bGVmaWxlcy9tdmFwaWNoMi8xLjlhMiIsWyJkZWZhdWx0Il09MSxbImZ1bGxOYW1lIl09Im12YXBpY2gyLzEuOWEyIixbImxvYWRPcmRlciJdPTgscHJvcFQ9e30sWyJzaG9ydCJdPSJtdmFwaWNoMiIsWyJzdGF0dXMiXT0iYWN0aXZlIix9LHhhbHQ9e1siRk4iXT0iL29wdC9hcHBzL21vZHVsZWZpbGVzL3hhbHQvMC40LjQubHVhIixbImRlZmF1bHQiXT0xLFsiZnVsbE5hbWUiXT0ieGFsdC8wLjQuNCIsWyJsb2FkT3JkZXIiXT00LHByb3BUPXt9LFsic2hvcnQiXT0ieGFsdCIsWyJzdGF0dXMiXT0iYWN0aXZlIix9LH0sbXBhdGhBPXsiL29wdC9hcHBzL2djYzRfNy9tdmFwaWNoMl8xXzkvbW9kdWxlZmlsZXMiLCIvb3B0L2FwcHMv" MPISPAWN_GENERIC_NAME_75=__PERSONAL_PATH__ MPISPAWN_GENERIC_VALUE_75="1" MPISPAWN_GENERIC_NAME_76=LMOD_SYSTEM_DEFAULT_MODULES MPISPAWN_GENERIC_VALUE_76="TACC" MPISPAWN_GENERIC_NAME_77=SLURM_JOB_NAME MPISPAWN_GENERIC_VALUE_77="stampedeLargeScaleAllTS.sh" MPISPAWN_GENERIC_NAME_78=SLURM_SUBMIT_HOST MPISPAWN_GENERIC_VALUE_78="login4.stampede.tacc.utexas.edu" MPISPAWN_GENERIC_NAME_79=LD_LIBRARY_PATH MPISPAWN_GENERIC_VALUE_79="/opt/apps/gcc4_7/mvapich2/1.9/lib:/opt/apps/gcc4_7/mvapich2/1.9/lib/shared:/opt/apps/gcc/4.7.1/lib64:/opt/apps/gcc/4.7.1/lib:/opt/apps/xsede/gsi-openssh-5.7/lib64:/opt/apps/xsede/gsi-openssh-5.7/lib64" MPISPAWN_GENERIC_NAME_80=SLURM_QUEUE MPISPAWN_GENERIC_VALUE_80="normal" MPISPAWN_GENERIC_NAME_81=PKG_CONFIG_PATH MPISPAWN_GENERIC_VALUE_81="/opt/apps/gcc4_7/mvapich2/1.9/lib/pkgconfig" MPISPAWN_GENERIC_NAME_82=LMOD_PREPEND_BLOCK MPISPAWN_GENERIC_VALUE_82="normal" MPISPAWN_GENERIC_NAME_83=TACC_FAMILY_COMPILER MPISPAWN_GENERIC_VALUE_83="gcc" MPISPAWN_GENERIC_NAME_84=TACC_DOMAIN MPISPAWN_GENERIC_VALUE_84="stampede" MPISPAWN_GENERIC_NAME_85=MV2_HOMOGENEOUS_CLUSTER MPISPAWN_GENERIC_VALUE_85="1" MPISPAWN_GENERIC_NAME_86=LMOD_FAMILY_COMPILER MPISPAWN_GENERIC_VALUE_86="gcc" MPISPAWN_GENERIC_NAME_87=MV2_USE_UD_HYBRID MPISPAWN_GENERIC_VALUE_87="0" MPISPAWN_GENERIC_NAME_88=MPICH_HOME MPISPAWN_GENERIC_VALUE_88="/opt/apps/gcc4_7/mvapich2/1.9" MPISPAWN_GENERIC_NAME_89=SLURM_JOB_NODELIST MPISPAWN_GENERIC_VALUE_89="c468-601,c471-503,c472-[401-402],c473-[303-304,401,804,901],c474-[203-204,301,401,403-404],c478-303,c482-404,c483-[001-004,101,504,601,804,901],c484-[101,103-104,201,602,701],c485-502,c486-102,c490-[302,904],c491-004,c492-701,c493-[001,102,201],c494-[101,402-403,602,702-703,904],c495-[001-002,201],c497-[102,202,504,601-603],c498-[304,402,603],c501-[204,301,604,801],c502-104,c503-901,c504-[602,604,702,801],c505-[001,202-204,401,502-504,802,901-904],c506-[302,704],c507-[102,202,503,601,904],c508-[101,204,601,702-703,802,903-904],c509-[002,201,301,303,401,501-502],c510-[003,401,404,501,503],c511-[601-602,902-903],c536-[801,902],c537-902,c538-[001,003,104],c540-[104,201-202,804,901-904]" MPISPAWN_GENERIC_NAME_90=SLURM_PROCID MPISPAWN_GENERIC_VALUE_90="0" MPISPAWN_GENERIC_NAME_91=I_MPI_DAPL_PROVIDER MPISPAWN_GENERIC_VALUE_91="ofa-v2-mlx4_0-1u" MPISPAWN_GENERIC_NAME_92=GLOBUS_LOCATION MPISPAWN_GENERIC_VALUE_92="/opt/apps/xsede/gsi-openssh-5.7" MPISPAWN_GENERIC_NAME_93=LMOD_COLORIZE MPISPAWN_GENERIC_VALUE_93="yes" MPISPAWN_GENERIC_NAME_94=I_MPI_HYDRA_BRANCH_COUNT MPISPAWN_GENERIC_VALUE_94="6000" MPISPAWN_GENERIC_NAME_95=SLURM_NNODES MPISPAWN_GENERIC_VALUE_95="128" MPISPAWN_GENERIC_NAME_96=MV2_DEFAULT_TIME_OUT MPISPAWN_GENERIC_VALUE_96="23" MPISPAWN_GENERIC_NAME_97=SLURM_TACC_NCORES_SET MPISPAWN_GENERIC_VALUE_97="1" MPISPAWN_GENERIC_NAME_98=__Init_Default_Modules MPISPAWN_GENERIC_VALUE_98="1" MPISPAWN_GENERIC_NAME_99=BASH_ENV MPISPAWN_GENERIC_VALUE_99="/etc/tacc/tacc_functions" MPISPAWN_GENERIC_NAME_100=SLURM_TOPOLOGY_ADDR_PATTERN MPISPAWN_GENERIC_VALUE_100="node" MPISPAWN_GENERIC_NAME_101=MV2_USE_RING_STARTUP MPISPAWN_GENERIC_VALUE_101="0" MPISPAWN_GENERIC_NAME_102=TACC_XALT_BIN MPISPAWN_GENERIC_VALUE_102="/opt/apps/xalt/0.4.4/bin" MPISPAWN_GENERIC_NAME_103=X509_USER_PROXY MPISPAWN_GENERIC_VALUE_103="/tmp/x509up_p3084.file9zM3fu.1" MPISPAWN_GENERIC_NAME_104=TMPDIR MPISPAWN_GENERIC_VALUE_104="/tmp" MPISPAWN_GENERIC_NAME_105=LMOD_FULL_SETTARG_SUPPORT MPISPAWN_GENERIC_VALUE_105="no" MPISPAWN_ID=30 MPISPAWN_WORKING_DIR=/home1/03222/tg825207/parallelnbody MPISPAWN_MPIRUN_RANK_0=480 MPISPAWN_MPIRUN_RANK_1=481 MPISPAWN_MPIRUN_RANK_2=482 MPISPAWN_MPIRUN_RANK_3=483 MPISPAWN_MPIRUN_RANK_4=484 MPISPAWN_MPIRUN_RANK_5=485 MPISPAWN_MPIRUN_RANK_6=486 MPISPAWN_MPIRUN_RANK_7=487 MPISPAWN_MPIRUN_RANK_8=488 MPISPAWN_MPIRUN_RANK_9=489 MPISPAWN_MPIRUN_RANK_10=490 MPISPAWN_MPIRUN_RANK_11=491 MPISPAWN_MPIRUN_RANK_12=492 MPISPAWN_MPIRUN_RANK_13=493 MPISPAWN_MPIRUN_RANK_14=494 MPISPAWN_MPIRUN_RANK_15=495 /opt/apps/gcc4_7/mvapich2/1.9/bin/mpispawn 0
[c474-301.stampede.tacc.utexas.edu:mpispawn_11][error_sighandler] Caught error: Segmentation fault (signal 11)
[c474-403.stampede.tacc.utexas.edu:mpispawn_13][error_sighandler] Caught error: Segmentation fault (signal 11)
bash: line 1: 101182 Segmentation fault      /usr/bin/env LD_LIBRARY_PATH=/opt/apps/gcc4_7/mvapich2/1.9/lib:/opt/apps/gcc4_7/mvapich2/1.9/lib/shared:/opt/apps/gcc/4.7.1/lib64:/opt/apps/gcc/4.7.1/lib:/opt/apps/xsede/gsi-openssh-5.7/lib64:/opt/apps/xsede/gsi-openssh-5.7/lib64 MPISPAWN_MPIRUN_MPD=0 USE_LINEAR_SSH=1 MPISPAWN_MPIRUN_HOST=c468-601.stampede.tacc.utexas.edu MPISPAWN_MPIRUN_HOSTIP=129.114.76.165 MPIRUN_RSH_LAUNCH=1 MPISPAWN_CHECKIN_PORT=36101 MPISPAWN_MPIRUN_PORT=36101 MPISPAWN_NNODES=64 MPISPAWN_GLOBAL_NPROCS=1024 MPISPAWN_MPIRUN_ID=44295 MPISPAWN_ARGC=4 MPDMAN_KVS_TEMPLATE=kvs_513_c468-601.stampede.tacc.utexas.edu_44295 MPISPAWN_LOCAL_NPROCS=16 MPISPAWN_ARGV_0='/home1/03222/tg825207/parallelnbody/teamscatter' MPISPAWN_ARGV_1='256000' MPISPAWN_ARGV_2='-c' MPISPAWN_ARGV_3='2' MPISPAWN_ARGC=4 MPISPAWN_GENERIC_ENV_COUNT=106 MPISPAWN_GENERIC_NAME_0=TACC_ENVLEN MPISPAWN_GENERIC_VALUE_0=6578 MPISPAWN_GENERIC_NAME_1=SLURM_JOB_ID MPISPAWN_GENERIC_VALUE_1="4306264" MPISPAWN_GENERIC_NAME_2=SLURMD_NODENAME MPISPAWN_GENERIC_VALUE_2="c468-601" MPISPAWN_GENERIC_NAME_3=STOCKYARD MPISPAWN_GENERIC_VALUE_3="/work/03222/tg825207" MPISPAWN_GENERIC_NAME_4=LC_ALL MPISPAWN_GENERIC_VALUE_4="en_US.UTF-8" MPISPAWN_GENERIC_NAME_5=SLURM_CHECKPOINT_IMAGE_DIR MPISPAWN_GENERIC_VALUE_5="/home1/03222/tg825207/parallelnbody" MPISPAWN_GENERIC_NAME_6=LMOD_FAMILY_MPI MPISPAWN_GENERIC_VALUE_6="mvapich2" MPISPAWN_GENERIC_NAME_7=SLURM_JOBID MPISPAWN_GENERIC_VALUE_7="4306264" MPISPAWN_GENERIC_NAME_8=_ModuleTable001_ MPISPAWN_GENERIC_VALUE_8="X01vZHVsZVRhYmxlXz17WyJhY3RpdmVTaXplIl09OCxiYXNlTXBhdGhBPXsiL29wdC9hcHBzL3hzZWRlL21vZHVsZWZpbGVzIiwiL29wdC9hcHBzL21vZHVsZWZpbGVzIiwiL29wdC9tb2R1bGVmaWxlcyIsfSxbImNfcmVidWlsZFRpbWUiXT1mYWxzZSxbImNfc2hvcnRUaW1lIl09ZmFsc2UsZmFtaWx5PXtbIk1QSSJdPSJtdmFwaWNoMiIsWyJjb21waWxlciJdPSJnY2MiLH0saW5hY3RpdmU9e30sbVQ9e0xpbnV4PXtbIkZOIl09Ii9vcHQvbW9kdWxlZmlsZXMvTGludXgiLFsiZGVmYXVsdCJdPTAsWyJmdWxsTmFtZSJdPSJMaW51eCIsWyJsb2FkT3JkZXIiXT0yLHByb3BUPXt9LFsic2hvcnQiXT0iTGludXgiLFsic3RhdHVzIl09ImFjdGl2ZSIsfSxUQUNDPXtbIkZOIl09Ii9v" MPISPAWN_GENERIC_NAME_9=TACC_SYSTEM MPISPAWN_GENERIC_VALUE_9="stampede" MPISPAWN_GENERIC_NAME_10=WORK MPISPAWN_GENERIC_VALUE_10="/work/03222/tg825207" MPISPAWN_GENERIC_NAME_11=SLURM_GTIDS MPISPAWN_GENERIC_VALUE_11="0" MPISPAWN_GENERIC_NAME_12=GCC_LIB MPISPAWN_GENERIC_VALUE_12="/opt/apps/gcc/4.7.1/lib64" MPISPAWN_GENERIC_NAME_13=SLURM_PRIO_PROCESS MPISPAWN_GENERIC_VALUE_13="0" MPISPAWN_GENERIC_NAME_14=SCRATCH MPISPAWN_GENERIC_VALUE_14="/scratch/03222/tg825207" MPISPAWN_GENERIC_NAME_15=LOGNAME MPISPAWN_GENERIC_VALUE_15="tg825207" MPISPAWN_GENERIC_NAME_16=_ModuleTable002_ MPISPAWN_GENERIC_VALUE_16="cHQvbW9kdWxlZmlsZXMvVEFDQyIsWyJkZWZhdWx0Il09MCxbImZ1bGxOYW1lIl09IlRBQ0MiLFsibG9hZE9yZGVyIl09Nixwcm9wVD17fSxbInNob3J0Il09IlRBQ0MiLFsic3RhdHVzIl09ImFjdGl2ZSIsfSxbIlRBQ0MtcGF0aHMiXT17WyJGTiJdPSIvb3B0L21vZHVsZWZpbGVzL1RBQ0MtcGF0aHMiLFsiZGVmYXVsdCJdPTAsWyJmdWxsTmFtZSJdPSJUQUNDLXBhdGhzIixbImxvYWRPcmRlciJdPTEscHJvcFQ9e30sWyJzaG9ydCJdPSJUQUNDLXBhdGhzIixbInN0YXR1cyJdPSJhY3RpdmUiLH0sY2x1c3Rlcj17WyJGTiJdPSIvb3B0L21vZHVsZWZpbGVzL2NsdXN0ZXIiLFsiZGVmYXVsdCJdPTAsWyJmdWxsTmFtZSJdPSJjbHVzdGVyIixbImxvYWRPcmRlciJdPTUscHJvcFQ9" MPISPAWN_GENERIC_NAME_17=INPUTRC MPISPAWN_GENERIC_VALUE_17="/etc/inputrc" MPISPAWN_GENERIC_NAME_18=LMOD_CMD MPISPAWN_GENERIC_VALUE_18="/opt/apps/lmod/lmod/libexec/lmod" MPISPAWN_GENERIC_NAME_19=I_MPI_OFA_ADAPTER_NAME MPISPAWN_GENERIC_VALUE_19="mlx4_0" MPISPAWN_GENERIC_NAME_20=QTLIB MPISPAWN_GENERIC_VALUE_20="/usr/lib64/qt-3.3/lib" MPISPAWN_GENERIC_NAME_21=NODE_TASKS_PPN_INFO MPISPAWN_GENERIC_VALUE_21=""16,0_"" MPISPAWN_GENERIC_NAME_22=SLURM_CPUS_ON_NODE MPISPAWN_GENERIC_VALUE_22="16" MPISPAWN_GENERIC_NAME_23=PATH MPISPAWN_GENERIC_VALUE_23="/opt/apps/gcc4_7/mvapich2/1.9/bin:/opt/apps/gcc/4.7.1/bin:/opt/apps/xalt/0.4.4/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/opt/apps/xsede/gsi-openssh-5.7/bin:/usr/X11R6/bin:/opt/ofed/bin:/opt/ofed/sbin:." MPISPAWN_GENERIC_NAME_24=SLURM_TACC_JOBNAME MPISPAWN_GENERIC_VALUE_24="stampedeLargeScaleAllTS.sh" MPISPAWN_GENERIC_NAME_25=SLURM_NODEID MPISPAWN_GENERIC_VALUE_25="0" MPISPAWN_GENERIC_NAME_26=LMOD_PKG MPISPAWN_GENERIC_VALUE_26="/opt/apps/lmod/lmod" MPISPAWN_GENERIC_NAME_27=MIC_I_MPI_OFA_ADAPTER_NAME MPISPAWN_GENERIC_VALUE_27="mlx4_0" MPISPAWN_GENERIC_NAME_28=HISTSIZE MPISPAWN_GENERIC_VALUE_28="1000" MPISPAWN_GENERIC_NAME_29=SLURM_NTASKS MPISPAWN_GENERIC_VALUE_29="2048" MPISPAWN_GENERIC_NAME_30=SHLIB_PATH MPISPAWN_GENERIC_VALUE_30="/opt/apps/xsede/gsi-openssh-5.7/lib64" MPISPAWN_GENERIC_NAME_31=TACC_OVERRIDE_PROJECT MPISPAWN_GENERIC_VALUE_31="TG-ASC140020" MPISPAWN_GENERIC_NAME_32=_ModuleTable005_ MPISPAWN_GENERIC_VALUE_32="Z2NjNF83L21vZHVsZWZpbGVzIiwiL29wdC9hcHBzL3hzZWRlL21vZHVsZWZpbGVzIiwiL29wdC9hcHBzL21vZHVsZWZpbGVzIiwiL29wdC9tb2R1bGVmaWxlcyIsfSxbInN5c3RlbUJhc2VNUEFUSCJdPSIvb3B0L2FwcHMveHNlZGUvbW9kdWxlZmlsZXM6L29wdC9hcHBzL21vZHVsZWZpbGVzOi9vcHQvbW9kdWxlZmlsZXMiLFsidmVyc2lvbiJdPTIsfQ==" MPISPAWN_GENERIC_NAME_33=_ModuleTable_Sz_ MPISPAWN_GENERIC_VALUE_33="5" MPISPAWN_GENERIC_NAME_34=MAIL MPISPAWN_GENERIC_VALUE_34="/var/spool/mail/tg825207" MPISPAWN_GENERIC_NAME_35=MV2_USE_HUGEPAGES MPISPAWN_GENERIC_VALUE_35="0" MPISPAWN_GENERIC_NAME_36=SLURM_TOPOLOGY_ADDR MPISPAWN_GENERIC_VALUE_36="c468-601" MPISPAWN_GENERIC_NAME_37=TACC_XALT_DIR MPISPAWN_GENERIC_VALUE_37="/opt/apps/xalt/0.4.4/" MPISPAWN_GENERIC_NAME_38=QTDIR MPISPAWN_GENERIC_VALUE_38="/usr/lib64/qt-3.3" MPISPAWN_GENERIC_NAME_39=SYSTEM MPISPAWN_GENERIC_VALUE_39="linux" MPISPAWN_GENERIC_NAME_40=SLURM_TACC_RUNLIMIT_MINS MPISPAWN_GENERIC_VALUE_40="180" MPISPAWN_GENERIC_NAME_41=SLURM_SUBMIT_DIR MPISPAWN_GENERIC_VALUE_41="/home1/03222/tg825207/parallelnbody" MPISPAWN_GENERIC_NAME_42=LIBPATH MPISPAWN_GENERIC_VALUE_42="/opt/apps/xsede/gsi-openssh-5.7/lib64:/usr/lib:/lib" MPISPAWN_GENERIC_NAME_43=HOSTNAME MPISPAWN_GENERIC_VALUE_43="c468-601" MPISPAWN_GENERIC_NAME_44=LMOD_DEFAULT_MODULEPATH MPISPAWN_GENERIC_VALUE_44="/opt/apps/xsede/modulefiles:/opt/apps/modulefiles:/opt/modulefiles" MPISPAWN_GENERIC_NAME_45=ARCHIVE MPISPAWN_GENERIC_VALUE_45="/home/03222/tg825207" MPISPAWN_GENERIC_NAME_46=_ MPISPAWN_GENERIC_VALUE_46="/usr/local/bin/build_env.pl" MPISPAWN_GENERIC_NAME_47=__BASHRC_SOURCED__ MPISPAWN_GENERIC_VALUE_47="1" MPISPAWN_GENERIC_NAME_48=APPS MPISPAWN_GENERIC_VALUE_48="/opt/apps" MPISPAWN_GENERIC_NAME_49=SHELL MPISPAWN_GENERIC_VALUE_49="/bin/bash" MPISPAWN_GENERIC_NAME_50=ARMCI_OPENIB_DEVICE MPISPAWN_GENERIC_VALUE_50="mlx4_0" MPISPAWN_GENERIC_NAME_51=ENVIRONMENT MPISPAWN_GENERIC_VALUE_51="BATCH" MPISPAWN_GENERIC_NAME_52=TACC_FAMILY_MPI MPISPAWN_GENERIC_VALUE_52="mvapich2" MPISPAWN_GENERIC_NAME_53=MV2_IBA_HCA MPISPAWN_GENERIC_VALUE_53="mlx4_0" MPISPAWN_GENERIC_NAME_54=TACC_PUBLIC_MACHINE MPISPAWN_GENERIC_VALUE_54="1" MPISPAWN_GENERIC_NAME_55=SLURM_TACC_NODES MPISPAWN_GENERIC_VALUE_55="1" MPISPAWN_GENERIC_NAME_56=_ModuleTable003_ MPISPAWN_GENERIC_VALUE_56="e30sWyJzaG9ydCJdPSJjbHVzdGVyIixbInN0YXR1cyJdPSJhY3RpdmUiLH0sWyJjbHVzdGVyLXBhdGhzIl09e1siRk4iXT0iL29wdC9tb2R1bGVmaWxlcy9jbHVzdGVyLXBhdGhzIixbImRlZmF1bHQiXT0wLFsiZnVsbE5hbWUiXT0iY2x1c3Rlci1wYXRocyIsWyJsb2FkT3JkZXIiXT0zLHByb3BUPXt9LFsic2hvcnQiXT0iY2x1c3Rlci1wYXRocyIsWyJzdGF0dXMiXT0iYWN0aXZlIix9LGdjYz17WyJGTiJdPSIvb3B0L2FwcHMvbW9kdWxlZmlsZXMvZ2NjLzQuNy4xLmx1YSIsWyJkZWZhdWx0Il09MCxbImZ1bGxOYW1lIl09ImdjYy80LjcuMSIsWyJsb2FkT3JkZXIiXT03LHByb3BUPXt9LFsic2hvcnQiXT0iZ2NjIixbInN0YXR1cyJdPSJhY3RpdmUiLH0sbXZhcGljaDI9e1si" MPISPAWN_GENERIC_NAME_57=QTINC MPISPAWN_GENERIC_VALUE_57="/usr/lib64/qt-3.3/include" MPISPAWN_GENERIC_NAME_58=CVS_RSH MPISPAWN_GENERIC_VALUE_58="ssh" MPISPAWN_GENERIC_NAME_59=MV2_USE_OLD_BCAST MPISPAWN_GENERIC_VALUE_59="1" MPISPAWN_GENERIC_NAME_60=SLURM_JOB_NUM_NODES MPISPAWN_GENERIC_VALUE_60="128" MPISPAWN_GENERIC_NAME_61=SLURM_TASK_PID MPISPAWN_GENERIC_VALUE_61="31479" MPISPAWN_GENERIC_NAME_62=TACC_MPI_GETMODE MPISPAWN_GENERIC_VALUE_62="mvapich2_ssh" MPISPAWN_GENERIC_NAME_63=LANG MPISPAWN_GENERIC_VALUE_63="en_US.UTF-8" MPISPAWN_GENERIC_NAME_64=USER MPISPAWN_GENERIC_VALUE_64="tg825207" MPISPAWN_GENERIC_NAME_65=MIC_I_MPI_DAPL_PROVIDER MPISPAWN_GENERIC_VALUE_65="ofa-v2-mlx4_0-1u" MPISPAWN_GENERIC_NAME_66=G_BROKEN_FILENAMES MPISPAWN_GENERIC_VALUE_66="1" MPISPAWN_GENERIC_NAME_67=SLURM_LOCALID MPISPAWN_GENERIC_VALUE_67="0" MPISPAWN_GENERIC_NAME_68=SLURM_NPROCS MPISPAWN_GENERIC_VALUE_68="2048" MPISPAWN_GENERIC_NAME_69=SLURM_TACC_NNODES_SET MPISPAWN_GENERIC_VALUE_69="0" MPISPAWN_GENERIC_NAME_70=SLURM_TACC_CORES MPISPAWN_GENERIC_VALUE_70="2048" MPISPAWN_GENERIC_NAME_71=LMOD_DIR MPISPAWN_GENERIC_VALUE_71="/opt/apps/lmod/lmod/libexec" MPISPAWN_GENERIC_NAME_72=LMOD_SETTARG_CMD MPISPAWN_GENERIC_VALUE_72=":" MPISPAWN_GENERIC_NAME_73=OMP_NUM_THREADS MPISPAWN_GENERIC_VALUE_73="1" MPISPAWN_GENERIC_NAME_74=_ModuleTable004_ MPISPAWN_GENERIC_VALUE_74="Rk4iXT0iL29wdC9hcHBzL2djYzRfNy9tb2R1bGVmaWxlcy9tdmFwaWNoMi8xLjlhMiIsWyJkZWZhdWx0Il09MSxbImZ1bGxOYW1lIl09Im12YXBpY2gyLzEuOWEyIixbImxvYWRPcmRlciJdPTgscHJvcFQ9e30sWyJzaG9ydCJdPSJtdmFwaWNoMiIsWyJzdGF0dXMiXT0iYWN0aXZlIix9LHhhbHQ9e1siRk4iXT0iL29wdC9hcHBzL21vZHVsZWZpbGVzL3hhbHQvMC40LjQubHVhIixbImRlZmF1bHQiXT0xLFsiZnVsbE5hbWUiXT0ieGFsdC8wLjQuNCIsWyJsb2FkT3JkZXIiXT00LHByb3BUPXt9LFsic2hvcnQiXT0ieGFsdCIsWyJzdGF0dXMiXT0iYWN0aXZlIix9LH0sbXBhdGhBPXsiL29wdC9hcHBzL2djYzRfNy9tdmFwaWNoMl8xXzkvbW9kdWxlZmlsZXMiLCIvb3B0L2FwcHMv" MPISPAWN_GENERIC_NAME_75=__PERSONAL_PATH__ MPISPAWN_GENERIC_VALUE_75="1" MPISPAWN_GENERIC_NAME_76=LMOD_SYSTEM_DEFAULT_MODULES MPISPAWN_GENERIC_VALUE_76="TACC" MPISPAWN_GENERIC_NAME_77=SLURM_JOB_NAME MPISPAWN_GENERIC_VALUE_77="stampedeLargeScaleAllTS.sh" MPISPAWN_GENERIC_NAME_78=SLURM_SUBMIT_HOST MPISPAWN_GENERIC_VALUE_78="login4.stampede.tacc.utexas.edu" MPISPAWN_GENERIC_NAME_79=LD_LIBRARY_PATH MPISPAWN_GENERIC_VALUE_79="/opt/apps/gcc4_7/mvapich2/1.9/lib:/opt/apps/gcc4_7/mvapich2/1.9/lib/shared:/opt/apps/gcc/4.7.1/lib64:/opt/apps/gcc/4.7.1/lib:/opt/apps/xsede/gsi-openssh-5.7/lib64:/opt/apps/xsede/gsi-openssh-5.7/lib64" MPISPAWN_GENERIC_NAME_80=SLURM_QUEUE MPISPAWN_GENERIC_VALUE_80="normal" MPISPAWN_GENERIC_NAME_81=PKG_CONFIG_PATH MPISPAWN_GENERIC_VALUE_81="/opt/apps/gcc4_7/mvapich2/1.9/lib/pkgconfig" MPISPAWN_GENERIC_NAME_82=LMOD_PREPEND_BLOCK MPISPAWN_GENERIC_VALUE_82="normal" MPISPAWN_GENERIC_NAME_83=TACC_FAMILY_COMPILER MPISPAWN_GENERIC_VALUE_83="gcc" MPISPAWN_GENERIC_NAME_84=TACC_DOMAIN MPISPAWN_GENERIC_VALUE_84="stampede" MPISPAWN_GENERIC_NAME_85=MV2_HOMOGENEOUS_CLUSTER MPISPAWN_GENERIC_VALUE_85="1" MPISPAWN_GENERIC_NAME_86=LMOD_FAMILY_COMPILER MPISPAWN_GENERIC_VALUE_86="gcc" MPISPAWN_GENERIC_NAME_87=MV2_USE_UD_HYBRID MPISPAWN_GENERIC_VALUE_87="0" MPISPAWN_GENERIC_NAME_88=MPICH_HOME MPISPAWN_GENERIC_VALUE_88="/opt/apps/gcc4_7/mvapich2/1.9" MPISPAWN_GENERIC_NAME_89=SLURM_JOB_NODELIST MPISPAWN_GENERIC_VALUE_89="c468-601,c471-503,c472-[401-402],c473-[303-304,401,804,901],c474-[203-204,301,401,403-404],c478-303,c482-404,c483-[001-004,101,504,601,804,901],c484-[101,103-104,201,602,701],c485-502,c486-102,c490-[302,904],c491-004,c492-701,c493-[001,102,201],c494-[101,402-403,602,702-703,904],c495-[001-002,201],c497-[102,202,504,601-603],c498-[304,402,603],c501-[204,301,604,801],c502-104,c503-901,c504-[602,604,702,801],c505-[001,202-204,401,502-504,802,901-904],c506-[302,704],c507-[102,202,503,601,904],c508-[101,204,601,702-703,802,903-904],c509-[002,201,301,303,401,501-502],c510-[003,401,404,501,503],c511-[601-602,902-903],c536-[801,902],c537-902,c538-[001,003,104],c540-[104,201-202,804,901-904]" MPISPAWN_GENERIC_NAME_90=SLURM_PROCID MPISPAWN_GENERIC_VALUE_90="0" MPISPAWN_GENERIC_NAME_91=I_MPI_DAPL_PROVIDER MPISPAWN_GENERIC_VALUE_91="ofa-v2-mlx4_0-1u" MPISPAWN_GENERIC_NAME_92=GLOBUS_LOCATION MPISPAWN_GENERIC_VALUE_92="/opt/apps/xsede/gsi-openssh-5.7" MPISPAWN_GENERIC_NAME_93=LMOD_COLORIZE MPISPAWN_GENERIC_VALUE_93="yes" MPISPAWN_GENERIC_NAME_94=I_MPI_HYDRA_BRANCH_COUNT MPISPAWN_GENERIC_VALUE_94="6000" MPISPAWN_GENERIC_NAME_95=SLURM_NNODES MPISPAWN_GENERIC_VALUE_95="128" MPISPAWN_GENERIC_NAME_96=MV2_DEFAULT_TIME_OUT MPISPAWN_GENERIC_VALUE_96="23" MPISPAWN_GENERIC_NAME_97=SLURM_TACC_NCORES_SET MPISPAWN_GENERIC_VALUE_97="1" MPISPAWN_GENERIC_NAME_98=__Init_Default_Modules MPISPAWN_GENERIC_VALUE_98="1" MPISPAWN_GENERIC_NAME_99=BASH_ENV MPISPAWN_GENERIC_VALUE_99="/etc/tacc/tacc_functions" MPISPAWN_GENERIC_NAME_100=SLURM_TOPOLOGY_ADDR_PATTERN MPISPAWN_GENERIC_VALUE_100="node" MPISPAWN_GENERIC_NAME_101=MV2_USE_RING_STARTUP MPISPAWN_GENERIC_VALUE_101="0" MPISPAWN_GENERIC_NAME_102=TACC_XALT_BIN MPISPAWN_GENERIC_VALUE_102="/opt/apps/xalt/0.4.4/bin" MPISPAWN_GENERIC_NAME_103=X509_USER_PROXY MPISPAWN_GENERIC_VALUE_103="/tmp/x509up_p3084.file9zM3fu.1" MPISPAWN_GENERIC_NAME_104=TMPDIR MPISPAWN_GENERIC_VALUE_104="/tmp" MPISPAWN_GENERIC_NAME_105=LMOD_FULL_SETTARG_SUPPORT MPISPAWN_GENERIC_VALUE_105="no" MPISPAWN_ID=13 MPISPAWN_WORKING_DIR=/home1/03222/tg825207/parallelnbody MPISPAWN_MPIRUN_RANK_0=208 MPISPAWN_MPIRUN_RANK_1=209 MPISPAWN_MPIRUN_RANK_2=210 MPISPAWN_MPIRUN_RANK_3=211 MPISPAWN_MPIRUN_RANK_4=212 MPISPAWN_MPIRUN_RANK_5=213 MPISPAWN_MPIRUN_RANK_6=214 MPISPAWN_MPIRUN_RANK_7=215 MPISPAWN_MPIRUN_RANK_8=216 MPISPAWN_MPIRUN_RANK_9=217 MPISPAWN_MPIRUN_RANK_10=218 MPISPAWN_MPIRUN_RANK_11=219 MPISPAWN_MPIRUN_RANK_12=220 MPISPAWN_MPIRUN_RANK_13=221 MPISPAWN_MPIRUN_RANK_14=222 MPISPAWN_MPIRUN_RANK_15=223 /opt/apps/gcc4_7/mvapich2/1.9/bin/mpispawn 0
bash: line 1: 12519 Segmentation fault      /usr/bin/env LD_LIBRARY_PATH=/opt/apps/gcc4_7/mvapich2/1.9/lib:/opt/apps/gcc4_7/mvapich2/1.9/lib/shared:/opt/apps/gcc/4.7.1/lib64:/opt/apps/gcc/4.7.1/lib:/opt/apps/xsede/gsi-openssh-5.7/lib64:/opt/apps/xsede/gsi-openssh-5.7/lib64 MPISPAWN_MPIRUN_MPD=0 USE_LINEAR_SSH=1 MPISPAWN_MPIRUN_HOST=c468-601.stampede.tacc.utexas.edu MPISPAWN_MPIRUN_HOSTIP=129.114.76.165 MPIRUN_RSH_LAUNCH=1 MPISPAWN_CHECKIN_PORT=36101 MPISPAWN_MPIRUN_PORT=36101 MPISPAWN_NNODES=64 MPISPAWN_GLOBAL_NPROCS=1024 MPISPAWN_MPIRUN_ID=44295 MPISPAWN_ARGC=4 MPDMAN_KVS_TEMPLATE=kvs_513_c468-601.stampede.tacc.utexas.edu_44295 MPISPAWN_LOCAL_NPROCS=16 MPISPAWN_ARGV_0='/home1/03222/tg825207/parallelnbody/teamscatter' MPISPAWN_ARGV_1='256000' MPISPAWN_ARGV_2='-c' MPISPAWN_ARGV_3='2' MPISPAWN_ARGC=4 MPISPAWN_GENERIC_ENV_COUNT=106 MPISPAWN_GENERIC_NAME_0=TACC_ENVLEN MPISPAWN_GENERIC_VALUE_0=6578 MPISPAWN_GENERIC_NAME_1=SLURM_JOB_ID MPISPAWN_GENERIC_VALUE_1="4306264" MPISPAWN_GENERIC_NAME_2=SLURMD_NODENAME MPISPAWN_GENERIC_VALUE_2="c468-601" MPISPAWN_GENERIC_NAME_3=STOCKYARD MPISPAWN_GENERIC_VALUE_3="/work/03222/tg825207" MPISPAWN_GENERIC_NAME_4=LC_ALL MPISPAWN_GENERIC_VALUE_4="en_US.UTF-8" MPISPAWN_GENERIC_NAME_5=SLURM_CHECKPOINT_IMAGE_DIR MPISPAWN_GENERIC_VALUE_5="/home1/03222/tg825207/parallelnbody" MPISPAWN_GENERIC_NAME_6=LMOD_FAMILY_MPI MPISPAWN_GENERIC_VALUE_6="mvapich2" MPISPAWN_GENERIC_NAME_7=SLURM_JOBID MPISPAWN_GENERIC_VALUE_7="4306264" MPISPAWN_GENERIC_NAME_8=_ModuleTable001_ MPISPAWN_GENERIC_VALUE_8="X01vZHVsZVRhYmxlXz17WyJhY3RpdmVTaXplIl09OCxiYXNlTXBhdGhBPXsiL29wdC9hcHBzL3hzZWRlL21vZHVsZWZpbGVzIiwiL29wdC9hcHBzL21vZHVsZWZpbGVzIiwiL29wdC9tb2R1bGVmaWxlcyIsfSxbImNfcmVidWlsZFRpbWUiXT1mYWxzZSxbImNfc2hvcnRUaW1lIl09ZmFsc2UsZmFtaWx5PXtbIk1QSSJdPSJtdmFwaWNoMiIsWyJjb21waWxlciJdPSJnY2MiLH0saW5hY3RpdmU9e30sbVQ9e0xpbnV4PXtbIkZOIl09Ii9vcHQvbW9kdWxlZmlsZXMvTGludXgiLFsiZGVmYXVsdCJdPTAsWyJmdWxsTmFtZSJdPSJMaW51eCIsWyJsb2FkT3JkZXIiXT0yLHByb3BUPXt9LFsic2hvcnQiXT0iTGludXgiLFsic3RhdHVzIl09ImFjdGl2ZSIsfSxUQUNDPXtbIkZOIl09Ii9v" MPISPAWN_GENERIC_NAME_9=TACC_SYSTEM MPISPAWN_GENERIC_VALUE_9="stampede" MPISPAWN_GENERIC_NAME_10=WORK MPISPAWN_GENERIC_VALUE_10="/work/03222/tg825207" MPISPAWN_GENERIC_NAME_11=SLURM_GTIDS MPISPAWN_GENERIC_VALUE_11="0" MPISPAWN_GENERIC_NAME_12=GCC_LIB MPISPAWN_GENERIC_VALUE_12="/opt/apps/gcc/4.7.1/lib64" MPISPAWN_GENERIC_NAME_13=SLURM_PRIO_PROCESS MPISPAWN_GENERIC_VALUE_13="0" MPISPAWN_GENERIC_NAME_14=SCRATCH MPISPAWN_GENERIC_VALUE_14="/scratch/03222/tg825207" MPISPAWN_GENERIC_NAME_15=LOGNAME MPISPAWN_GENERIC_VALUE_15="tg825207" MPISPAWN_GENERIC_NAME_16=_ModuleTable002_ MPISPAWN_GENERIC_VALUE_16="cHQvbW9kdWxlZmlsZXMvVEFDQyIsWyJkZWZhdWx0Il09MCxbImZ1bGxOYW1lIl09IlRBQ0MiLFsibG9hZE9yZGVyIl09Nixwcm9wVD17fSxbInNob3J0Il09IlRBQ0MiLFsic3RhdHVzIl09ImFjdGl2ZSIsfSxbIlRBQ0MtcGF0aHMiXT17WyJGTiJdPSIvb3B0L21vZHVsZWZpbGVzL1RBQ0MtcGF0aHMiLFsiZGVmYXVsdCJdPTAsWyJmdWxsTmFtZSJdPSJUQUNDLXBhdGhzIixbImxvYWRPcmRlciJdPTEscHJvcFQ9e30sWyJzaG9ydCJdPSJUQUNDLXBhdGhzIixbInN0YXR1cyJdPSJhY3RpdmUiLH0sY2x1c3Rlcj17WyJGTiJdPSIvb3B0L21vZHVsZWZpbGVzL2NsdXN0ZXIiLFsiZGVmYXVsdCJdPTAsWyJmdWxsTmFtZSJdPSJjbHVzdGVyIixbImxvYWRPcmRlciJdPTUscHJvcFQ9" MPISPAWN_GENERIC_NAME_17=INPUTRC MPISPAWN_GENERIC_VALUE_17="/etc/inputrc" MPISPAWN_GENERIC_NAME_18=LMOD_CMD MPISPAWN_GENERIC_VALUE_18="/opt/apps/lmod/lmod/libexec/lmod" MPISPAWN_GENERIC_NAME_19=I_MPI_OFA_ADAPTER_NAME MPISPAWN_GENERIC_VALUE_19="mlx4_0" MPISPAWN_GENERIC_NAME_20=QTLIB MPISPAWN_GENERIC_VALUE_20="/usr/lib64/qt-3.3/lib" MPISPAWN_GENERIC_NAME_21=NODE_TASKS_PPN_INFO MPISPAWN_GENERIC_VALUE_21=""16,0_"" MPISPAWN_GENERIC_NAME_22=SLURM_CPUS_ON_NODE MPISPAWN_GENERIC_VALUE_22="16" MPISPAWN_GENERIC_NAME_23=PATH MPISPAWN_GENERIC_VALUE_23="/opt/apps/gcc4_7/mvapich2/1.9/bin:/opt/apps/gcc/4.7.1/bin:/opt/apps/xalt/0.4.4/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/opt/apps/xsede/gsi-openssh-5.7/bin:/usr/X11R6/bin:/opt/ofed/bin:/opt/ofed/sbin:." MPISPAWN_GENERIC_NAME_24=SLURM_TACC_JOBNAME MPISPAWN_GENERIC_VALUE_24="stampedeLargeScaleAllTS.sh" MPISPAWN_GENERIC_NAME_25=SLURM_NODEID MPISPAWN_GENERIC_VALUE_25="0" MPISPAWN_GENERIC_NAME_26=LMOD_PKG MPISPAWN_GENERIC_VALUE_26="/opt/apps/lmod/lmod" MPISPAWN_GENERIC_NAME_27=MIC_I_MPI_OFA_ADAPTER_NAME MPISPAWN_GENERIC_VALUE_27="mlx4_0" MPISPAWN_GENERIC_NAME_28=HISTSIZE MPISPAWN_GENERIC_VALUE_28="1000" MPISPAWN_GENERIC_NAME_29=SLURM_NTASKS MPISPAWN_GENERIC_VALUE_29="2048" MPISPAWN_GENERIC_NAME_30=SHLIB_PATH MPISPAWN_GENERIC_VALUE_30="/opt/apps/xsede/gsi-openssh-5.7/lib64" MPISPAWN_GENERIC_NAME_31=TACC_OVERRIDE_PROJECT MPISPAWN_GENERIC_VALUE_31="TG-ASC140020" MPISPAWN_GENERIC_NAME_32=_ModuleTable005_ MPISPAWN_GENERIC_VALUE_32="Z2NjNF83L21vZHVsZWZpbGVzIiwiL29wdC9hcHBzL3hzZWRlL21vZHVsZWZpbGVzIiwiL29wdC9hcHBzL21vZHVsZWZpbGVzIiwiL29wdC9tb2R1bGVmaWxlcyIsfSxbInN5c3RlbUJhc2VNUEFUSCJdPSIvb3B0L2FwcHMveHNlZGUvbW9kdWxlZmlsZXM6L29wdC9hcHBzL21vZHVsZWZpbGVzOi9vcHQvbW9kdWxlZmlsZXMiLFsidmVyc2lvbiJdPTIsfQ==" MPISPAWN_GENERIC_NAME_33=_ModuleTable_Sz_ MPISPAWN_GENERIC_VALUE_33="5" MPISPAWN_GENERIC_NAME_34=MAIL MPISPAWN_GENERIC_VALUE_34="/var/spool/mail/tg825207" MPISPAWN_GENERIC_NAME_35=MV2_USE_HUGEPAGES MPISPAWN_GENERIC_VALUE_35="0" MPISPAWN_GENERIC_NAME_36=SLURM_TOPOLOGY_ADDR MPISPAWN_GENERIC_VALUE_36="c468-601" MPISPAWN_GENERIC_NAME_37=TACC_XALT_DIR MPISPAWN_GENERIC_VALUE_37="/opt/apps/xalt/0.4.4/" MPISPAWN_GENERIC_NAME_38=QTDIR MPISPAWN_GENERIC_VALUE_38="/usr/lib64/qt-3.3" MPISPAWN_GENERIC_NAME_39=SYSTEM MPISPAWN_GENERIC_VALUE_39="linux" MPISPAWN_GENERIC_NAME_40=SLURM_TACC_RUNLIMIT_MINS MPISPAWN_GENERIC_VALUE_40="180" MPISPAWN_GENERIC_NAME_41=SLURM_SUBMIT_DIR MPISPAWN_GENERIC_VALUE_41="/home1/03222/tg825207/parallelnbody" MPISPAWN_GENERIC_NAME_42=LIBPATH MPISPAWN_GENERIC_VALUE_42="/opt/apps/xsede/gsi-openssh-5.7/lib64:/usr/lib:/lib" MPISPAWN_GENERIC_NAME_43=HOSTNAME MPISPAWN_GENERIC_VALUE_43="c468-601" MPISPAWN_GENERIC_NAME_44=LMOD_DEFAULT_MODULEPATH MPISPAWN_GENERIC_VALUE_44="/opt/apps/xsede/modulefiles:/opt/apps/modulefiles:/opt/modulefiles" MPISPAWN_GENERIC_NAME_45=ARCHIVE MPISPAWN_GENERIC_VALUE_45="/home/03222/tg825207" MPISPAWN_GENERIC_NAME_46=_ MPISPAWN_GENERIC_VALUE_46="/usr/local/bin/build_env.pl" MPISPAWN_GENERIC_NAME_47=__BASHRC_SOURCED__ MPISPAWN_GENERIC_VALUE_47="1" MPISPAWN_GENERIC_NAME_48=APPS MPISPAWN_GENERIC_VALUE_48="/opt/apps" MPISPAWN_GENERIC_NAME_49=SHELL MPISPAWN_GENERIC_VALUE_49="/bin/bash" MPISPAWN_GENERIC_NAME_50=ARMCI_OPENIB_DEVICE MPISPAWN_GENERIC_VALUE_50="mlx4_0" MPISPAWN_GENERIC_NAME_51=ENVIRONMENT MPISPAWN_GENERIC_VALUE_51="BATCH" MPISPAWN_GENERIC_NAME_52=TACC_FAMILY_MPI MPISPAWN_GENERIC_VALUE_52="mvapich2" MPISPAWN_GENERIC_NAME_53=MV2_IBA_HCA MPISPAWN_GENERIC_VALUE_53="mlx4_0" MPISPAWN_GENERIC_NAME_54=TACC_PUBLIC_MACHINE MPISPAWN_GENERIC_VALUE_54="1" MPISPAWN_GENERIC_NAME_55=SLURM_TACC_NODES MPISPAWN_GENERIC_VALUE_55="1" MPISPAWN_GENERIC_NAME_56=_ModuleTable003_ MPISPAWN_GENERIC_VALUE_56="e30sWyJzaG9ydCJdPSJjbHVzdGVyIixbInN0YXR1cyJdPSJhY3RpdmUiLH0sWyJjbHVzdGVyLXBhdGhzIl09e1siRk4iXT0iL29wdC9tb2R1bGVmaWxlcy9jbHVzdGVyLXBhdGhzIixbImRlZmF1bHQiXT0wLFsiZnVsbE5hbWUiXT0iY2x1c3Rlci1wYXRocyIsWyJsb2FkT3JkZXIiXT0zLHByb3BUPXt9LFsic2hvcnQiXT0iY2x1c3Rlci1wYXRocyIsWyJzdGF0dXMiXT0iYWN0aXZlIix9LGdjYz17WyJGTiJdPSIvb3B0L2FwcHMvbW9kdWxlZmlsZXMvZ2NjLzQuNy4xLmx1YSIsWyJkZWZhdWx0Il09MCxbImZ1bGxOYW1lIl09ImdjYy80LjcuMSIsWyJsb2FkT3JkZXIiXT03LHByb3BUPXt9LFsic2hvcnQiXT0iZ2NjIixbInN0YXR1cyJdPSJhY3RpdmUiLH0sbXZhcGljaDI9e1si" MPISPAWN_GENERIC_NAME_57=QTINC MPISPAWN_GENERIC_VALUE_57="/usr/lib64/qt-3.3/include" MPISPAWN_GENERIC_NAME_58=CVS_RSH MPISPAWN_GENERIC_VALUE_58="ssh" MPISPAWN_GENERIC_NAME_59=MV2_USE_OLD_BCAST MPISPAWN_GENERIC_VALUE_59="1" MPISPAWN_GENERIC_NAME_60=SLURM_JOB_NUM_NODES MPISPAWN_GENERIC_VALUE_60="128" MPISPAWN_GENERIC_NAME_61=SLURM_TASK_PID MPISPAWN_GENERIC_VALUE_61="31479" MPISPAWN_GENERIC_NAME_62=TACC_MPI_GETMODE MPISPAWN_GENERIC_VALUE_62="mvapich2_ssh" MPISPAWN_GENERIC_NAME_63=LANG MPISPAWN_GENERIC_VALUE_63="en_US.UTF-8" MPISPAWN_GENERIC_NAME_64=USER MPISPAWN_GENERIC_VALUE_64="tg825207" MPISPAWN_GENERIC_NAME_65=MIC_I_MPI_DAPL_PROVIDER MPISPAWN_GENERIC_VALUE_65="ofa-v2-mlx4_0-1u" MPISPAWN_GENERIC_NAME_66=G_BROKEN_FILENAMES MPISPAWN_GENERIC_VALUE_66="1" MPISPAWN_GENERIC_NAME_67=SLURM_LOCALID MPISPAWN_GENERIC_VALUE_67="0" MPISPAWN_GENERIC_NAME_68=SLURM_NPROCS MPISPAWN_GENERIC_VALUE_68="2048" MPISPAWN_GENERIC_NAME_69=SLURM_TACC_NNODES_SET MPISPAWN_GENERIC_VALUE_69="0" MPISPAWN_GENERIC_NAME_70=SLURM_TACC_CORES MPISPAWN_GENERIC_VALUE_70="2048" MPISPAWN_GENERIC_NAME_71=LMOD_DIR MPISPAWN_GENERIC_VALUE_71="/opt/apps/lmod/lmod/libexec" MPISPAWN_GENERIC_NAME_72=LMOD_SETTARG_CMD MPISPAWN_GENERIC_VALUE_72=":" MPISPAWN_GENERIC_NAME_73=OMP_NUM_THREADS MPISPAWN_GENERIC_VALUE_73="1" MPISPAWN_GENERIC_NAME_74=_ModuleTable004_ MPISPAWN_GENERIC_VALUE_74="Rk4iXT0iL29wdC9hcHBzL2djYzRfNy9tb2R1bGVmaWxlcy9tdmFwaWNoMi8xLjlhMiIsWyJkZWZhdWx0Il09MSxbImZ1bGxOYW1lIl09Im12YXBpY2gyLzEuOWEyIixbImxvYWRPcmRlciJdPTgscHJvcFQ9e30sWyJzaG9ydCJdPSJtdmFwaWNoMiIsWyJzdGF0dXMiXT0iYWN0aXZlIix9LHhhbHQ9e1siRk4iXT0iL29wdC9hcHBzL21vZHVsZWZpbGVzL3hhbHQvMC40LjQubHVhIixbImRlZmF1bHQiXT0xLFsiZnVsbE5hbWUiXT0ieGFsdC8wLjQuNCIsWyJsb2FkT3JkZXIiXT00LHByb3BUPXt9LFsic2hvcnQiXT0ieGFsdCIsWyJzdGF0dXMiXT0iYWN0aXZlIix9LH0sbXBhdGhBPXsiL29wdC9hcHBzL2djYzRfNy9tdmFwaWNoMl8xXzkvbW9kdWxlZmlsZXMiLCIvb3B0L2FwcHMv" MPISPAWN_GENERIC_NAME_75=__PERSONAL_PATH__ MPISPAWN_GENERIC_VALUE_75="1" MPISPAWN_GENERIC_NAME_76=LMOD_SYSTEM_DEFAULT_MODULES MPISPAWN_GENERIC_VALUE_76="TACC" MPISPAWN_GENERIC_NAME_77=SLURM_JOB_NAME MPISPAWN_GENERIC_VALUE_77="stampedeLargeScaleAllTS.sh" MPISPAWN_GENERIC_NAME_78=SLURM_SUBMIT_HOST MPISPAWN_GENERIC_VALUE_78="login4.stampede.tacc.utexas.edu" MPISPAWN_GENERIC_NAME_79=LD_LIBRARY_PATH MPISPAWN_GENERIC_VALUE_79="/opt/apps/gcc4_7/mvapich2/1.9/lib:/opt/apps/gcc4_7/mvapich2/1.9/lib/shared:/opt/apps/gcc/4.7.1/lib64:/opt/apps/gcc/4.7.1/lib:/opt/apps/xsede/gsi-openssh-5.7/lib64:/opt/apps/xsede/gsi-openssh-5.7/lib64" MPISPAWN_GENERIC_NAME_80=SLURM_QUEUE MPISPAWN_GENERIC_VALUE_80="normal" MPISPAWN_GENERIC_NAME_81=PKG_CONFIG_PATH MPISPAWN_GENERIC_VALUE_81="/opt/apps/gcc4_7/mvapich2/1.9/lib/pkgconfig" MPISPAWN_GENERIC_NAME_82=LMOD_PREPEND_BLOCK MPISPAWN_GENERIC_VALUE_82="normal" MPISPAWN_GENERIC_NAME_83=TACC_FAMILY_COMPILER MPISPAWN_GENERIC_VALUE_83="gcc" MPISPAWN_GENERIC_NAME_84=TACC_DOMAIN MPISPAWN_GENERIC_VALUE_84="stampede" MPISPAWN_GENERIC_NAME_85=MV2_HOMOGENEOUS_CLUSTER MPISPAWN_GENERIC_VALUE_85="1" MPISPAWN_GENERIC_NAME_86=LMOD_FAMILY_COMPILER MPISPAWN_GENERIC_VALUE_86="gcc" MPISPAWN_GENERIC_NAME_87=MV2_USE_UD_HYBRID MPISPAWN_GENERIC_VALUE_87="0" MPISPAWN_GENERIC_NAME_88=MPICH_HOME MPISPAWN_GENERIC_VALUE_88="/opt/apps/gcc4_7/mvapich2/1.9" MPISPAWN_GENERIC_NAME_89=SLURM_JOB_NODELIST MPISPAWN_GENERIC_VALUE_89="c468-601,c471-503,c472-[401-402],c473-[303-304,401,804,901],c474-[203-204,301,401,403-404],c478-303,c482-404,c483-[001-004,101,504,601,804,901],c484-[101,103-104,201,602,701],c485-502,c486-102,c490-[302,904],c491-004,c492-701,c493-[001,102,201],c494-[101,402-403,602,702-703,904],c495-[001-002,201],c497-[102,202,504,601-603],c498-[304,402,603],c501-[204,301,604,801],c502-104,c503-901,c504-[602,604,702,801],c505-[001,202-204,401,502-504,802,901-904],c506-[302,704],c507-[102,202,503,601,904],c508-[101,204,601,702-703,802,903-904],c509-[002,201,301,303,401,501-502],c510-[003,401,404,501,503],c511-[601-602,902-903],c536-[801,902],c537-902,c538-[001,003,104],c540-[104,201-202,804,901-904]" MPISPAWN_GENERIC_NAME_90=SLURM_PROCID MPISPAWN_GENERIC_VALUE_90="0" MPISPAWN_GENERIC_NAME_91=I_MPI_DAPL_PROVIDER MPISPAWN_GENERIC_VALUE_91="ofa-v2-mlx4_0-1u" MPISPAWN_GENERIC_NAME_92=GLOBUS_LOCATION MPISPAWN_GENERIC_VALUE_92="/opt/apps/xsede/gsi-openssh-5.7" MPISPAWN_GENERIC_NAME_93=LMOD_COLORIZE MPISPAWN_GENERIC_VALUE_93="yes" MPISPAWN_GENERIC_NAME_94=I_MPI_HYDRA_BRANCH_COUNT MPISPAWN_GENERIC_VALUE_94="6000" MPISPAWN_GENERIC_NAME_95=SLURM_NNODES MPISPAWN_GENERIC_VALUE_95="128" MPISPAWN_GENERIC_NAME_96=MV2_DEFAULT_TIME_OUT MPISPAWN_GENERIC_VALUE_96="23" MPISPAWN_GENERIC_NAME_97=SLURM_TACC_NCORES_SET MPISPAWN_GENERIC_VALUE_97="1" MPISPAWN_GENERIC_NAME_98=__Init_Default_Modules MPISPAWN_GENERIC_VALUE_98="1" MPISPAWN_GENERIC_NAME_99=BASH_ENV MPISPAWN_GENERIC_VALUE_99="/etc/tacc/tacc_functions" MPISPAWN_GENERIC_NAME_100=SLURM_TOPOLOGY_ADDR_PATTERN MPISPAWN_GENERIC_VALUE_100="node" MPISPAWN_GENERIC_NAME_101=MV2_USE_RING_STARTUP MPISPAWN_GENERIC_VALUE_101="0" MPISPAWN_GENERIC_NAME_102=TACC_XALT_BIN MPISPAWN_GENERIC_VALUE_102="/opt/apps/xalt/0.4.4/bin" MPISPAWN_GENERIC_NAME_103=X509_USER_PROXY MPISPAWN_GENERIC_VALUE_103="/tmp/x509up_p3084.file9zM3fu.1" MPISPAWN_GENERIC_NAME_104=TMPDIR MPISPAWN_GENERIC_VALUE_104="/tmp" MPISPAWN_GENERIC_NAME_105=LMOD_FULL_SETTARG_SUPPORT MPISPAWN_GENERIC_VALUE_105="no" MPISPAWN_ID=11 MPISPAWN_WORKING_DIR=/home1/03222/tg825207/parallelnbody MPISPAWN_MPIRUN_RANK_0=176 MPISPAWN_MPIRUN_RANK_1=177 MPISPAWN_MPIRUN_RANK_2=178 MPISPAWN_MPIRUN_RANK_3=179 MPISPAWN_MPIRUN_RANK_4=180 MPISPAWN_MPIRUN_RANK_5=181 MPISPAWN_MPIRUN_RANK_6=182 MPISPAWN_MPIRUN_RANK_7=183 MPISPAWN_MPIRUN_RANK_8=184 MPISPAWN_MPIRUN_RANK_9=185 MPISPAWN_MPIRUN_RANK_10=186 MPISPAWN_MPIRUN_RANK_11=187 MPISPAWN_MPIRUN_RANK_12=188 MPISPAWN_MPIRUN_RANK_13=189 MPISPAWN_MPIRUN_RANK_14=190 MPISPAWN_MPIRUN_RANK_15=191 /opt/apps/gcc4_7/mvapich2/1.9/bin/mpispawn 0
[c483-003.stampede.tacc.utexas.edu:mpispawn_19][error_sighandler] Caught error: Segmentation fault (signal 11)
[c484-104.stampede.tacc.utexas.edu:mpispawn_28][error_sighandler] Caught error: Segmentation fault (signal 11)
bash: line 1: 87876 Segmentation fault      /usr/bin/env LD_LIBRARY_PATH=/opt/apps/gcc4_7/mvapich2/1.9/lib:/opt/apps/gcc4_7/mvapich2/1.9/lib/shared:/opt/apps/gcc/4.7.1/lib64:/opt/apps/gcc/4.7.1/lib:/opt/apps/xsede/gsi-openssh-5.7/lib64:/opt/apps/xsede/gsi-openssh-5.7/lib64 MPISPAWN_MPIRUN_MPD=0 USE_LINEAR_SSH=1 MPISPAWN_MPIRUN_HOST=c468-601.stampede.tacc.utexas.edu MPISPAWN_MPIRUN_HOSTIP=129.114.76.165 MPIRUN_RSH_LAUNCH=1 MPISPAWN_CHECKIN_PORT=36101 MPISPAWN_MPIRUN_PORT=36101 MPISPAWN_NNODES=64 MPISPAWN_GLOBAL_NPROCS=1024 MPISPAWN_MPIRUN_ID=44295 MPISPAWN_ARGC=4 MPDMAN_KVS_TEMPLATE=kvs_513_c468-601.stampede.tacc.utexas.edu_44295 MPISPAWN_LOCAL_NPROCS=16 MPISPAWN_ARGV_0='/home1/03222/tg825207/parallelnbody/teamscatter' MPISPAWN_ARGV_1='256000' MPISPAWN_ARGV_2='-c' MPISPAWN_ARGV_3='2' MPISPAWN_ARGC=4 MPISPAWN_GENERIC_ENV_COUNT=106 MPISPAWN_GENERIC_NAME_0=TACC_ENVLEN MPISPAWN_GENERIC_VALUE_0=6578 MPISPAWN_GENERIC_NAME_1=SLURM_JOB_ID MPISPAWN_GENERIC_VALUE_1="4306264" MPISPAWN_GENERIC_NAME_2=SLURMD_NODENAME MPISPAWN_GENERIC_VALUE_2="c468-601" MPISPAWN_GENERIC_NAME_3=STOCKYARD MPISPAWN_GENERIC_VALUE_3="/work/03222/tg825207" MPISPAWN_GENERIC_NAME_4=LC_ALL MPISPAWN_GENERIC_VALUE_4="en_US.UTF-8" MPISPAWN_GENERIC_NAME_5=SLURM_CHECKPOINT_IMAGE_DIR MPISPAWN_GENERIC_VALUE_5="/home1/03222/tg825207/parallelnbody" MPISPAWN_GENERIC_NAME_6=LMOD_FAMILY_MPI MPISPAWN_GENERIC_VALUE_6="mvapich2" MPISPAWN_GENERIC_NAME_7=SLURM_JOBID MPISPAWN_GENERIC_VALUE_7="4306264" MPISPAWN_GENERIC_NAME_8=_ModuleTable001_ MPISPAWN_GENERIC_VALUE_8="X01vZHVsZVRhYmxlXz17WyJhY3RpdmVTaXplIl09OCxiYXNlTXBhdGhBPXsiL29wdC9hcHBzL3hzZWRlL21vZHVsZWZpbGVzIiwiL29wdC9hcHBzL21vZHVsZWZpbGVzIiwiL29wdC9tb2R1bGVmaWxlcyIsfSxbImNfcmVidWlsZFRpbWUiXT1mYWxzZSxbImNfc2hvcnRUaW1lIl09ZmFsc2UsZmFtaWx5PXtbIk1QSSJdPSJtdmFwaWNoMiIsWyJjb21waWxlciJdPSJnY2MiLH0saW5hY3RpdmU9e30sbVQ9e0xpbnV4PXtbIkZOIl09Ii9vcHQvbW9kdWxlZmlsZXMvTGludXgiLFsiZGVmYXVsdCJdPTAsWyJmdWxsTmFtZSJdPSJMaW51eCIsWyJsb2FkT3JkZXIiXT0yLHByb3BUPXt9LFsic2hvcnQiXT0iTGludXgiLFsic3RhdHVzIl09ImFjdGl2ZSIsfSxUQUNDPXtbIkZOIl09Ii9v" MPISPAWN_GENERIC_NAME_9=TACC_SYSTEM MPISPAWN_GENERIC_VALUE_9="stampede" MPISPAWN_GENERIC_NAME_10=WORK MPISPAWN_GENERIC_VALUE_10="/work/03222/tg825207" MPISPAWN_GENERIC_NAME_11=SLURM_GTIDS MPISPAWN_GENERIC_VALUE_11="0" MPISPAWN_GENERIC_NAME_12=GCC_LIB MPISPAWN_GENERIC_VALUE_12="/opt/apps/gcc/4.7.1/lib64" MPISPAWN_GENERIC_NAME_13=SLURM_PRIO_PROCESS MPISPAWN_GENERIC_VALUE_13="0" MPISPAWN_GENERIC_NAME_14=SCRATCH MPISPAWN_GENERIC_VALUE_14="/scratch/03222/tg825207" MPISPAWN_GENERIC_NAME_15=LOGNAME MPISPAWN_GENERIC_VALUE_15="tg825207" MPISPAWN_GENERIC_NAME_16=_ModuleTable002_ MPISPAWN_GENERIC_VALUE_16="cHQvbW9kdWxlZmlsZXMvVEFDQyIsWyJkZWZhdWx0Il09MCxbImZ1bGxOYW1lIl09IlRBQ0MiLFsibG9hZE9yZGVyIl09Nixwcm9wVD17fSxbInNob3J0Il09IlRBQ0MiLFsic3RhdHVzIl09ImFjdGl2ZSIsfSxbIlRBQ0MtcGF0aHMiXT17WyJGTiJdPSIvb3B0L21vZHVsZWZpbGVzL1RBQ0MtcGF0aHMiLFsiZGVmYXVsdCJdPTAsWyJmdWxsTmFtZSJdPSJUQUNDLXBhdGhzIixbImxvYWRPcmRlciJdPTEscHJvcFQ9e30sWyJzaG9ydCJdPSJUQUNDLXBhdGhzIixbInN0YXR1cyJdPSJhY3RpdmUiLH0sY2x1c3Rlcj17WyJGTiJdPSIvb3B0L21vZHVsZWZpbGVzL2NsdXN0ZXIiLFsiZGVmYXVsdCJdPTAsWyJmdWxsTmFtZSJdPSJjbHVzdGVyIixbImxvYWRPcmRlciJdPTUscHJvcFQ9" MPISPAWN_GENERIC_NAME_17=INPUTRC MPISPAWN_GENERIC_VALUE_17="/etc/inputrc" MPISPAWN_GENERIC_NAME_18=LMOD_CMD MPISPAWN_GENERIC_VALUE_18="/opt/apps/lmod/lmod/libexec/lmod" MPISPAWN_GENERIC_NAME_19=I_MPI_OFA_ADAPTER_NAME MPISPAWN_GENERIC_VALUE_19="mlx4_0" MPISPAWN_GENERIC_NAME_20=QTLIB MPISPAWN_GENERIC_VALUE_20="/usr/lib64/qt-3.3/lib" MPISPAWN_GENERIC_NAME_21=NODE_TASKS_PPN_INFO MPISPAWN_GENERIC_VALUE_21=""16,0_"" MPISPAWN_GENERIC_NAME_22=SLURM_CPUS_ON_NODE MPISPAWN_GENERIC_VALUE_22="16" MPISPAWN_GENERIC_NAME_23=PATH MPISPAWN_GENERIC_VALUE_23="/opt/apps/gcc4_7/mvapich2/1.9/bin:/opt/apps/gcc/4.7.1/bin:/opt/apps/xalt/0.4.4/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/opt/apps/xsede/gsi-openssh-5.7/bin:/usr/X11R6/bin:/opt/ofed/bin:/opt/ofed/sbin:." MPISPAWN_GENERIC_NAME_24=SLURM_TACC_JOBNAME MPISPAWN_GENERIC_VALUE_24="stampedeLargeScaleAllTS.sh" MPISPAWN_GENERIC_NAME_25=SLURM_NODEID MPISPAWN_GENERIC_VALUE_25="0" MPISPAWN_GENERIC_NAME_26=LMOD_PKG MPISPAWN_GENERIC_VALUE_26="/opt/apps/lmod/lmod" MPISPAWN_GENERIC_NAME_27=MIC_I_MPI_OFA_ADAPTER_NAME MPISPAWN_GENERIC_VALUE_27="mlx4_0" MPISPAWN_GENERIC_NAME_28=HISTSIZE MPISPAWN_GENERIC_VALUE_28="1000" MPISPAWN_GENERIC_NAME_29=SLURM_NTASKS MPISPAWN_GENERIC_VALUE_29="2048" MPISPAWN_GENERIC_NAME_30=SHLIB_PATH MPISPAWN_GENERIC_VALUE_30="/opt/apps/xsede/gsi-openssh-5.7/lib64" MPISPAWN_GENERIC_NAME_31=TACC_OVERRIDE_PROJECT MPISPAWN_GENERIC_VALUE_31="TG-ASC140020" MPISPAWN_GENERIC_NAME_32=_ModuleTable005_ MPISPAWN_GENERIC_VALUE_32="Z2NjNF83L21vZHVsZWZpbGVzIiwiL29wdC9hcHBzL3hzZWRlL21vZHVsZWZpbGVzIiwiL29wdC9hcHBzL21vZHVsZWZpbGVzIiwiL29wdC9tb2R1bGVmaWxlcyIsfSxbInN5c3RlbUJhc2VNUEFUSCJdPSIvb3B0L2FwcHMveHNlZGUvbW9kdWxlZmlsZXM6L29wdC9hcHBzL21vZHVsZWZpbGVzOi9vcHQvbW9kdWxlZmlsZXMiLFsidmVyc2lvbiJdPTIsfQ==" MPISPAWN_GENERIC_NAME_33=_ModuleTable_Sz_ MPISPAWN_GENERIC_VALUE_33="5" MPISPAWN_GENERIC_NAME_34=MAIL MPISPAWN_GENERIC_VALUE_34="/var/spool/mail/tg825207" MPISPAWN_GENERIC_NAME_35=MV2_USE_HUGEPAGES MPISPAWN_GENERIC_VALUE_35="0" MPISPAWN_GENERIC_NAME_36=SLURM_TOPOLOGY_ADDR MPISPAWN_GENERIC_VALUE_36="c468-601" MPISPAWN_GENERIC_NAME_37=TACC_XALT_DIR MPISPAWN_GENERIC_VALUE_37="/opt/apps/xalt/0.4.4/" MPISPAWN_GENERIC_NAME_38=QTDIR MPISPAWN_GENERIC_VALUE_38="/usr/lib64/qt-3.3" MPISPAWN_GENERIC_NAME_39=SYSTEM MPISPAWN_GENERIC_VALUE_39="linux" MPISPAWN_GENERIC_NAME_40=SLURM_TACC_RUNLIMIT_MINS MPISPAWN_GENERIC_VALUE_40="180" MPISPAWN_GENERIC_NAME_41=SLURM_SUBMIT_DIR MPISPAWN_GENERIC_VALUE_41="/home1/03222/tg825207/parallelnbody" MPISPAWN_GENERIC_NAME_42=LIBPATH MPISPAWN_GENERIC_VALUE_42="/opt/apps/xsede/gsi-openssh-5.7/lib64:/usr/lib:/lib" MPISPAWN_GENERIC_NAME_43=HOSTNAME MPISPAWN_GENERIC_VALUE_43="c468-601" MPISPAWN_GENERIC_NAME_44=LMOD_DEFAULT_MODULEPATH MPISPAWN_GENERIC_VALUE_44="/opt/apps/xsede/modulefiles:/opt/apps/modulefiles:/opt/modulefiles" MPISPAWN_GENERIC_NAME_45=ARCHIVE MPISPAWN_GENERIC_VALUE_45="/home/03222/tg825207" MPISPAWN_GENERIC_NAME_46=_ MPISPAWN_GENERIC_VALUE_46="/usr/local/bin/build_env.pl" MPISPAWN_GENERIC_NAME_47=__BASHRC_SOURCED__ MPISPAWN_GENERIC_VALUE_47="1" MPISPAWN_GENERIC_NAME_48=APPS MPISPAWN_GENERIC_VALUE_48="/opt/apps" MPISPAWN_GENERIC_NAME_49=SHELL MPISPAWN_GENERIC_VALUE_49="/bin/bash" MPISPAWN_GENERIC_NAME_50=ARMCI_OPENIB_DEVICE MPISPAWN_GENERIC_VALUE_50="mlx4_0" MPISPAWN_GENERIC_NAME_51=ENVIRONMENT MPISPAWN_GENERIC_VALUE_51="BATCH" MPISPAWN_GENERIC_NAME_52=TACC_FAMILY_MPI MPISPAWN_GENERIC_VALUE_52="mvapich2" MPISPAWN_GENERIC_NAME_53=MV2_IBA_HCA MPISPAWN_GENERIC_VALUE_53="mlx4_0" MPISPAWN_GENERIC_NAME_54=TACC_PUBLIC_MACHINE MPISPAWN_GENERIC_VALUE_54="1" MPISPAWN_GENERIC_NAME_55=SLURM_TACC_NODES MPISPAWN_GENERIC_VALUE_55="1" MPISPAWN_GENERIC_NAME_56=_ModuleTable003_ MPISPAWN_GENERIC_VALUE_56="e30sWyJzaG9ydCJdPSJjbHVzdGVyIixbInN0YXR1cyJdPSJhY3RpdmUiLH0sWyJjbHVzdGVyLXBhdGhzIl09e1siRk4iXT0iL29wdC9tb2R1bGVmaWxlcy9jbHVzdGVyLXBhdGhzIixbImRlZmF1bHQiXT0wLFsiZnVsbE5hbWUiXT0iY2x1c3Rlci1wYXRocyIsWyJsb2FkT3JkZXIiXT0zLHByb3BUPXt9LFsic2hvcnQiXT0iY2x1c3Rlci1wYXRocyIsWyJzdGF0dXMiXT0iYWN0aXZlIix9LGdjYz17WyJGTiJdPSIvb3B0L2FwcHMvbW9kdWxlZmlsZXMvZ2NjLzQuNy4xLmx1YSIsWyJkZWZhdWx0Il09MCxbImZ1bGxOYW1lIl09ImdjYy80LjcuMSIsWyJsb2FkT3JkZXIiXT03LHByb3BUPXt9LFsic2hvcnQiXT0iZ2NjIixbInN0YXR1cyJdPSJhY3RpdmUiLH0sbXZhcGljaDI9e1si" MPISPAWN_GENERIC_NAME_57=QTINC MPISPAWN_GENERIC_VALUE_57="/usr/lib64/qt-3.3/include" MPISPAWN_GENERIC_NAME_58=CVS_RSH MPISPAWN_GENERIC_VALUE_58="ssh" MPISPAWN_GENERIC_NAME_59=MV2_USE_OLD_BCAST MPISPAWN_GENERIC_VALUE_59="1" MPISPAWN_GENERIC_NAME_60=SLURM_JOB_NUM_NODES MPISPAWN_GENERIC_VALUE_60="128" MPISPAWN_GENERIC_NAME_61=SLURM_TASK_PID MPISPAWN_GENERIC_VALUE_61="31479" MPISPAWN_GENERIC_NAME_62=TACC_MPI_GETMODE MPISPAWN_GENERIC_VALUE_62="mvapich2_ssh" MPISPAWN_GENERIC_NAME_63=LANG MPISPAWN_GENERIC_VALUE_63="en_US.UTF-8" MPISPAWN_GENERIC_NAME_64=USER MPISPAWN_GENERIC_VALUE_64="tg825207" MPISPAWN_GENERIC_NAME_65=MIC_I_MPI_DAPL_PROVIDER MPISPAWN_GENERIC_VALUE_65="ofa-v2-mlx4_0-1u" MPISPAWN_GENERIC_NAME_66=G_BROKEN_FILENAMES MPISPAWN_GENERIC_VALUE_66="1" MPISPAWN_GENERIC_NAME_67=SLURM_LOCALID MPISPAWN_GENERIC_VALUE_67="0" MPISPAWN_GENERIC_NAME_68=SLURM_NPROCS MPISPAWN_GENERIC_VALUE_68="2048" MPISPAWN_GENERIC_NAME_69=SLURM_TACC_NNODES_SET MPISPAWN_GENERIC_VALUE_69="0" MPISPAWN_GENERIC_NAME_70=SLURM_TACC_CORES MPISPAWN_GENERIC_VALUE_70="2048" MPISPAWN_GENERIC_NAME_71=LMOD_DIR MPISPAWN_GENERIC_VALUE_71="/opt/apps/lmod/lmod/libexec" MPISPAWN_GENERIC_NAME_72=LMOD_SETTARG_CMD MPISPAWN_GENERIC_VALUE_72=":" MPISPAWN_GENERIC_NAME_73=OMP_NUM_THREADS MPISPAWN_GENERIC_VALUE_73="1" MPISPAWN_GENERIC_NAME_74=_ModuleTable004_ MPISPAWN_GENERIC_VALUE_74="Rk4iXT0iL29wdC9hcHBzL2djYzRfNy9tb2R1bGVmaWxlcy9tdmFwaWNoMi8xLjlhMiIsWyJkZWZhdWx0Il09MSxbImZ1bGxOYW1lIl09Im12YXBpY2gyLzEuOWEyIixbImxvYWRPcmRlciJdPTgscHJvcFQ9e30sWyJzaG9ydCJdPSJtdmFwaWNoMiIsWyJzdGF0dXMiXT0iYWN0aXZlIix9LHhhbHQ9e1siRk4iXT0iL29wdC9hcHBzL21vZHVsZWZpbGVzL3hhbHQvMC40LjQubHVhIixbImRlZmF1bHQiXT0xLFsiZnVsbE5hbWUiXT0ieGFsdC8wLjQuNCIsWyJsb2FkT3JkZXIiXT00LHByb3BUPXt9LFsic2hvcnQiXT0ieGFsdCIsWyJzdGF0dXMiXT0iYWN0aXZlIix9LH0sbXBhdGhBPXsiL29wdC9hcHBzL2djYzRfNy9tdmFwaWNoMl8xXzkvbW9kdWxlZmlsZXMiLCIvb3B0L2FwcHMv" MPISPAWN_GENERIC_NAME_75=__PERSONAL_PATH__ MPISPAWN_GENERIC_VALUE_75="1" MPISPAWN_GENERIC_NAME_76=LMOD_SYSTEM_DEFAULT_MODULES MPISPAWN_GENERIC_VALUE_76="TACC" MPISPAWN_GENERIC_NAME_77=SLURM_JOB_NAME MPISPAWN_GENERIC_VALUE_77="stampedeLargeScaleAllTS.sh" MPISPAWN_GENERIC_NAME_78=SLURM_SUBMIT_HOST MPISPAWN_GENERIC_VALUE_78="login4.stampede.tacc.utexas.edu" MPISPAWN_GENERIC_NAME_79=LD_LIBRARY_PATH MPISPAWN_GENERIC_VALUE_79="/opt/apps/gcc4_7/mvapich2/1.9/lib:/opt/apps/gcc4_7/mvapich2/1.9/lib/shared:/opt/apps/gcc/4.7.1/lib64:/opt/apps/gcc/4.7.1/lib:/opt/apps/xsede/gsi-openssh-5.7/lib64:/opt/apps/xsede/gsi-openssh-5.7/lib64" MPISPAWN_GENERIC_NAME_80=SLURM_QUEUE MPISPAWN_GENERIC_VALUE_80="normal" MPISPAWN_GENERIC_NAME_81=PKG_CONFIG_PATH MPISPAWN_GENERIC_VALUE_81="/opt/apps/gcc4_7/mvapich2/1.9/lib/pkgconfig" MPISPAWN_GENERIC_NAME_82=LMOD_PREPEND_BLOCK MPISPAWN_GENERIC_VALUE_82="normal" MPISPAWN_GENERIC_NAME_83=TACC_FAMILY_COMPILER MPISPAWN_GENERIC_VALUE_83="gcc" MPISPAWN_GENERIC_NAME_84=TACC_DOMAIN MPISPAWN_GENERIC_VALUE_84="stampede" MPISPAWN_GENERIC_NAME_85=MV2_HOMOGENEOUS_CLUSTER MPISPAWN_GENERIC_VALUE_85="1" MPISPAWN_GENERIC_NAME_86=LMOD_FAMILY_COMPILER MPISPAWN_GENERIC_VALUE_86="gcc" MPISPAWN_GENERIC_NAME_87=MV2_USE_UD_HYBRID MPISPAWN_GENERIC_VALUE_87="0" MPISPAWN_GENERIC_NAME_88=MPICH_HOME MPISPAWN_GENERIC_VALUE_88="/opt/apps/gcc4_7/mvapich2/1.9" MPISPAWN_GENERIC_NAME_89=SLURM_JOB_NODELIST MPISPAWN_GENERIC_VALUE_89="c468-601,c471-503,c472-[401-402],c473-[303-304,401,804,901],c474-[203-204,301,401,403-404],c478-303,c482-404,c483-[001-004,101,504,601,804,901],c484-[101,103-104,201,602,701],c485-502,c486-102,c490-[302,904],c491-004,c492-701,c493-[001,102,201],c494-[101,402-403,602,702-703,904],c495-[001-002,201],c497-[102,202,504,601-603],c498-[304,402,603],c501-[204,301,604,801],c502-104,c503-901,c504-[602,604,702,801],c505-[001,202-204,401,502-504,802,901-904],c506-[302,704],c507-[102,202,503,601,904],c508-[101,204,601,702-703,802,903-904],c509-[002,201,301,303,401,501-502],c510-[003,401,404,501,503],c511-[601-602,902-903],c536-[801,902],c537-902,c538-[001,003,104],c540-[104,201-202,804,901-904]" MPISPAWN_GENERIC_NAME_90=SLURM_PROCID MPISPAWN_GENERIC_VALUE_90="0" MPISPAWN_GENERIC_NAME_91=I_MPI_DAPL_PROVIDER MPISPAWN_GENERIC_VALUE_91="ofa-v2-mlx4_0-1u" MPISPAWN_GENERIC_NAME_92=GLOBUS_LOCATION MPISPAWN_GENERIC_VALUE_92="/opt/apps/xsede/gsi-openssh-5.7" MPISPAWN_GENERIC_NAME_93=LMOD_COLORIZE MPISPAWN_GENERIC_VALUE_93="yes" MPISPAWN_GENERIC_NAME_94=I_MPI_HYDRA_BRANCH_COUNT MPISPAWN_GENERIC_VALUE_94="6000" MPISPAWN_GENERIC_NAME_95=SLURM_NNODES MPISPAWN_GENERIC_VALUE_95="128" MPISPAWN_GENERIC_NAME_96=MV2_DEFAULT_TIME_OUT MPISPAWN_GENERIC_VALUE_96="23" MPISPAWN_GENERIC_NAME_97=SLURM_TACC_NCORES_SET MPISPAWN_GENERIC_VALUE_97="1" MPISPAWN_GENERIC_NAME_98=__Init_Default_Modules MPISPAWN_GENERIC_VALUE_98="1" MPISPAWN_GENERIC_NAME_99=BASH_ENV MPISPAWN_GENERIC_VALUE_99="/etc/tacc/tacc_functions" MPISPAWN_GENERIC_NAME_100=SLURM_TOPOLOGY_ADDR_PATTERN MPISPAWN_GENERIC_VALUE_100="node" MPISPAWN_GENERIC_NAME_101=MV2_USE_RING_STARTUP MPISPAWN_GENERIC_VALUE_101="0" MPISPAWN_GENERIC_NAME_102=TACC_XALT_BIN MPISPAWN_GENERIC_VALUE_102="/opt/apps/xalt/0.4.4/bin" MPISPAWN_GENERIC_NAME_103=X509_USER_PROXY MPISPAWN_GENERIC_VALUE_103="/tmp/x509up_p3084.file9zM3fu.1" MPISPAWN_GENERIC_NAME_104=TMPDIR MPISPAWN_GENERIC_VALUE_104="/tmp" MPISPAWN_GENERIC_NAME_105=LMOD_FULL_SETTARG_SUPPORT MPISPAWN_GENERIC_VALUE_105="no" MPISPAWN_ID=28 MPISPAWN_WORKING_DIR=/home1/03222/tg825207/parallelnbody MPISPAWN_MPIRUN_RANK_0=448 MPISPAWN_MPIRUN_RANK_1=449 MPISPAWN_MPIRUN_RANK_2=450 MPISPAWN_MPIRUN_RANK_3=451 MPISPAWN_MPIRUN_RANK_4=452 MPISPAWN_MPIRUN_RANK_5=453 MPISPAWN_MPIRUN_RANK_6=454 MPISPAWN_MPIRUN_RANK_7=455 MPISPAWN_MPIRUN_RANK_8=456 MPISPAWN_MPIRUN_RANK_9=457 MPISPAWN_MPIRUN_RANK_10=458 MPISPAWN_MPIRUN_RANK_11=459 MPISPAWN_MPIRUN_RANK_12=460 MPISPAWN_MPIRUN_RANK_13=461 MPISPAWN_MPIRUN_RANK_14=462 MPISPAWN_MPIRUN_RANK_15=463 /opt/apps/gcc4_7/mvapich2/1.9/bin/mpispawn 0
bash: line 1: 99025 Segmentation fault      /usr/bin/env LD_LIBRARY_PATH=/opt/apps/gcc4_7/mvapich2/1.9/lib:/opt/apps/gcc4_7/mvapich2/1.9/lib/shared:/opt/apps/gcc/4.7.1/lib64:/opt/apps/gcc/4.7.1/lib:/opt/apps/xsede/gsi-openssh-5.7/lib64:/opt/apps/xsede/gsi-openssh-5.7/lib64 MPISPAWN_MPIRUN_MPD=0 USE_LINEAR_SSH=1 MPISPAWN_MPIRUN_HOST=c468-601.stampede.tacc.utexas.edu MPISPAWN_MPIRUN_HOSTIP=129.114.76.165 MPIRUN_RSH_LAUNCH=1 MPISPAWN_CHECKIN_PORT=36101 MPISPAWN_MPIRUN_PORT=36101 MPISPAWN_NNODES=64 MPISPAWN_GLOBAL_NPROCS=1024 MPISPAWN_MPIRUN_ID=44295 MPISPAWN_ARGC=4 MPDMAN_KVS_TEMPLATE=kvs_513_c468-601.stampede.tacc.utexas.edu_44295 MPISPAWN_LOCAL_NPROCS=16 MPISPAWN_ARGV_0='/home1/03222/tg825207/parallelnbody/teamscatter' MPISPAWN_ARGV_1='256000' MPISPAWN_ARGV_2='-c' MPISPAWN_ARGV_3='2' MPISPAWN_ARGC=4 MPISPAWN_GENERIC_ENV_COUNT=106 MPISPAWN_GENERIC_NAME_0=TACC_ENVLEN MPISPAWN_GENERIC_VALUE_0=6578 MPISPAWN_GENERIC_NAME_1=SLURM_JOB_ID MPISPAWN_GENERIC_VALUE_1="4306264" MPISPAWN_GENERIC_NAME_2=SLURMD_NODENAME MPISPAWN_GENERIC_VALUE_2="c468-601" MPISPAWN_GENERIC_NAME_3=STOCKYARD MPISPAWN_GENERIC_VALUE_3="/work/03222/tg825207" MPISPAWN_GENERIC_NAME_4=LC_ALL MPISPAWN_GENERIC_VALUE_4="en_US.UTF-8" MPISPAWN_GENERIC_NAME_5=SLURM_CHECKPOINT_IMAGE_DIR MPISPAWN_GENERIC_VALUE_5="/home1/03222/tg825207/parallelnbody" MPISPAWN_GENERIC_NAME_6=LMOD_FAMILY_MPI MPISPAWN_GENERIC_VALUE_6="mvapich2" MPISPAWN_GENERIC_NAME_7=SLURM_JOBID MPISPAWN_GENERIC_VALUE_7="4306264" MPISPAWN_GENERIC_NAME_8=_ModuleTable001_ MPISPAWN_GENERIC_VALUE_8="X01vZHVsZVRhYmxlXz17WyJhY3RpdmVTaXplIl09OCxiYXNlTXBhdGhBPXsiL29wdC9hcHBzL3hzZWRlL21vZHVsZWZpbGVzIiwiL29wdC9hcHBzL21vZHVsZWZpbGVzIiwiL29wdC9tb2R1bGVmaWxlcyIsfSxbImNfcmVidWlsZFRpbWUiXT1mYWxzZSxbImNfc2hvcnRUaW1lIl09ZmFsc2UsZmFtaWx5PXtbIk1QSSJdPSJtdmFwaWNoMiIsWyJjb21waWxlciJdPSJnY2MiLH0saW5hY3RpdmU9e30sbVQ9e0xpbnV4PXtbIkZOIl09Ii9vcHQvbW9kdWxlZmlsZXMvTGludXgiLFsiZGVmYXVsdCJdPTAsWyJmdWxsTmFtZSJdPSJMaW51eCIsWyJsb2FkT3JkZXIiXT0yLHByb3BUPXt9LFsic2hvcnQiXT0iTGludXgiLFsic3RhdHVzIl09ImFjdGl2ZSIsfSxUQUNDPXtbIkZOIl09Ii9v" MPISPAWN_GENERIC_NAME_9=TACC_SYSTEM MPISPAWN_GENERIC_VALUE_9="stampede" MPISPAWN_GENERIC_NAME_10=WORK MPISPAWN_GENERIC_VALUE_10="/work/03222/tg825207" MPISPAWN_GENERIC_NAME_11=SLURM_GTIDS MPISPAWN_GENERIC_VALUE_11="0" MPISPAWN_GENERIC_NAME_12=GCC_LIB MPISPAWN_GENERIC_VALUE_12="/opt/apps/gcc/4.7.1/lib64" MPISPAWN_GENERIC_NAME_13=SLURM_PRIO_PROCESS MPISPAWN_GENERIC_VALUE_13="0" MPISPAWN_GENERIC_NAME_14=SCRATCH MPISPAWN_GENERIC_VALUE_14="/scratch/03222/tg825207" MPISPAWN_GENERIC_NAME_15=LOGNAME MPISPAWN_GENERIC_VALUE_15="tg825207" MPISPAWN_GENERIC_NAME_16=_ModuleTable002_ MPISPAWN_GENERIC_VALUE_16="cHQvbW9kdWxlZmlsZXMvVEFDQyIsWyJkZWZhdWx0Il09MCxbImZ1bGxOYW1lIl09IlRBQ0MiLFsibG9hZE9yZGVyIl09Nixwcm9wVD17fSxbInNob3J0Il09IlRBQ0MiLFsic3RhdHVzIl09ImFjdGl2ZSIsfSxbIlRBQ0MtcGF0aHMiXT17WyJGTiJdPSIvb3B0L21vZHVsZWZpbGVzL1RBQ0MtcGF0aHMiLFsiZGVmYXVsdCJdPTAsWyJmdWxsTmFtZSJdPSJUQUNDLXBhdGhzIixbImxvYWRPcmRlciJdPTEscHJvcFQ9e30sWyJzaG9ydCJdPSJUQUNDLXBhdGhzIixbInN0YXR1cyJdPSJhY3RpdmUiLH0sY2x1c3Rlcj17WyJGTiJdPSIvb3B0L21vZHVsZWZpbGVzL2NsdXN0ZXIiLFsiZGVmYXVsdCJdPTAsWyJmdWxsTmFtZSJdPSJjbHVzdGVyIixbImxvYWRPcmRlciJdPTUscHJvcFQ9" MPISPAWN_GENERIC_NAME_17=INPUTRC MPISPAWN_GENERIC_VALUE_17="/etc/inputrc" MPISPAWN_GENERIC_NAME_18=LMOD_CMD MPISPAWN_GENERIC_VALUE_18="/opt/apps/lmod/lmod/libexec/lmod" MPISPAWN_GENERIC_NAME_19=I_MPI_OFA_ADAPTER_NAME MPISPAWN_GENERIC_VALUE_19="mlx4_0" MPISPAWN_GENERIC_NAME_20=QTLIB MPISPAWN_GENERIC_VALUE_20="/usr/lib64/qt-3.3/lib" MPISPAWN_GENERIC_NAME_21=NODE_TASKS_PPN_INFO MPISPAWN_GENERIC_VALUE_21=""16,0_"" MPISPAWN_GENERIC_NAME_22=SLURM_CPUS_ON_NODE MPISPAWN_GENERIC_VALUE_22="16" MPISPAWN_GENERIC_NAME_23=PATH MPISPAWN_GENERIC_VALUE_23="/opt/apps/gcc4_7/mvapich2/1.9/bin:/opt/apps/gcc/4.7.1/bin:/opt/apps/xalt/0.4.4/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/opt/apps/xsede/gsi-openssh-5.7/bin:/usr/X11R6/bin:/opt/ofed/bin:/opt/ofed/sbin:." MPISPAWN_GENERIC_NAME_24=SLURM_TACC_JOBNAME MPISPAWN_GENERIC_VALUE_24="stampedeLargeScaleAllTS.sh" MPISPAWN_GENERIC_NAME_25=SLURM_NODEID MPISPAWN_GENERIC_VALUE_25="0" MPISPAWN_GENERIC_NAME_26=LMOD_PKG MPISPAWN_GENERIC_VALUE_26="/opt/apps/lmod/lmod" MPISPAWN_GENERIC_NAME_27=MIC_I_MPI_OFA_ADAPTER_NAME MPISPAWN_GENERIC_VALUE_27="mlx4_0" MPISPAWN_GENERIC_NAME_28=HISTSIZE MPISPAWN_GENERIC_VALUE_28="1000" MPISPAWN_GENERIC_NAME_29=SLURM_NTASKS MPISPAWN_GENERIC_VALUE_29="2048" MPISPAWN_GENERIC_NAME_30=SHLIB_PATH MPISPAWN_GENERIC_VALUE_30="/opt/apps/xsede/gsi-openssh-5.7/lib64" MPISPAWN_GENERIC_NAME_31=TACC_OVERRIDE_PROJECT MPISPAWN_GENERIC_VALUE_31="TG-ASC140020" MPISPAWN_GENERIC_NAME_32=_ModuleTable005_ MPISPAWN_GENERIC_VALUE_32="Z2NjNF83L21vZHVsZWZpbGVzIiwiL29wdC9hcHBzL3hzZWRlL21vZHVsZWZpbGVzIiwiL29wdC9hcHBzL21vZHVsZWZpbGVzIiwiL29wdC9tb2R1bGVmaWxlcyIsfSxbInN5c3RlbUJhc2VNUEFUSCJdPSIvb3B0L2FwcHMveHNlZGUvbW9kdWxlZmlsZXM6L29wdC9hcHBzL21vZHVsZWZpbGVzOi9vcHQvbW9kdWxlZmlsZXMiLFsidmVyc2lvbiJdPTIsfQ==" MPISPAWN_GENERIC_NAME_33=_ModuleTable_Sz_ MPISPAWN_GENERIC_VALUE_33="5" MPISPAWN_GENERIC_NAME_34=MAIL MPISPAWN_GENERIC_VALUE_34="/var/spool/mail/tg825207" MPISPAWN_GENERIC_NAME_35=MV2_USE_HUGEPAGES MPISPAWN_GENERIC_VALUE_35="0" MPISPAWN_GENERIC_NAME_36=SLURM_TOPOLOGY_ADDR MPISPAWN_GENERIC_VALUE_36="c468-601" MPISPAWN_GENERIC_NAME_37=TACC_XALT_DIR MPISPAWN_GENERIC_VALUE_37="/opt/apps/xalt/0.4.4/" MPISPAWN_GENERIC_NAME_38=QTDIR MPISPAWN_GENERIC_VALUE_38="/usr/lib64/qt-3.3" MPISPAWN_GENERIC_NAME_39=SYSTEM MPISPAWN_GENERIC_VALUE_39="linux" MPISPAWN_GENERIC_NAME_40=SLURM_TACC_RUNLIMIT_MINS MPISPAWN_GENERIC_VALUE_40="180" MPISPAWN_GENERIC_NAME_41=SLURM_SUBMIT_DIR MPISPAWN_GENERIC_VALUE_41="/home1/03222/tg825207/parallelnbody" MPISPAWN_GENERIC_NAME_42=LIBPATH MPISPAWN_GENERIC_VALUE_42="/opt/apps/xsede/gsi-openssh-5.7/lib64:/usr/lib:/lib" MPISPAWN_GENERIC_NAME_43=HOSTNAME MPISPAWN_GENERIC_VALUE_43="c468-601" MPISPAWN_GENERIC_NAME_44=LMOD_DEFAULT_MODULEPATH MPISPAWN_GENERIC_VALUE_44="/opt/apps/xsede/modulefiles:/opt/apps/modulefiles:/opt/modulefiles" MPISPAWN_GENERIC_NAME_45=ARCHIVE MPISPAWN_GENERIC_VALUE_45="/home/03222/tg825207" MPISPAWN_GENERIC_NAME_46=_ MPISPAWN_GENERIC_VALUE_46="/usr/local/bin/build_env.pl" MPISPAWN_GENERIC_NAME_47=__BASHRC_SOURCED__ MPISPAWN_GENERIC_VALUE_47="1" MPISPAWN_GENERIC_NAME_48=APPS MPISPAWN_GENERIC_VALUE_48="/opt/apps" MPISPAWN_GENERIC_NAME_49=SHELL MPISPAWN_GENERIC_VALUE_49="/bin/bash" MPISPAWN_GENERIC_NAME_50=ARMCI_OPENIB_DEVICE MPISPAWN_GENERIC_VALUE_50="mlx4_0" MPISPAWN_GENERIC_NAME_51=ENVIRONMENT MPISPAWN_GENERIC_VALUE_51="BATCH" MPISPAWN_GENERIC_NAME_52=TACC_FAMILY_MPI MPISPAWN_GENERIC_VALUE_52="mvapich2" MPISPAWN_GENERIC_NAME_53=MV2_IBA_HCA MPISPAWN_GENERIC_VALUE_53="mlx4_0" MPISPAWN_GENERIC_NAME_54=TACC_PUBLIC_MACHINE MPISPAWN_GENERIC_VALUE_54="1" MPISPAWN_GENERIC_NAME_55=SLURM_TACC_NODES MPISPAWN_GENERIC_VALUE_55="1" MPISPAWN_GENERIC_NAME_56=_ModuleTable003_ MPISPAWN_GENERIC_VALUE_56="e30sWyJzaG9ydCJdPSJjbHVzdGVyIixbInN0YXR1cyJdPSJhY3RpdmUiLH0sWyJjbHVzdGVyLXBhdGhzIl09e1siRk4iXT0iL29wdC9tb2R1bGVmaWxlcy9jbHVzdGVyLXBhdGhzIixbImRlZmF1bHQiXT0wLFsiZnVsbE5hbWUiXT0iY2x1c3Rlci1wYXRocyIsWyJsb2FkT3JkZXIiXT0zLHByb3BUPXt9LFsic2hvcnQiXT0iY2x1c3Rlci1wYXRocyIsWyJzdGF0dXMiXT0iYWN0aXZlIix9LGdjYz17WyJGTiJdPSIvb3B0L2FwcHMvbW9kdWxlZmlsZXMvZ2NjLzQuNy4xLmx1YSIsWyJkZWZhdWx0Il09MCxbImZ1bGxOYW1lIl09ImdjYy80LjcuMSIsWyJsb2FkT3JkZXIiXT03LHByb3BUPXt9LFsic2hvcnQiXT0iZ2NjIixbInN0YXR1cyJdPSJhY3RpdmUiLH0sbXZhcGljaDI9e1si" MPISPAWN_GENERIC_NAME_57=QTINC MPISPAWN_GENERIC_VALUE_57="/usr/lib64/qt-3.3/include" MPISPAWN_GENERIC_NAME_58=CVS_RSH MPISPAWN_GENERIC_VALUE_58="ssh" MPISPAWN_GENERIC_NAME_59=MV2_USE_OLD_BCAST MPISPAWN_GENERIC_VALUE_59="1" MPISPAWN_GENERIC_NAME_60=SLURM_JOB_NUM_NODES MPISPAWN_GENERIC_VALUE_60="128" MPISPAWN_GENERIC_NAME_61=SLURM_TASK_PID MPISPAWN_GENERIC_VALUE_61="31479" MPISPAWN_GENERIC_NAME_62=TACC_MPI_GETMODE MPISPAWN_GENERIC_VALUE_62="mvapich2_ssh" MPISPAWN_GENERIC_NAME_63=LANG MPISPAWN_GENERIC_VALUE_63="en_US.UTF-8" MPISPAWN_GENERIC_NAME_64=USER MPISPAWN_GENERIC_VALUE_64="tg825207" MPISPAWN_GENERIC_NAME_65=MIC_I_MPI_DAPL_PROVIDER MPISPAWN_GENERIC_VALUE_65="ofa-v2-mlx4_0-1u" MPISPAWN_GENERIC_NAME_66=G_BROKEN_FILENAMES MPISPAWN_GENERIC_VALUE_66="1" MPISPAWN_GENERIC_NAME_67=SLURM_LOCALID MPISPAWN_GENERIC_VALUE_67="0" MPISPAWN_GENERIC_NAME_68=SLURM_NPROCS MPISPAWN_GENERIC_VALUE_68="2048" MPISPAWN_GENERIC_NAME_69=SLURM_TACC_NNODES_SET MPISPAWN_GENERIC_VALUE_69="0" MPISPAWN_GENERIC_NAME_70=SLURM_TACC_CORES MPISPAWN_GENERIC_VALUE_70="2048" MPISPAWN_GENERIC_NAME_71=LMOD_DIR MPISPAWN_GENERIC_VALUE_71="/opt/apps/lmod/lmod/libexec" MPISPAWN_GENERIC_NAME_72=LMOD_SETTARG_CMD MPISPAWN_GENERIC_VALUE_72=":" MPISPAWN_GENERIC_NAME_73=OMP_NUM_THREADS MPISPAWN_GENERIC_VALUE_73="1" MPISPAWN_GENERIC_NAME_74=_ModuleTable004_ MPISPAWN_GENERIC_VALUE_74="Rk4iXT0iL29wdC9hcHBzL2djYzRfNy9tb2R1bGVmaWxlcy9tdmFwaWNoMi8xLjlhMiIsWyJkZWZhdWx0Il09MSxbImZ1bGxOYW1lIl09Im12YXBpY2gyLzEuOWEyIixbImxvYWRPcmRlciJdPTgscHJvcFQ9e30sWyJzaG9ydCJdPSJtdmFwaWNoMiIsWyJzdGF0dXMiXT0iYWN0aXZlIix9LHhhbHQ9e1siRk4iXT0iL29wdC9hcHBzL21vZHVsZWZpbGVzL3hhbHQvMC40LjQubHVhIixbImRlZmF1bHQiXT0xLFsiZnVsbE5hbWUiXT0ieGFsdC8wLjQuNCIsWyJsb2FkT3JkZXIiXT00LHByb3BUPXt9LFsic2hvcnQiXT0ieGFsdCIsWyJzdGF0dXMiXT0iYWN0aXZlIix9LH0sbXBhdGhBPXsiL29wdC9hcHBzL2djYzRfNy9tdmFwaWNoMl8xXzkvbW9kdWxlZmlsZXMiLCIvb3B0L2FwcHMv" MPISPAWN_GENERIC_NAME_75=__PERSONAL_PATH__ MPISPAWN_GENERIC_VALUE_75="1" MPISPAWN_GENERIC_NAME_76=LMOD_SYSTEM_DEFAULT_MODULES MPISPAWN_GENERIC_VALUE_76="TACC" MPISPAWN_GENERIC_NAME_77=SLURM_JOB_NAME MPISPAWN_GENERIC_VALUE_77="stampedeLargeScaleAllTS.sh" MPISPAWN_GENERIC_NAME_78=SLURM_SUBMIT_HOST MPISPAWN_GENERIC_VALUE_78="login4.stampede.tacc.utexas.edu" MPISPAWN_GENERIC_NAME_79=LD_LIBRARY_PATH MPISPAWN_GENERIC_VALUE_79="/opt/apps/gcc4_7/mvapich2/1.9/lib:/opt/apps/gcc4_7/mvapich2/1.9/lib/shared:/opt/apps/gcc/4.7.1/lib64:/opt/apps/gcc/4.7.1/lib:/opt/apps/xsede/gsi-openssh-5.7/lib64:/opt/apps/xsede/gsi-openssh-5.7/lib64" MPISPAWN_GENERIC_NAME_80=SLURM_QUEUE MPISPAWN_GENERIC_VALUE_80="normal" MPISPAWN_GENERIC_NAME_81=PKG_CONFIG_PATH MPISPAWN_GENERIC_VALUE_81="/opt/apps/gcc4_7/mvapich2/1.9/lib/pkgconfig" MPISPAWN_GENERIC_NAME_82=LMOD_PREPEND_BLOCK MPISPAWN_GENERIC_VALUE_82="normal" MPISPAWN_GENERIC_NAME_83=TACC_FAMILY_COMPILER MPISPAWN_GENERIC_VALUE_83="gcc" MPISPAWN_GENERIC_NAME_84=TACC_DOMAIN MPISPAWN_GENERIC_VALUE_84="stampede" MPISPAWN_GENERIC_NAME_85=MV2_HOMOGENEOUS_CLUSTER MPISPAWN_GENERIC_VALUE_85="1" MPISPAWN_GENERIC_NAME_86=LMOD_FAMILY_COMPILER MPISPAWN_GENERIC_VALUE_86="gcc" MPISPAWN_GENERIC_NAME_87=MV2_USE_UD_HYBRID MPISPAWN_GENERIC_VALUE_87="0" MPISPAWN_GENERIC_NAME_88=MPICH_HOME MPISPAWN_GENERIC_VALUE_88="/opt/apps/gcc4_7/mvapich2/1.9" MPISPAWN_GENERIC_NAME_89=SLURM_JOB_NODELIST MPISPAWN_GENERIC_VALUE_89="c468-601,c471-503,c472-[401-402],c473-[303-304,401,804,901],c474-[203-204,301,401,403-404],c478-303,c482-404,c483-[001-004,101,504,601,804,901],c484-[101,103-104,201,602,701],c485-502,c486-102,c490-[302,904],c491-004,c492-701,c493-[001,102,201],c494-[101,402-403,602,702-703,904],c495-[001-002,201],c497-[102,202,504,601-603],c498-[304,402,603],c501-[204,301,604,801],c502-104,c503-901,c504-[602,604,702,801],c505-[001,202-204,401,502-504,802,901-904],c506-[302,704],c507-[102,202,503,601,904],c508-[101,204,601,702-703,802,903-904],c509-[002,201,301,303,401,501-502],c510-[003,401,404,501,503],c511-[601-602,902-903],c536-[801,902],c537-902,c538-[001,003,104],c540-[104,201-202,804,901-904]" MPISPAWN_GENERIC_NAME_90=SLURM_PROCID MPISPAWN_GENERIC_VALUE_90="0" MPISPAWN_GENERIC_NAME_91=I_MPI_DAPL_PROVIDER MPISPAWN_GENERIC_VALUE_91="ofa-v2-mlx4_0-1u" MPISPAWN_GENERIC_NAME_92=GLOBUS_LOCATION MPISPAWN_GENERIC_VALUE_92="/opt/apps/xsede/gsi-openssh-5.7" MPISPAWN_GENERIC_NAME_93=LMOD_COLORIZE MPISPAWN_GENERIC_VALUE_93="yes" MPISPAWN_GENERIC_NAME_94=I_MPI_HYDRA_BRANCH_COUNT MPISPAWN_GENERIC_VALUE_94="6000" MPISPAWN_GENERIC_NAME_95=SLURM_NNODES MPISPAWN_GENERIC_VALUE_95="128" MPISPAWN_GENERIC_NAME_96=MV2_DEFAULT_TIME_OUT MPISPAWN_GENERIC_VALUE_96="23" MPISPAWN_GENERIC_NAME_97=SLURM_TACC_NCORES_SET MPISPAWN_GENERIC_VALUE_97="1" MPISPAWN_GENERIC_NAME_98=__Init_Default_Modules MPISPAWN_GENERIC_VALUE_98="1" MPISPAWN_GENERIC_NAME_99=BASH_ENV MPISPAWN_GENERIC_VALUE_99="/etc/tacc/tacc_functions" MPISPAWN_GENERIC_NAME_100=SLURM_TOPOLOGY_ADDR_PATTERN MPISPAWN_GENERIC_VALUE_100="node" MPISPAWN_GENERIC_NAME_101=MV2_USE_RING_STARTUP MPISPAWN_GENERIC_VALUE_101="0" MPISPAWN_GENERIC_NAME_102=TACC_XALT_BIN MPISPAWN_GENERIC_VALUE_102="/opt/apps/xalt/0.4.4/bin" MPISPAWN_GENERIC_NAME_103=X509_USER_PROXY MPISPAWN_GENERIC_VALUE_103="/tmp/x509up_p3084.file9zM3fu.1" MPISPAWN_GENERIC_NAME_104=TMPDIR MPISPAWN_GENERIC_VALUE_104="/tmp" MPISPAWN_GENERIC_NAME_105=LMOD_FULL_SETTARG_SUPPORT MPISPAWN_GENERIC_VALUE_105="no" MPISPAWN_ID=19 MPISPAWN_WORKING_DIR=/home1/03222/tg825207/parallelnbody MPISPAWN_MPIRUN_RANK_0=304 MPISPAWN_MPIRUN_RANK_1=305 MPISPAWN_MPIRUN_RANK_2=306 MPISPAWN_MPIRUN_RANK_3=307 MPISPAWN_MPIRUN_RANK_4=308 MPISPAWN_MPIRUN_RANK_5=309 MPISPAWN_MPIRUN_RANK_6=310 MPISPAWN_MPIRUN_RANK_7=311 MPISPAWN_MPIRUN_RANK_8=312 MPISPAWN_MPIRUN_RANK_9=313 MPISPAWN_MPIRUN_RANK_10=314 MPISPAWN_MPIRUN_RANK_11=315 MPISPAWN_MPIRUN_RANK_12=316 MPISPAWN_MPIRUN_RANK_13=317 MPISPAWN_MPIRUN_RANK_14=318 MPISPAWN_MPIRUN_RANK_15=319 /opt/apps/gcc4_7/mvapich2/1.9/bin/mpispawn 0
[c474-404.stampede.tacc.utexas.edu:mpispawn_14][error_sighandler] Caught error: Segmentation fault (signal 11)
[c484-201.stampede.tacc.utexas.edu:mpispawn_29][error_sighandler] Caught error: Segmentation fault (signal 11)
bash: line 1: 19409 Segmentation fault      /usr/bin/env LD_LIBRARY_PATH=/opt/apps/gcc4_7/mvapich2/1.9/lib:/opt/apps/gcc4_7/mvapich2/1.9/lib/shared:/opt/apps/gcc/4.7.1/lib64:/opt/apps/gcc/4.7.1/lib:/opt/apps/xsede/gsi-openssh-5.7/lib64:/opt/apps/xsede/gsi-openssh-5.7/lib64 MPISPAWN_MPIRUN_MPD=0 USE_LINEAR_SSH=1 MPISPAWN_MPIRUN_HOST=c468-601.stampede.tacc.utexas.edu MPISPAWN_MPIRUN_HOSTIP=129.114.76.165 MPIRUN_RSH_LAUNCH=1 MPISPAWN_CHECKIN_PORT=36101 MPISPAWN_MPIRUN_PORT=36101 MPISPAWN_NNODES=64 MPISPAWN_GLOBAL_NPROCS=1024 MPISPAWN_MPIRUN_ID=44295 MPISPAWN_ARGC=4 MPDMAN_KVS_TEMPLATE=kvs_513_c468-601.stampede.tacc.utexas.edu_44295 MPISPAWN_LOCAL_NPROCS=16 MPISPAWN_ARGV_0='/home1/03222/tg825207/parallelnbody/teamscatter' MPISPAWN_ARGV_1='256000' MPISPAWN_ARGV_2='-c' MPISPAWN_ARGV_3='2' MPISPAWN_ARGC=4 MPISPAWN_GENERIC_ENV_COUNT=106 MPISPAWN_GENERIC_NAME_0=TACC_ENVLEN MPISPAWN_GENERIC_VALUE_0=6578 MPISPAWN_GENERIC_NAME_1=SLURM_JOB_ID MPISPAWN_GENERIC_VALUE_1="4306264" MPISPAWN_GENERIC_NAME_2=SLURMD_NODENAME MPISPAWN_GENERIC_VALUE_2="c468-601" MPISPAWN_GENERIC_NAME_3=STOCKYARD MPISPAWN_GENERIC_VALUE_3="/work/03222/tg825207" MPISPAWN_GENERIC_NAME_4=LC_ALL MPISPAWN_GENERIC_VALUE_4="en_US.UTF-8" MPISPAWN_GENERIC_NAME_5=SLURM_CHECKPOINT_IMAGE_DIR MPISPAWN_GENERIC_VALUE_5="/home1/03222/tg825207/parallelnbody" MPISPAWN_GENERIC_NAME_6=LMOD_FAMILY_MPI MPISPAWN_GENERIC_VALUE_6="mvapich2" MPISPAWN_GENERIC_NAME_7=SLURM_JOBID MPISPAWN_GENERIC_VALUE_7="4306264" MPISPAWN_GENERIC_NAME_8=_ModuleTable001_ MPISPAWN_GENERIC_VALUE_8="X01vZHVsZVRhYmxlXz17WyJhY3RpdmVTaXplIl09OCxiYXNlTXBhdGhBPXsiL29wdC9hcHBzL3hzZWRlL21vZHVsZWZpbGVzIiwiL29wdC9hcHBzL21vZHVsZWZpbGVzIiwiL29wdC9tb2R1bGVmaWxlcyIsfSxbImNfcmVidWlsZFRpbWUiXT1mYWxzZSxbImNfc2hvcnRUaW1lIl09ZmFsc2UsZmFtaWx5PXtbIk1QSSJdPSJtdmFwaWNoMiIsWyJjb21waWxlciJdPSJnY2MiLH0saW5hY3RpdmU9e30sbVQ9e0xpbnV4PXtbIkZOIl09Ii9vcHQvbW9kdWxlZmlsZXMvTGludXgiLFsiZGVmYXVsdCJdPTAsWyJmdWxsTmFtZSJdPSJMaW51eCIsWyJsb2FkT3JkZXIiXT0yLHByb3BUPXt9LFsic2hvcnQiXT0iTGludXgiLFsic3RhdHVzIl09ImFjdGl2ZSIsfSxUQUNDPXtbIkZOIl09Ii9v" MPISPAWN_GENERIC_NAME_9=TACC_SYSTEM MPISPAWN_GENERIC_VALUE_9="stampede" MPISPAWN_GENERIC_NAME_10=WORK MPISPAWN_GENERIC_VALUE_10="/work/03222/tg825207" MPISPAWN_GENERIC_NAME_11=SLURM_GTIDS MPISPAWN_GENERIC_VALUE_11="0" MPISPAWN_GENERIC_NAME_12=GCC_LIB MPISPAWN_GENERIC_VALUE_12="/opt/apps/gcc/4.7.1/lib64" MPISPAWN_GENERIC_NAME_13=SLURM_PRIO_PROCESS MPISPAWN_GENERIC_VALUE_13="0" MPISPAWN_GENERIC_NAME_14=SCRATCH MPISPAWN_GENERIC_VALUE_14="/scratch/03222/tg825207" MPISPAWN_GENERIC_NAME_15=LOGNAME MPISPAWN_GENERIC_VALUE_15="tg825207" MPISPAWN_GENERIC_NAME_16=_ModuleTable002_ MPISPAWN_GENERIC_VALUE_16="cHQvbW9kdWxlZmlsZXMvVEFDQyIsWyJkZWZhdWx0Il09MCxbImZ1bGxOYW1lIl09IlRBQ0MiLFsibG9hZE9yZGVyIl09Nixwcm9wVD17fSxbInNob3J0Il09IlRBQ0MiLFsic3RhdHVzIl09ImFjdGl2ZSIsfSxbIlRBQ0MtcGF0aHMiXT17WyJGTiJdPSIvb3B0L21vZHVsZWZpbGVzL1RBQ0MtcGF0aHMiLFsiZGVmYXVsdCJdPTAsWyJmdWxsTmFtZSJdPSJUQUNDLXBhdGhzIixbImxvYWRPcmRlciJdPTEscHJvcFQ9e30sWyJzaG9ydCJdPSJUQUNDLXBhdGhzIixbInN0YXR1cyJdPSJhY3RpdmUiLH0sY2x1c3Rlcj17WyJGTiJdPSIvb3B0L21vZHVsZWZpbGVzL2NsdXN0ZXIiLFsiZGVmYXVsdCJdPTAsWyJmdWxsTmFtZSJdPSJjbHVzdGVyIixbImxvYWRPcmRlciJdPTUscHJvcFQ9" MPISPAWN_GENERIC_NAME_17=INPUTRC MPISPAWN_GENERIC_VALUE_17="/etc/inputrc" MPISPAWN_GENERIC_NAME_18=LMOD_CMD MPISPAWN_GENERIC_VALUE_18="/opt/apps/lmod/lmod/libexec/lmod" MPISPAWN_GENERIC_NAME_19=I_MPI_OFA_ADAPTER_NAME MPISPAWN_GENERIC_VALUE_19="mlx4_0" MPISPAWN_GENERIC_NAME_20=QTLIB MPISPAWN_GENERIC_VALUE_20="/usr/lib64/qt-3.3/lib" MPISPAWN_GENERIC_NAME_21=NODE_TASKS_PPN_INFO MPISPAWN_GENERIC_VALUE_21=""16,0_"" MPISPAWN_GENERIC_NAME_22=SLURM_CPUS_ON_NODE MPISPAWN_GENERIC_VALUE_22="16" MPISPAWN_GENERIC_NAME_23=PATH MPISPAWN_GENERIC_VALUE_23="/opt/apps/gcc4_7/mvapich2/1.9/bin:/opt/apps/gcc/4.7.1/bin:/opt/apps/xalt/0.4.4/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/opt/apps/xsede/gsi-openssh-5.7/bin:/usr/X11R6/bin:/opt/ofed/bin:/opt/ofed/sbin:." MPISPAWN_GENERIC_NAME_24=SLURM_TACC_JOBNAME MPISPAWN_GENERIC_VALUE_24="stampedeLargeScaleAllTS.sh" MPISPAWN_GENERIC_NAME_25=SLURM_NODEID MPISPAWN_GENERIC_VALUE_25="0" MPISPAWN_GENERIC_NAME_26=LMOD_PKG MPISPAWN_GENERIC_VALUE_26="/opt/apps/lmod/lmod" MPISPAWN_GENERIC_NAME_27=MIC_I_MPI_OFA_ADAPTER_NAME MPISPAWN_GENERIC_VALUE_27="mlx4_0" MPISPAWN_GENERIC_NAME_28=HISTSIZE MPISPAWN_GENERIC_VALUE_28="1000" MPISPAWN_GENERIC_NAME_29=SLURM_NTASKS MPISPAWN_GENERIC_VALUE_29="2048" MPISPAWN_GENERIC_NAME_30=SHLIB_PATH MPISPAWN_GENERIC_VALUE_30="/opt/apps/xsede/gsi-openssh-5.7/lib64" MPISPAWN_GENERIC_NAME_31=TACC_OVERRIDE_PROJECT MPISPAWN_GENERIC_VALUE_31="TG-ASC140020" MPISPAWN_GENERIC_NAME_32=_ModuleTable005_ MPISPAWN_GENERIC_VALUE_32="Z2NjNF83L21vZHVsZWZpbGVzIiwiL29wdC9hcHBzL3hzZWRlL21vZHVsZWZpbGVzIiwiL29wdC9hcHBzL21vZHVsZWZpbGVzIiwiL29wdC9tb2R1bGVmaWxlcyIsfSxbInN5c3RlbUJhc2VNUEFUSCJdPSIvb3B0L2FwcHMveHNlZGUvbW9kdWxlZmlsZXM6L29wdC9hcHBzL21vZHVsZWZpbGVzOi9vcHQvbW9kdWxlZmlsZXMiLFsidmVyc2lvbiJdPTIsfQ==" MPISPAWN_GENERIC_NAME_33=_ModuleTable_Sz_ MPISPAWN_GENERIC_VALUE_33="5" MPISPAWN_GENERIC_NAME_34=MAIL MPISPAWN_GENERIC_VALUE_34="/var/spool/mail/tg825207" MPISPAWN_GENERIC_NAME_35=MV2_USE_HUGEPAGES MPISPAWN_GENERIC_VALUE_35="0" MPISPAWN_GENERIC_NAME_36=SLURM_TOPOLOGY_ADDR MPISPAWN_GENERIC_VALUE_36="c468-601" MPISPAWN_GENERIC_NAME_37=TACC_XALT_DIR MPISPAWN_GENERIC_VALUE_37="/opt/apps/xalt/0.4.4/" MPISPAWN_GENERIC_NAME_38=QTDIR MPISPAWN_GENERIC_VALUE_38="/usr/lib64/qt-3.3" MPISPAWN_GENERIC_NAME_39=SYSTEM MPISPAWN_GENERIC_VALUE_39="linux" MPISPAWN_GENERIC_NAME_40=SLURM_TACC_RUNLIMIT_MINS MPISPAWN_GENERIC_VALUE_40="180" MPISPAWN_GENERIC_NAME_41=SLURM_SUBMIT_DIR MPISPAWN_GENERIC_VALUE_41="/home1/03222/tg825207/parallelnbody" MPISPAWN_GENERIC_NAME_42=LIBPATH MPISPAWN_GENERIC_VALUE_42="/opt/apps/xsede/gsi-openssh-5.7/lib64:/usr/lib:/lib" MPISPAWN_GENERIC_NAME_43=HOSTNAME MPISPAWN_GENERIC_VALUE_43="c468-601" MPISPAWN_GENERIC_NAME_44=LMOD_DEFAULT_MODULEPATH MPISPAWN_GENERIC_VALUE_44="/opt/apps/xsede/modulefiles:/opt/apps/modulefiles:/opt/modulefiles" MPISPAWN_GENERIC_NAME_45=ARCHIVE MPISPAWN_GENERIC_VALUE_45="/home/03222/tg825207" MPISPAWN_GENERIC_NAME_46=_ MPISPAWN_GENERIC_VALUE_46="/usr/local/bin/build_env.pl" MPISPAWN_GENERIC_NAME_47=__BASHRC_SOURCED__ MPISPAWN_GENERIC_VALUE_47="1" MPISPAWN_GENERIC_NAME_48=APPS MPISPAWN_GENERIC_VALUE_48="/opt/apps" MPISPAWN_GENERIC_NAME_49=SHELL MPISPAWN_GENERIC_VALUE_49="/bin/bash" MPISPAWN_GENERIC_NAME_50=ARMCI_OPENIB_DEVICE MPISPAWN_GENERIC_VALUE_50="mlx4_0" MPISPAWN_GENERIC_NAME_51=ENVIRONMENT MPISPAWN_GENERIC_VALUE_51="BATCH" MPISPAWN_GENERIC_NAME_52=TACC_FAMILY_MPI MPISPAWN_GENERIC_VALUE_52="mvapich2" MPISPAWN_GENERIC_NAME_53=MV2_IBA_HCA MPISPAWN_GENERIC_VALUE_53="mlx4_0" MPISPAWN_GENERIC_NAME_54=TACC_PUBLIC_MACHINE MPISPAWN_GENERIC_VALUE_54="1" MPISPAWN_GENERIC_NAME_55=SLURM_TACC_NODES MPISPAWN_GENERIC_VALUE_55="1" MPISPAWN_GENERIC_NAME_56=_ModuleTable003_ MPISPAWN_GENERIC_VALUE_56="e30sWyJzaG9ydCJdPSJjbHVzdGVyIixbInN0YXR1cyJdPSJhY3RpdmUiLH0sWyJjbHVzdGVyLXBhdGhzIl09e1siRk4iXT0iL29wdC9tb2R1bGVmaWxlcy9jbHVzdGVyLXBhdGhzIixbImRlZmF1bHQiXT0wLFsiZnVsbE5hbWUiXT0iY2x1c3Rlci1wYXRocyIsWyJsb2FkT3JkZXIiXT0zLHByb3BUPXt9LFsic2hvcnQiXT0iY2x1c3Rlci1wYXRocyIsWyJzdGF0dXMiXT0iYWN0aXZlIix9LGdjYz17WyJGTiJdPSIvb3B0L2FwcHMvbW9kdWxlZmlsZXMvZ2NjLzQuNy4xLmx1YSIsWyJkZWZhdWx0Il09MCxbImZ1bGxOYW1lIl09ImdjYy80LjcuMSIsWyJsb2FkT3JkZXIiXT03LHByb3BUPXt9LFsic2hvcnQiXT0iZ2NjIixbInN0YXR1cyJdPSJhY3RpdmUiLH0sbXZhcGljaDI9e1si" MPISPAWN_GENERIC_NAME_57=QTINC MPISPAWN_GENERIC_VALUE_57="/usr/lib64/qt-3.3/include" MPISPAWN_GENERIC_NAME_58=CVS_RSH MPISPAWN_GENERIC_VALUE_58="ssh" MPISPAWN_GENERIC_NAME_59=MV2_USE_OLD_BCAST MPISPAWN_GENERIC_VALUE_59="1" MPISPAWN_GENERIC_NAME_60=SLURM_JOB_NUM_NODES MPISPAWN_GENERIC_VALUE_60="128" MPISPAWN_GENERIC_NAME_61=SLURM_TASK_PID MPISPAWN_GENERIC_VALUE_61="31479" MPISPAWN_GENERIC_NAME_62=TACC_MPI_GETMODE MPISPAWN_GENERIC_VALUE_62="mvapich2_ssh" MPISPAWN_GENERIC_NAME_63=LANG MPISPAWN_GENERIC_VALUE_63="en_US.UTF-8" MPISPAWN_GENERIC_NAME_64=USER MPISPAWN_GENERIC_VALUE_64="tg825207" MPISPAWN_GENERIC_NAME_65=MIC_I_MPI_DAPL_PROVIDER MPISPAWN_GENERIC_VALUE_65="ofa-v2-mlx4_0-1u" MPISPAWN_GENERIC_NAME_66=G_BROKEN_FILENAMES MPISPAWN_GENERIC_VALUE_66="1" MPISPAWN_GENERIC_NAME_67=SLURM_LOCALID MPISPAWN_GENERIC_VALUE_67="0" MPISPAWN_GENERIC_NAME_68=SLURM_NPROCS MPISPAWN_GENERIC_VALUE_68="2048" MPISPAWN_GENERIC_NAME_69=SLURM_TACC_NNODES_SET MPISPAWN_GENERIC_VALUE_69="0" MPISPAWN_GENERIC_NAME_70=SLURM_TACC_CORES MPISPAWN_GENERIC_VALUE_70="2048" MPISPAWN_GENERIC_NAME_71=LMOD_DIR MPISPAWN_GENERIC_VALUE_71="/opt/apps/lmod/lmod/libexec" MPISPAWN_GENERIC_NAME_72=LMOD_SETTARG_CMD MPISPAWN_GENERIC_VALUE_72=":" MPISPAWN_GENERIC_NAME_73=OMP_NUM_THREADS MPISPAWN_GENERIC_VALUE_73="1" MPISPAWN_GENERIC_NAME_74=_ModuleTable004_ MPISPAWN_GENERIC_VALUE_74="Rk4iXT0iL29wdC9hcHBzL2djYzRfNy9tb2R1bGVmaWxlcy9tdmFwaWNoMi8xLjlhMiIsWyJkZWZhdWx0Il09MSxbImZ1bGxOYW1lIl09Im12YXBpY2gyLzEuOWEyIixbImxvYWRPcmRlciJdPTgscHJvcFQ9e30sWyJzaG9ydCJdPSJtdmFwaWNoMiIsWyJzdGF0dXMiXT0iYWN0aXZlIix9LHhhbHQ9e1siRk4iXT0iL29wdC9hcHBzL21vZHVsZWZpbGVzL3hhbHQvMC40LjQubHVhIixbImRlZmF1bHQiXT0xLFsiZnVsbE5hbWUiXT0ieGFsdC8wLjQuNCIsWyJsb2FkT3JkZXIiXT00LHByb3BUPXt9LFsic2hvcnQiXT0ieGFsdCIsWyJzdGF0dXMiXT0iYWN0aXZlIix9LH0sbXBhdGhBPXsiL29wdC9hcHBzL2djYzRfNy9tdmFwaWNoMl8xXzkvbW9kdWxlZmlsZXMiLCIvb3B0L2FwcHMv" MPISPAWN_GENERIC_NAME_75=__PERSONAL_PATH__ MPISPAWN_GENERIC_VALUE_75="1" MPISPAWN_GENERIC_NAME_76=LMOD_SYSTEM_DEFAULT_MODULES MPISPAWN_GENERIC_VALUE_76="TACC" MPISPAWN_GENERIC_NAME_77=SLURM_JOB_NAME MPISPAWN_GENERIC_VALUE_77="stampedeLargeScaleAllTS.sh" MPISPAWN_GENERIC_NAME_78=SLURM_SUBMIT_HOST MPISPAWN_GENERIC_VALUE_78="login4.stampede.tacc.utexas.edu" MPISPAWN_GENERIC_NAME_79=LD_LIBRARY_PATH MPISPAWN_GENERIC_VALUE_79="/opt/apps/gcc4_7/mvapich2/1.9/lib:/opt/apps/gcc4_7/mvapich2/1.9/lib/shared:/opt/apps/gcc/4.7.1/lib64:/opt/apps/gcc/4.7.1/lib:/opt/apps/xsede/gsi-openssh-5.7/lib64:/opt/apps/xsede/gsi-openssh-5.7/lib64" MPISPAWN_GENERIC_NAME_80=SLURM_QUEUE MPISPAWN_GENERIC_VALUE_80="normal" MPISPAWN_GENERIC_NAME_81=PKG_CONFIG_PATH MPISPAWN_GENERIC_VALUE_81="/opt/apps/gcc4_7/mvapich2/1.9/lib/pkgconfig" MPISPAWN_GENERIC_NAME_82=LMOD_PREPEND_BLOCK MPISPAWN_GENERIC_VALUE_82="normal" MPISPAWN_GENERIC_NAME_83=TACC_FAMILY_COMPILER MPISPAWN_GENERIC_VALUE_83="gcc" MPISPAWN_GENERIC_NAME_84=TACC_DOMAIN MPISPAWN_GENERIC_VALUE_84="stampede" MPISPAWN_GENERIC_NAME_85=MV2_HOMOGENEOUS_CLUSTER MPISPAWN_GENERIC_VALUE_85="1" MPISPAWN_GENERIC_NAME_86=LMOD_FAMILY_COMPILER MPISPAWN_GENERIC_VALUE_86="gcc" MPISPAWN_GENERIC_NAME_87=MV2_USE_UD_HYBRID MPISPAWN_GENERIC_VALUE_87="0" MPISPAWN_GENERIC_NAME_88=MPICH_HOME MPISPAWN_GENERIC_VALUE_88="/opt/apps/gcc4_7/mvapich2/1.9" MPISPAWN_GENERIC_NAME_89=SLURM_JOB_NODELIST MPISPAWN_GENERIC_VALUE_89="c468-601,c471-503,c472-[401-402],c473-[303-304,401,804,901],c474-[203-204,301,401,403-404],c478-303,c482-404,c483-[001-004,101,504,601,804,901],c484-[101,103-104,201,602,701],c485-502,c486-102,c490-[302,904],c491-004,c492-701,c493-[001,102,201],c494-[101,402-403,602,702-703,904],c495-[001-002,201],c497-[102,202,504,601-603],c498-[304,402,603],c501-[204,301,604,801],c502-104,c503-901,c504-[602,604,702,801],c505-[001,202-204,401,502-504,802,901-904],c506-[302,704],c507-[102,202,503,601,904],c508-[101,204,601,702-703,802,903-904],c509-[002,201,301,303,401,501-502],c510-[003,401,404,501,503],c511-[601-602,902-903],c536-[801,902],c537-902,c538-[001,003,104],c540-[104,201-202,804,901-904]" MPISPAWN_GENERIC_NAME_90=SLURM_PROCID MPISPAWN_GENERIC_VALUE_90="0" MPISPAWN_GENERIC_NAME_91=I_MPI_DAPL_PROVIDER MPISPAWN_GENERIC_VALUE_91="ofa-v2-mlx4_0-1u" MPISPAWN_GENERIC_NAME_92=GLOBUS_LOCATION MPISPAWN_GENERIC_VALUE_92="/opt/apps/xsede/gsi-openssh-5.7" MPISPAWN_GENERIC_NAME_93=LMOD_COLORIZE MPISPAWN_GENERIC_VALUE_93="yes" MPISPAWN_GENERIC_NAME_94=I_MPI_HYDRA_BRANCH_COUNT MPISPAWN_GENERIC_VALUE_94="6000" MPISPAWN_GENERIC_NAME_95=SLURM_NNODES MPISPAWN_GENERIC_VALUE_95="128" MPISPAWN_GENERIC_NAME_96=MV2_DEFAULT_TIME_OUT MPISPAWN_GENERIC_VALUE_96="23" MPISPAWN_GENERIC_NAME_97=SLURM_TACC_NCORES_SET MPISPAWN_GENERIC_VALUE_97="1" MPISPAWN_GENERIC_NAME_98=__Init_Default_Modules MPISPAWN_GENERIC_VALUE_98="1" MPISPAWN_GENERIC_NAME_99=BASH_ENV MPISPAWN_GENERIC_VALUE_99="/etc/tacc/tacc_functions" MPISPAWN_GENERIC_NAME_100=SLURM_TOPOLOGY_ADDR_PATTERN MPISPAWN_GENERIC_VALUE_100="node" MPISPAWN_GENERIC_NAME_101=MV2_USE_RING_STARTUP MPISPAWN_GENERIC_VALUE_101="0" MPISPAWN_GENERIC_NAME_102=TACC_XALT_BIN MPISPAWN_GENERIC_VALUE_102="/opt/apps/xalt/0.4.4/bin" MPISPAWN_GENERIC_NAME_103=X509_USER_PROXY MPISPAWN_GENERIC_VALUE_103="/tmp/x509up_p3084.file9zM3fu.1" MPISPAWN_GENERIC_NAME_104=TMPDIR MPISPAWN_GENERIC_VALUE_104="/tmp" MPISPAWN_GENERIC_NAME_105=LMOD_FULL_SETTARG_SUPPORT MPISPAWN_GENERIC_VALUE_105="no" MPISPAWN_ID=14 MPISPAWN_WORKING_DIR=/home1/03222/tg825207/parallelnbody MPISPAWN_MPIRUN_RANK_0=224 MPISPAWN_MPIRUN_RANK_1=225 MPISPAWN_MPIRUN_RANK_2=226 MPISPAWN_MPIRUN_RANK_3=227 MPISPAWN_MPIRUN_RANK_4=228 MPISPAWN_MPIRUN_RANK_5=229 MPISPAWN_MPIRUN_RANK_6=230 MPISPAWN_MPIRUN_RANK_7=231 MPISPAWN_MPIRUN_RANK_8=232 MPISPAWN_MPIRUN_RANK_9=233 MPISPAWN_MPIRUN_RANK_10=234 MPISPAWN_MPIRUN_RANK_11=235 MPISPAWN_MPIRUN_RANK_12=236 MPISPAWN_MPIRUN_RANK_13=237 MPISPAWN_MPIRUN_RANK_14=238 MPISPAWN_MPIRUN_RANK_15=239 /opt/apps/gcc4_7/mvapich2/1.9/bin/mpispawn 0
bash: line 1: 90379 Segmentation fault      /usr/bin/env LD_LIBRARY_PATH=/opt/apps/gcc4_7/mvapich2/1.9/lib:/opt/apps/gcc4_7/mvapich2/1.9/lib/shared:/opt/apps/gcc/4.7.1/lib64:/opt/apps/gcc/4.7.1/lib:/opt/apps/xsede/gsi-openssh-5.7/lib64:/opt/apps/xsede/gsi-openssh-5.7/lib64 MPISPAWN_MPIRUN_MPD=0 USE_LINEAR_SSH=1 MPISPAWN_MPIRUN_HOST=c468-601.stampede.tacc.utexas.edu MPISPAWN_MPIRUN_HOSTIP=129.114.76.165 MPIRUN_RSH_LAUNCH=1 MPISPAWN_CHECKIN_PORT=36101 MPISPAWN_MPIRUN_PORT=36101 MPISPAWN_NNODES=64 MPISPAWN_GLOBAL_NPROCS=1024 MPISPAWN_MPIRUN_ID=44295 MPISPAWN_ARGC=4 MPDMAN_KVS_TEMPLATE=kvs_513_c468-601.stampede.tacc.utexas.edu_44295 MPISPAWN_LOCAL_NPROCS=16 MPISPAWN_ARGV_0='/home1/03222/tg825207/parallelnbody/teamscatter' MPISPAWN_ARGV_1='256000' MPISPAWN_ARGV_2='-c' MPISPAWN_ARGV_3='2' MPISPAWN_ARGC=4 MPISPAWN_GENERIC_ENV_COUNT=106 MPISPAWN_GENERIC_NAME_0=TACC_ENVLEN MPISPAWN_GENERIC_VALUE_0=6578 MPISPAWN_GENERIC_NAME_1=SLURM_JOB_ID MPISPAWN_GENERIC_VALUE_1="4306264" MPISPAWN_GENERIC_NAME_2=SLURMD_NODENAME MPISPAWN_GENERIC_VALUE_2="c468-601" MPISPAWN_GENERIC_NAME_3=STOCKYARD MPISPAWN_GENERIC_VALUE_3="/work/03222/tg825207" MPISPAWN_GENERIC_NAME_4=LC_ALL MPISPAWN_GENERIC_VALUE_4="en_US.UTF-8" MPISPAWN_GENERIC_NAME_5=SLURM_CHECKPOINT_IMAGE_DIR MPISPAWN_GENERIC_VALUE_5="/home1/03222/tg825207/parallelnbody" MPISPAWN_GENERIC_NAME_6=LMOD_FAMILY_MPI MPISPAWN_GENERIC_VALUE_6="mvapich2" MPISPAWN_GENERIC_NAME_7=SLURM_JOBID MPISPAWN_GENERIC_VALUE_7="4306264" MPISPAWN_GENERIC_NAME_8=_ModuleTable001_ MPISPAWN_GENERIC_VALUE_8="X01vZHVsZVRhYmxlXz17WyJhY3RpdmVTaXplIl09OCxiYXNlTXBhdGhBPXsiL29wdC9hcHBzL3hzZWRlL21vZHVsZWZpbGVzIiwiL29wdC9hcHBzL21vZHVsZWZpbGVzIiwiL29wdC9tb2R1bGVmaWxlcyIsfSxbImNfcmVidWlsZFRpbWUiXT1mYWxzZSxbImNfc2hvcnRUaW1lIl09ZmFsc2UsZmFtaWx5PXtbIk1QSSJdPSJtdmFwaWNoMiIsWyJjb21waWxlciJdPSJnY2MiLH0saW5hY3RpdmU9e30sbVQ9e0xpbnV4PXtbIkZOIl09Ii9vcHQvbW9kdWxlZmlsZXMvTGludXgiLFsiZGVmYXVsdCJdPTAsWyJmdWxsTmFtZSJdPSJMaW51eCIsWyJsb2FkT3JkZXIiXT0yLHByb3BUPXt9LFsic2hvcnQiXT0iTGludXgiLFsic3RhdHVzIl09ImFjdGl2ZSIsfSxUQUNDPXtbIkZOIl09Ii9v" MPISPAWN_GENERIC_NAME_9=TACC_SYSTEM MPISPAWN_GENERIC_VALUE_9="stampede" MPISPAWN_GENERIC_NAME_10=WORK MPISPAWN_GENERIC_VALUE_10="/work/03222/tg825207" MPISPAWN_GENERIC_NAME_11=SLURM_GTIDS MPISPAWN_GENERIC_VALUE_11="0" MPISPAWN_GENERIC_NAME_12=GCC_LIB MPISPAWN_GENERIC_VALUE_12="/opt/apps/gcc/4.7.1/lib64" MPISPAWN_GENERIC_NAME_13=SLURM_PRIO_PROCESS MPISPAWN_GENERIC_VALUE_13="0" MPISPAWN_GENERIC_NAME_14=SCRATCH MPISPAWN_GENERIC_VALUE_14="/scratch/03222/tg825207" MPISPAWN_GENERIC_NAME_15=LOGNAME MPISPAWN_GENERIC_VALUE_15="tg825207" MPISPAWN_GENERIC_NAME_16=_ModuleTable002_ MPISPAWN_GENERIC_VALUE_16="cHQvbW9kdWxlZmlsZXMvVEFDQyIsWyJkZWZhdWx0Il09MCxbImZ1bGxOYW1lIl09IlRBQ0MiLFsibG9hZE9yZGVyIl09Nixwcm9wVD17fSxbInNob3J0Il09IlRBQ0MiLFsic3RhdHVzIl09ImFjdGl2ZSIsfSxbIlRBQ0MtcGF0aHMiXT17WyJGTiJdPSIvb3B0L21vZHVsZWZpbGVzL1RBQ0MtcGF0aHMiLFsiZGVmYXVsdCJdPTAsWyJmdWxsTmFtZSJdPSJUQUNDLXBhdGhzIixbImxvYWRPcmRlciJdPTEscHJvcFQ9e30sWyJzaG9ydCJdPSJUQUNDLXBhdGhzIixbInN0YXR1cyJdPSJhY3RpdmUiLH0sY2x1c3Rlcj17WyJGTiJdPSIvb3B0L21vZHVsZWZpbGVzL2NsdXN0ZXIiLFsiZGVmYXVsdCJdPTAsWyJmdWxsTmFtZSJdPSJjbHVzdGVyIixbImxvYWRPcmRlciJdPTUscHJvcFQ9" MPISPAWN_GENERIC_NAME_17=INPUTRC MPISPAWN_GENERIC_VALUE_17="/etc/inputrc" MPISPAWN_GENERIC_NAME_18=LMOD_CMD MPISPAWN_GENERIC_VALUE_18="/opt/apps/lmod/lmod/libexec/lmod" MPISPAWN_GENERIC_NAME_19=I_MPI_OFA_ADAPTER_NAME MPISPAWN_GENERIC_VALUE_19="mlx4_0" MPISPAWN_GENERIC_NAME_20=QTLIB MPISPAWN_GENERIC_VALUE_20="/usr/lib64/qt-3.3/lib" MPISPAWN_GENERIC_NAME_21=NODE_TASKS_PPN_INFO MPISPAWN_GENERIC_VALUE_21=""16,0_"" MPISPAWN_GENERIC_NAME_22=SLURM_CPUS_ON_NODE MPISPAWN_GENERIC_VALUE_22="16" MPISPAWN_GENERIC_NAME_23=PATH MPISPAWN_GENERIC_VALUE_23="/opt/apps/gcc4_7/mvapich2/1.9/bin:/opt/apps/gcc/4.7.1/bin:/opt/apps/xalt/0.4.4/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/opt/apps/xsede/gsi-openssh-5.7/bin:/usr/X11R6/bin:/opt/ofed/bin:/opt/ofed/sbin:." MPISPAWN_GENERIC_NAME_24=SLURM_TACC_JOBNAME MPISPAWN_GENERIC_VALUE_24="stampedeLargeScaleAllTS.sh" MPISPAWN_GENERIC_NAME_25=SLURM_NODEID MPISPAWN_GENERIC_VALUE_25="0" MPISPAWN_GENERIC_NAME_26=LMOD_PKG MPISPAWN_GENERIC_VALUE_26="/opt/apps/lmod/lmod" MPISPAWN_GENERIC_NAME_27=MIC_I_MPI_OFA_ADAPTER_NAME MPISPAWN_GENERIC_VALUE_27="mlx4_0" MPISPAWN_GENERIC_NAME_28=HISTSIZE MPISPAWN_GENERIC_VALUE_28="1000" MPISPAWN_GENERIC_NAME_29=SLURM_NTASKS MPISPAWN_GENERIC_VALUE_29="2048" MPISPAWN_GENERIC_NAME_30=SHLIB_PATH MPISPAWN_GENERIC_VALUE_30="/opt/apps/xsede/gsi-openssh-5.7/lib64" MPISPAWN_GENERIC_NAME_31=TACC_OVERRIDE_PROJECT MPISPAWN_GENERIC_VALUE_31="TG-ASC140020" MPISPAWN_GENERIC_NAME_32=_ModuleTable005_ MPISPAWN_GENERIC_VALUE_32="Z2NjNF83L21vZHVsZWZpbGVzIiwiL29wdC9hcHBzL3hzZWRlL21vZHVsZWZpbGVzIiwiL29wdC9hcHBzL21vZHVsZWZpbGVzIiwiL29wdC9tb2R1bGVmaWxlcyIsfSxbInN5c3RlbUJhc2VNUEFUSCJdPSIvb3B0L2FwcHMveHNlZGUvbW9kdWxlZmlsZXM6L29wdC9hcHBzL21vZHVsZWZpbGVzOi9vcHQvbW9kdWxlZmlsZXMiLFsidmVyc2lvbiJdPTIsfQ==" MPISPAWN_GENERIC_NAME_33=_ModuleTable_Sz_ MPISPAWN_GENERIC_VALUE_33="5" MPISPAWN_GENERIC_NAME_34=MAIL MPISPAWN_GENERIC_VALUE_34="/var/spool/mail/tg825207" MPISPAWN_GENERIC_NAME_35=MV2_USE_HUGEPAGES MPISPAWN_GENERIC_VALUE_35="0" MPISPAWN_GENERIC_NAME_36=SLURM_TOPOLOGY_ADDR MPISPAWN_GENERIC_VALUE_36="c468-601" MPISPAWN_GENERIC_NAME_37=TACC_XALT_DIR MPISPAWN_GENERIC_VALUE_37="/opt/apps/xalt/0.4.4/" MPISPAWN_GENERIC_NAME_38=QTDIR MPISPAWN_GENERIC_VALUE_38="/usr/lib64/qt-3.3" MPISPAWN_GENERIC_NAME_39=SYSTEM MPISPAWN_GENERIC_VALUE_39="linux" MPISPAWN_GENERIC_NAME_40=SLURM_TACC_RUNLIMIT_MINS MPISPAWN_GENERIC_VALUE_40="180" MPISPAWN_GENERIC_NAME_41=SLURM_SUBMIT_DIR MPISPAWN_GENERIC_VALUE_41="/home1/03222/tg825207/parallelnbody" MPISPAWN_GENERIC_NAME_42=LIBPATH MPISPAWN_GENERIC_VALUE_42="/opt/apps/xsede/gsi-openssh-5.7/lib64:/usr/lib:/lib" MPISPAWN_GENERIC_NAME_43=HOSTNAME MPISPAWN_GENERIC_VALUE_43="c468-601" MPISPAWN_GENERIC_NAME_44=LMOD_DEFAULT_MODULEPATH MPISPAWN_GENERIC_VALUE_44="/opt/apps/xsede/modulefiles:/opt/apps/modulefiles:/opt/modulefiles" MPISPAWN_GENERIC_NAME_45=ARCHIVE MPISPAWN_GENERIC_VALUE_45="/home/03222/tg825207" MPISPAWN_GENERIC_NAME_46=_ MPISPAWN_GENERIC_VALUE_46="/usr/local/bin/build_env.pl" MPISPAWN_GENERIC_NAME_47=__BASHRC_SOURCED__ MPISPAWN_GENERIC_VALUE_47="1" MPISPAWN_GENERIC_NAME_48=APPS MPISPAWN_GENERIC_VALUE_48="/opt/apps" MPISPAWN_GENERIC_NAME_49=SHELL MPISPAWN_GENERIC_VALUE_49="/bin/bash" MPISPAWN_GENERIC_NAME_50=ARMCI_OPENIB_DEVICE MPISPAWN_GENERIC_VALUE_50="mlx4_0" MPISPAWN_GENERIC_NAME_51=ENVIRONMENT MPISPAWN_GENERIC_VALUE_51="BATCH" MPISPAWN_GENERIC_NAME_52=TACC_FAMILY_MPI MPISPAWN_GENERIC_VALUE_52="mvapich2" MPISPAWN_GENERIC_NAME_53=MV2_IBA_HCA MPISPAWN_GENERIC_VALUE_53="mlx4_0" MPISPAWN_GENERIC_NAME_54=TACC_PUBLIC_MACHINE MPISPAWN_GENERIC_VALUE_54="1" MPISPAWN_GENERIC_NAME_55=SLURM_TACC_NODES MPISPAWN_GENERIC_VALUE_55="1" MPISPAWN_GENERIC_NAME_56=_ModuleTable003_ MPISPAWN_GENERIC_VALUE_56="e30sWyJzaG9ydCJdPSJjbHVzdGVyIixbInN0YXR1cyJdPSJhY3RpdmUiLH0sWyJjbHVzdGVyLXBhdGhzIl09e1siRk4iXT0iL29wdC9tb2R1bGVmaWxlcy9jbHVzdGVyLXBhdGhzIixbImRlZmF1bHQiXT0wLFsiZnVsbE5hbWUiXT0iY2x1c3Rlci1wYXRocyIsWyJsb2FkT3JkZXIiXT0zLHByb3BUPXt9LFsic2hvcnQiXT0iY2x1c3Rlci1wYXRocyIsWyJzdGF0dXMiXT0iYWN0aXZlIix9LGdjYz17WyJGTiJdPSIvb3B0L2FwcHMvbW9kdWxlZmlsZXMvZ2NjLzQuNy4xLmx1YSIsWyJkZWZhdWx0Il09MCxbImZ1bGxOYW1lIl09ImdjYy80LjcuMSIsWyJsb2FkT3JkZXIiXT03LHByb3BUPXt9LFsic2hvcnQiXT0iZ2NjIixbInN0YXR1cyJdPSJhY3RpdmUiLH0sbXZhcGljaDI9e1si" MPISPAWN_GENERIC_NAME_57=QTINC MPISPAWN_GENERIC_VALUE_57="/usr/lib64/qt-3.3/include" MPISPAWN_GENERIC_NAME_58=CVS_RSH MPISPAWN_GENERIC_VALUE_58="ssh" MPISPAWN_GENERIC_NAME_59=MV2_USE_OLD_BCAST MPISPAWN_GENERIC_VALUE_59="1" MPISPAWN_GENERIC_NAME_60=SLURM_JOB_NUM_NODES MPISPAWN_GENERIC_VALUE_60="128" MPISPAWN_GENERIC_NAME_61=SLURM_TASK_PID MPISPAWN_GENERIC_VALUE_61="31479" MPISPAWN_GENERIC_NAME_62=TACC_MPI_GETMODE MPISPAWN_GENERIC_VALUE_62="mvapich2_ssh" MPISPAWN_GENERIC_NAME_63=LANG MPISPAWN_GENERIC_VALUE_63="en_US.UTF-8" MPISPAWN_GENERIC_NAME_64=USER MPISPAWN_GENERIC_VALUE_64="tg825207" MPISPAWN_GENERIC_NAME_65=MIC_I_MPI_DAPL_PROVIDER MPISPAWN_GENERIC_VALUE_65="ofa-v2-mlx4_0-1u" MPISPAWN_GENERIC_NAME_66=G_BROKEN_FILENAMES MPISPAWN_GENERIC_VALUE_66="1" MPISPAWN_GENERIC_NAME_67=SLURM_LOCALID MPISPAWN_GENERIC_VALUE_67="0" MPISPAWN_GENERIC_NAME_68=SLURM_NPROCS MPISPAWN_GENERIC_VALUE_68="2048" MPISPAWN_GENERIC_NAME_69=SLURM_TACC_NNODES_SET MPISPAWN_GENERIC_VALUE_69="0" MPISPAWN_GENERIC_NAME_70=SLURM_TACC_CORES MPISPAWN_GENERIC_VALUE_70="2048" MPISPAWN_GENERIC_NAME_71=LMOD_DIR MPISPAWN_GENERIC_VALUE_71="/opt/apps/lmod/lmod/libexec" MPISPAWN_GENERIC_NAME_72=LMOD_SETTARG_CMD MPISPAWN_GENERIC_VALUE_72=":" MPISPAWN_GENERIC_NAME_73=OMP_NUM_THREADS MPISPAWN_GENERIC_VALUE_73="1" MPISPAWN_GENERIC_NAME_74=_ModuleTable004_ MPISPAWN_GENERIC_VALUE_74="Rk4iXT0iL29wdC9hcHBzL2djYzRfNy9tb2R1bGVmaWxlcy9tdmFwaWNoMi8xLjlhMiIsWyJkZWZhdWx0Il09MSxbImZ1bGxOYW1lIl09Im12YXBpY2gyLzEuOWEyIixbImxvYWRPcmRlciJdPTgscHJvcFQ9e30sWyJzaG9ydCJdPSJtdmFwaWNoMiIsWyJzdGF0dXMiXT0iYWN0aXZlIix9LHhhbHQ9e1siRk4iXT0iL29wdC9hcHBzL21vZHVsZWZpbGVzL3hhbHQvMC40LjQubHVhIixbImRlZmF1bHQiXT0xLFsiZnVsbE5hbWUiXT0ieGFsdC8wLjQuNCIsWyJsb2FkT3JkZXIiXT00LHByb3BUPXt9LFsic2hvcnQiXT0ieGFsdCIsWyJzdGF0dXMiXT0iYWN0aXZlIix9LH0sbXBhdGhBPXsiL29wdC9hcHBzL2djYzRfNy9tdmFwaWNoMl8xXzkvbW9kdWxlZmlsZXMiLCIvb3B0L2FwcHMv" MPISPAWN_GENERIC_NAME_75=__PERSONAL_PATH__ MPISPAWN_GENERIC_VALUE_75="1" MPISPAWN_GENERIC_NAME_76=LMOD_SYSTEM_DEFAULT_MODULES MPISPAWN_GENERIC_VALUE_76="TACC" MPISPAWN_GENERIC_NAME_77=SLURM_JOB_NAME MPISPAWN_GENERIC_VALUE_77="stampedeLargeScaleAllTS.sh" MPISPAWN_GENERIC_NAME_78=SLURM_SUBMIT_HOST MPISPAWN_GENERIC_VALUE_78="login4.stampede.tacc.utexas.edu" MPISPAWN_GENERIC_NAME_79=LD_LIBRARY_PATH MPISPAWN_GENERIC_VALUE_79="/opt/apps/gcc4_7/mvapich2/1.9/lib:/opt/apps/gcc4_7/mvapich2/1.9/lib/shared:/opt/apps/gcc/4.7.1/lib64:/opt/apps/gcc/4.7.1/lib:/opt/apps/xsede/gsi-openssh-5.7/lib64:/opt/apps/xsede/gsi-openssh-5.7/lib64" MPISPAWN_GENERIC_NAME_80=SLURM_QUEUE MPISPAWN_GENERIC_VALUE_80="normal" MPISPAWN_GENERIC_NAME_81=PKG_CONFIG_PATH MPISPAWN_GENERIC_VALUE_81="/opt/apps/gcc4_7/mvapich2/1.9/lib/pkgconfig" MPISPAWN_GENERIC_NAME_82=LMOD_PREPEND_BLOCK MPISPAWN_GENERIC_VALUE_82="normal" MPISPAWN_GENERIC_NAME_83=TACC_FAMILY_COMPILER MPISPAWN_GENERIC_VALUE_83="gcc" MPISPAWN_GENERIC_NAME_84=TACC_DOMAIN MPISPAWN_GENERIC_VALUE_84="stampede" MPISPAWN_GENERIC_NAME_85=MV2_HOMOGENEOUS_CLUSTER MPISPAWN_GENERIC_VALUE_85="1" MPISPAWN_GENERIC_NAME_86=LMOD_FAMILY_COMPILER MPISPAWN_GENERIC_VALUE_86="gcc" MPISPAWN_GENERIC_NAME_87=MV2_USE_UD_HYBRID MPISPAWN_GENERIC_VALUE_87="0" MPISPAWN_GENERIC_NAME_88=MPICH_HOME MPISPAWN_GENERIC_VALUE_88="/opt/apps/gcc4_7/mvapich2/1.9" MPISPAWN_GENERIC_NAME_89=SLURM_JOB_NODELIST MPISPAWN_GENERIC_VALUE_89="c468-601,c471-503,c472-[401-402],c473-[303-304,401,804,901],c474-[203-204,301,401,403-404],c478-303,c482-404,c483-[001-004,101,504,601,804,901],c484-[101,103-104,201,602,701],c485-502,c486-102,c490-[302,904],c491-004,c492-701,c493-[001,102,201],c494-[101,402-403,602,702-703,904],c495-[001-002,201],c497-[102,202,504,601-603],c498-[304,402,603],c501-[204,301,604,801],c502-104,c503-901,c504-[602,604,702,801],c505-[001,202-204,401,502-504,802,901-904],c506-[302,704],c507-[102,202,503,601,904],c508-[101,204,601,702-703,802,903-904],c509-[002,201,301,303,401,501-502],c510-[003,401,404,501,503],c511-[601-602,902-903],c536-[801,902],c537-902,c538-[001,003,104],c540-[104,201-202,804,901-904]" MPISPAWN_GENERIC_NAME_90=SLURM_PROCID MPISPAWN_GENERIC_VALUE_90="0" MPISPAWN_GENERIC_NAME_91=I_MPI_DAPL_PROVIDER MPISPAWN_GENERIC_VALUE_91="ofa-v2-mlx4_0-1u" MPISPAWN_GENERIC_NAME_92=GLOBUS_LOCATION MPISPAWN_GENERIC_VALUE_92="/opt/apps/xsede/gsi-openssh-5.7" MPISPAWN_GENERIC_NAME_93=LMOD_COLORIZE MPISPAWN_GENERIC_VALUE_93="yes" MPISPAWN_GENERIC_NAME_94=I_MPI_HYDRA_BRANCH_COUNT MPISPAWN_GENERIC_VALUE_94="6000" MPISPAWN_GENERIC_NAME_95=SLURM_NNODES MPISPAWN_GENERIC_VALUE_95="128" MPISPAWN_GENERIC_NAME_96=MV2_DEFAULT_TIME_OUT MPISPAWN_GENERIC_VALUE_96="23" MPISPAWN_GENERIC_NAME_97=SLURM_TACC_NCORES_SET MPISPAWN_GENERIC_VALUE_97="1" MPISPAWN_GENERIC_NAME_98=__Init_Default_Modules MPISPAWN_GENERIC_VALUE_98="1" MPISPAWN_GENERIC_NAME_99=BASH_ENV MPISPAWN_GENERIC_VALUE_99="/etc/tacc/tacc_functions" MPISPAWN_GENERIC_NAME_100=SLURM_TOPOLOGY_ADDR_PATTERN MPISPAWN_GENERIC_VALUE_100="node" MPISPAWN_GENERIC_NAME_101=MV2_USE_RING_STARTUP MPISPAWN_GENERIC_VALUE_101="0" MPISPAWN_GENERIC_NAME_102=TACC_XALT_BIN MPISPAWN_GENERIC_VALUE_102="/opt/apps/xalt/0.4.4/bin" MPISPAWN_GENERIC_NAME_103=X509_USER_PROXY MPISPAWN_GENERIC_VALUE_103="/tmp/x509up_p3084.file9zM3fu.1" MPISPAWN_GENERIC_NAME_104=TMPDIR MPISPAWN_GENERIC_VALUE_104="/tmp" MPISPAWN_GENERIC_NAME_105=LMOD_FULL_SETTARG_SUPPORT MPISPAWN_GENERIC_VALUE_105="no" MPISPAWN_ID=29 MPISPAWN_WORKING_DIR=/home1/03222/tg825207/parallelnbody MPISPAWN_MPIRUN_RANK_0=464 MPISPAWN_MPIRUN_RANK_1=465 MPISPAWN_MPIRUN_RANK_2=466 MPISPAWN_MPIRUN_RANK_3=467 MPISPAWN_MPIRUN_RANK_4=468 MPISPAWN_MPIRUN_RANK_5=469 MPISPAWN_MPIRUN_RANK_6=470 MPISPAWN_MPIRUN_RANK_7=471 MPISPAWN_MPIRUN_RANK_8=472 MPISPAWN_MPIRUN_RANK_9=473 MPISPAWN_MPIRUN_RANK_10=474 MPISPAWN_MPIRUN_RANK_11=475 MPISPAWN_MPIRUN_RANK_12=476 MPISPAWN_MPIRUN_RANK_13=477 MPISPAWN_MPIRUN_RANK_14=478 MPISPAWN_MPIRUN_RANK_15=479 /opt/apps/gcc4_7/mvapich2/1.9/bin/mpispawn 0
[c494-402.stampede.tacc.utexas.edu:mpispawn_42][error_sighandler] Caught error: Segmentation fault (signal 11)
[c497-603.stampede.tacc.utexas.edu:mpispawn_56][error_sighandler] Caught error: Segmentation fault (signal 11)
bash: line 1: 77265 Segmentation fault      /usr/bin/env LD_LIBRARY_PATH=/opt/apps/gcc4_7/mvapich2/1.9/lib:/opt/apps/gcc4_7/mvapich2/1.9/lib/shared:/opt/apps/gcc/4.7.1/lib64:/opt/apps/gcc/4.7.1/lib:/opt/apps/xsede/gsi-openssh-5.7/lib64:/opt/apps/xsede/gsi-openssh-5.7/lib64 MPISPAWN_MPIRUN_MPD=0 USE_LINEAR_SSH=1 MPISPAWN_MPIRUN_HOST=c468-601.stampede.tacc.utexas.edu MPISPAWN_MPIRUN_HOSTIP=129.114.76.165 MPIRUN_RSH_LAUNCH=1 MPISPAWN_CHECKIN_PORT=36101 MPISPAWN_MPIRUN_PORT=36101 MPISPAWN_NNODES=64 MPISPAWN_GLOBAL_NPROCS=1024 MPISPAWN_MPIRUN_ID=44295 MPISPAWN_ARGC=4 MPDMAN_KVS_TEMPLATE=kvs_513_c468-601.stampede.tacc.utexas.edu_44295 MPISPAWN_LOCAL_NPROCS=16 MPISPAWN_ARGV_0='/home1/03222/tg825207/parallelnbody/teamscatter' MPISPAWN_ARGV_1='256000' MPISPAWN_ARGV_2='-c' MPISPAWN_ARGV_3='2' MPISPAWN_ARGC=4 MPISPAWN_GENERIC_ENV_COUNT=106 MPISPAWN_GENERIC_NAME_0=TACC_ENVLEN MPISPAWN_GENERIC_VALUE_0=6578 MPISPAWN_GENERIC_NAME_1=SLURM_JOB_ID MPISPAWN_GENERIC_VALUE_1="4306264" MPISPAWN_GENERIC_NAME_2=SLURMD_NODENAME MPISPAWN_GENERIC_VALUE_2="c468-601" MPISPAWN_GENERIC_NAME_3=STOCKYARD MPISPAWN_GENERIC_VALUE_3="/work/03222/tg825207" MPISPAWN_GENERIC_NAME_4=LC_ALL MPISPAWN_GENERIC_VALUE_4="en_US.UTF-8" MPISPAWN_GENERIC_NAME_5=SLURM_CHECKPOINT_IMAGE_DIR MPISPAWN_GENERIC_VALUE_5="/home1/03222/tg825207/parallelnbody" MPISPAWN_GENERIC_NAME_6=LMOD_FAMILY_MPI MPISPAWN_GENERIC_VALUE_6="mvapich2" MPISPAWN_GENERIC_NAME_7=SLURM_JOBID MPISPAWN_GENERIC_VALUE_7="4306264" MPISPAWN_GENERIC_NAME_8=_ModuleTable001_ MPISPAWN_GENERIC_VALUE_8="X01vZHVsZVRhYmxlXz17WyJhY3RpdmVTaXplIl09OCxiYXNlTXBhdGhBPXsiL29wdC9hcHBzL3hzZWRlL21vZHVsZWZpbGVzIiwiL29wdC9hcHBzL21vZHVsZWZpbGVzIiwiL29wdC9tb2R1bGVmaWxlcyIsfSxbImNfcmVidWlsZFRpbWUiXT1mYWxzZSxbImNfc2hvcnRUaW1lIl09ZmFsc2UsZmFtaWx5PXtbIk1QSSJdPSJtdmFwaWNoMiIsWyJjb21waWxlciJdPSJnY2MiLH0saW5hY3RpdmU9e30sbVQ9e0xpbnV4PXtbIkZOIl09Ii9vcHQvbW9kdWxlZmlsZXMvTGludXgiLFsiZGVmYXVsdCJdPTAsWyJmdWxsTmFtZSJdPSJMaW51eCIsWyJsb2FkT3JkZXIiXT0yLHByb3BUPXt9LFsic2hvcnQiXT0iTGludXgiLFsic3RhdHVzIl09ImFjdGl2ZSIsfSxUQUNDPXtbIkZOIl09Ii9v" MPISPAWN_GENERIC_NAME_9=TACC_SYSTEM MPISPAWN_GENERIC_VALUE_9="stampede" MPISPAWN_GENERIC_NAME_10=WORK MPISPAWN_GENERIC_VALUE_10="/work/03222/tg825207" MPISPAWN_GENERIC_NAME_11=SLURM_GTIDS MPISPAWN_GENERIC_VALUE_11="0" MPISPAWN_GENERIC_NAME_12=GCC_LIB MPISPAWN_GENERIC_VALUE_12="/opt/apps/gcc/4.7.1/lib64" MPISPAWN_GENERIC_NAME_13=SLURM_PRIO_PROCESS MPISPAWN_GENERIC_VALUE_13="0" MPISPAWN_GENERIC_NAME_14=SCRATCH MPISPAWN_GENERIC_VALUE_14="/scratch/03222/tg825207" MPISPAWN_GENERIC_NAME_15=LOGNAME MPISPAWN_GENERIC_VALUE_15="tg825207" MPISPAWN_GENERIC_NAME_16=_ModuleTable002_ MPISPAWN_GENERIC_VALUE_16="cHQvbW9kdWxlZmlsZXMvVEFDQyIsWyJkZWZhdWx0Il09MCxbImZ1bGxOYW1lIl09IlRBQ0MiLFsibG9hZE9yZGVyIl09Nixwcm9wVD17fSxbInNob3J0Il09IlRBQ0MiLFsic3RhdHVzIl09ImFjdGl2ZSIsfSxbIlRBQ0MtcGF0aHMiXT17WyJGTiJdPSIvb3B0L21vZHVsZWZpbGVzL1RBQ0MtcGF0aHMiLFsiZGVmYXVsdCJdPTAsWyJmdWxsTmFtZSJdPSJUQUNDLXBhdGhzIixbImxvYWRPcmRlciJdPTEscHJvcFQ9e30sWyJzaG9ydCJdPSJUQUNDLXBhdGhzIixbInN0YXR1cyJdPSJhY3RpdmUiLH0sY2x1c3Rlcj17WyJGTiJdPSIvb3B0L21vZHVsZWZpbGVzL2NsdXN0ZXIiLFsiZGVmYXVsdCJdPTAsWyJmdWxsTmFtZSJdPSJjbHVzdGVyIixbImxvYWRPcmRlciJdPTUscHJvcFQ9" MPISPAWN_GENERIC_NAME_17=INPUTRC MPISPAWN_GENERIC_VALUE_17="/etc/inputrc" MPISPAWN_GENERIC_NAME_18=LMOD_CMD MPISPAWN_GENERIC_VALUE_18="/opt/apps/lmod/lmod/libexec/lmod" MPISPAWN_GENERIC_NAME_19=I_MPI_OFA_ADAPTER_NAME MPISPAWN_GENERIC_VALUE_19="mlx4_0" MPISPAWN_GENERIC_NAME_20=QTLIB MPISPAWN_GENERIC_VALUE_20="/usr/lib64/qt-3.3/lib" MPISPAWN_GENERIC_NAME_21=NODE_TASKS_PPN_INFO MPISPAWN_GENERIC_VALUE_21=""16,0_"" MPISPAWN_GENERIC_NAME_22=SLURM_CPUS_ON_NODE MPISPAWN_GENERIC_VALUE_22="16" MPISPAWN_GENERIC_NAME_23=PATH MPISPAWN_GENERIC_VALUE_23="/opt/apps/gcc4_7/mvapich2/1.9/bin:/opt/apps/gcc/4.7.1/bin:/opt/apps/xalt/0.4.4/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/opt/apps/xsede/gsi-openssh-5.7/bin:/usr/X11R6/bin:/opt/ofed/bin:/opt/ofed/sbin:." MPISPAWN_GENERIC_NAME_24=SLURM_TACC_JOBNAME MPISPAWN_GENERIC_VALUE_24="stampedeLargeScaleAllTS.sh" MPISPAWN_GENERIC_NAME_25=SLURM_NODEID MPISPAWN_GENERIC_VALUE_25="0" MPISPAWN_GENERIC_NAME_26=LMOD_PKG MPISPAWN_GENERIC_VALUE_26="/opt/apps/lmod/lmod" MPISPAWN_GENERIC_NAME_27=MIC_I_MPI_OFA_ADAPTER_NAME MPISPAWN_GENERIC_VALUE_27="mlx4_0" MPISPAWN_GENERIC_NAME_28=HISTSIZE MPISPAWN_GENERIC_VALUE_28="1000" MPISPAWN_GENERIC_NAME_29=SLURM_NTASKS MPISPAWN_GENERIC_VALUE_29="2048" MPISPAWN_GENERIC_NAME_30=SHLIB_PATH MPISPAWN_GENERIC_VALUE_30="/opt/apps/xsede/gsi-openssh-5.7/lib64" MPISPAWN_GENERIC_NAME_31=TACC_OVERRIDE_PROJECT MPISPAWN_GENERIC_VALUE_31="TG-ASC140020" MPISPAWN_GENERIC_NAME_32=_ModuleTable005_ MPISPAWN_GENERIC_VALUE_32="Z2NjNF83L21vZHVsZWZpbGVzIiwiL29wdC9hcHBzL3hzZWRlL21vZHVsZWZpbGVzIiwiL29wdC9hcHBzL21vZHVsZWZpbGVzIiwiL29wdC9tb2R1bGVmaWxlcyIsfSxbInN5c3RlbUJhc2VNUEFUSCJdPSIvb3B0L2FwcHMveHNlZGUvbW9kdWxlZmlsZXM6L29wdC9hcHBzL21vZHVsZWZpbGVzOi9vcHQvbW9kdWxlZmlsZXMiLFsidmVyc2lvbiJdPTIsfQ==" MPISPAWN_GENERIC_NAME_33=_ModuleTable_Sz_ MPISPAWN_GENERIC_VALUE_33="5" MPISPAWN_GENERIC_NAME_34=MAIL MPISPAWN_GENERIC_VALUE_34="/var/spool/mail/tg825207" MPISPAWN_GENERIC_NAME_35=MV2_USE_HUGEPAGES MPISPAWN_GENERIC_VALUE_35="0" MPISPAWN_GENERIC_NAME_36=SLURM_TOPOLOGY_ADDR MPISPAWN_GENERIC_VALUE_36="c468-601" MPISPAWN_GENERIC_NAME_37=TACC_XALT_DIR MPISPAWN_GENERIC_VALUE_37="/opt/apps/xalt/0.4.4/" MPISPAWN_GENERIC_NAME_38=QTDIR MPISPAWN_GENERIC_VALUE_38="/usr/lib64/qt-3.3" MPISPAWN_GENERIC_NAME_39=SYSTEM MPISPAWN_GENERIC_VALUE_39="linux" MPISPAWN_GENERIC_NAME_40=SLURM_TACC_RUNLIMIT_MINS MPISPAWN_GENERIC_VALUE_40="180" MPISPAWN_GENERIC_NAME_41=SLURM_SUBMIT_DIR MPISPAWN_GENERIC_VALUE_41="/home1/03222/tg825207/parallelnbody" MPISPAWN_GENERIC_NAME_42=LIBPATH MPISPAWN_GENERIC_VALUE_42="/opt/apps/xsede/gsi-openssh-5.7/lib64:/usr/lib:/lib" MPISPAWN_GENERIC_NAME_43=HOSTNAME MPISPAWN_GENERIC_VALUE_43="c468-601" MPISPAWN_GENERIC_NAME_44=LMOD_DEFAULT_MODULEPATH MPISPAWN_GENERIC_VALUE_44="/opt/apps/xsede/modulefiles:/opt/apps/modulefiles:/opt/modulefiles" MPISPAWN_GENERIC_NAME_45=ARCHIVE MPISPAWN_GENERIC_VALUE_45="/home/03222/tg825207" MPISPAWN_GENERIC_NAME_46=_ MPISPAWN_GENERIC_VALUE_46="/usr/local/bin/build_env.pl" MPISPAWN_GENERIC_NAME_47=__BASHRC_SOURCED__ MPISPAWN_GENERIC_VALUE_47="1" MPISPAWN_GENERIC_NAME_48=APPS MPISPAWN_GENERIC_VALUE_48="/opt/apps" MPISPAWN_GENERIC_NAME_49=SHELL MPISPAWN_GENERIC_VALUE_49="/bin/bash" MPISPAWN_GENERIC_NAME_50=ARMCI_OPENIB_DEVICE MPISPAWN_GENERIC_VALUE_50="mlx4_0" MPISPAWN_GENERIC_NAME_51=ENVIRONMENT MPISPAWN_GENERIC_VALUE_51="BATCH" MPISPAWN_GENERIC_NAME_52=TACC_FAMILY_MPI MPISPAWN_GENERIC_VALUE_52="mvapich2" MPISPAWN_GENERIC_NAME_53=MV2_IBA_HCA MPISPAWN_GENERIC_VALUE_53="mlx4_0" MPISPAWN_GENERIC_NAME_54=TACC_PUBLIC_MACHINE MPISPAWN_GENERIC_VALUE_54="1" MPISPAWN_GENERIC_NAME_55=SLURM_TACC_NODES MPISPAWN_GENERIC_VALUE_55="1" MPISPAWN_GENERIC_NAME_56=_ModuleTable003_ MPISPAWN_GENERIC_VALUE_56="e30sWyJzaG9ydCJdPSJjbHVzdGVyIixbInN0YXR1cyJdPSJhY3RpdmUiLH0sWyJjbHVzdGVyLXBhdGhzIl09e1siRk4iXT0iL29wdC9tb2R1bGVmaWxlcy9jbHVzdGVyLXBhdGhzIixbImRlZmF1bHQiXT0wLFsiZnVsbE5hbWUiXT0iY2x1c3Rlci1wYXRocyIsWyJsb2FkT3JkZXIiXT0zLHByb3BUPXt9LFsic2hvcnQiXT0iY2x1c3Rlci1wYXRocyIsWyJzdGF0dXMiXT0iYWN0aXZlIix9LGdjYz17WyJGTiJdPSIvb3B0L2FwcHMvbW9kdWxlZmlsZXMvZ2NjLzQuNy4xLmx1YSIsWyJkZWZhdWx0Il09MCxbImZ1bGxOYW1lIl09ImdjYy80LjcuMSIsWyJsb2FkT3JkZXIiXT03LHByb3BUPXt9LFsic2hvcnQiXT0iZ2NjIixbInN0YXR1cyJdPSJhY3RpdmUiLH0sbXZhcGljaDI9e1si" MPISPAWN_GENERIC_NAME_57=QTINC MPISPAWN_GENERIC_VALUE_57="/usr/lib64/qt-3.3/include" MPISPAWN_GENERIC_NAME_58=CVS_RSH MPISPAWN_GENERIC_VALUE_58="ssh" MPISPAWN_GENERIC_NAME_59=MV2_USE_OLD_BCAST MPISPAWN_GENERIC_VALUE_59="1" MPISPAWN_GENERIC_NAME_60=SLURM_JOB_NUM_NODES MPISPAWN_GENERIC_VALUE_60="128" MPISPAWN_GENERIC_NAME_61=SLURM_TASK_PID MPISPAWN_GENERIC_VALUE_61="31479" MPISPAWN_GENERIC_NAME_62=TACC_MPI_GETMODE MPISPAWN_GENERIC_VALUE_62="mvapich2_ssh" MPISPAWN_GENERIC_NAME_63=LANG MPISPAWN_GENERIC_VALUE_63="en_US.UTF-8" MPISPAWN_GENERIC_NAME_64=USER MPISPAWN_GENERIC_VALUE_64="tg825207" MPISPAWN_GENERIC_NAME_65=MIC_I_MPI_DAPL_PROVIDER MPISPAWN_GENERIC_VALUE_65="ofa-v2-mlx4_0-1u" MPISPAWN_GENERIC_NAME_66=G_BROKEN_FILENAMES MPISPAWN_GENERIC_VALUE_66="1" MPISPAWN_GENERIC_NAME_67=SLURM_LOCALID MPISPAWN_GENERIC_VALUE_67="0" MPISPAWN_GENERIC_NAME_68=SLURM_NPROCS MPISPAWN_GENERIC_VALUE_68="2048" MPISPAWN_GENERIC_NAME_69=SLURM_TACC_NNODES_SET MPISPAWN_GENERIC_VALUE_69="0" MPISPAWN_GENERIC_NAME_70=SLURM_TACC_CORES MPISPAWN_GENERIC_VALUE_70="2048" MPISPAWN_GENERIC_NAME_71=LMOD_DIR MPISPAWN_GENERIC_VALUE_71="/opt/apps/lmod/lmod/libexec" MPISPAWN_GENERIC_NAME_72=LMOD_SETTARG_CMD MPISPAWN_GENERIC_VALUE_72=":" MPISPAWN_GENERIC_NAME_73=OMP_NUM_THREADS MPISPAWN_GENERIC_VALUE_73="1" MPISPAWN_GENERIC_NAME_74=_ModuleTable004_ MPISPAWN_GENERIC_VALUE_74="Rk4iXT0iL29wdC9hcHBzL2djYzRfNy9tb2R1bGVmaWxlcy9tdmFwaWNoMi8xLjlhMiIsWyJkZWZhdWx0Il09MSxbImZ1bGxOYW1lIl09Im12YXBpY2gyLzEuOWEyIixbImxvYWRPcmRlciJdPTgscHJvcFQ9e30sWyJzaG9ydCJdPSJtdmFwaWNoMiIsWyJzdGF0dXMiXT0iYWN0aXZlIix9LHhhbHQ9e1siRk4iXT0iL29wdC9hcHBzL21vZHVsZWZpbGVzL3hhbHQvMC40LjQubHVhIixbImRlZmF1bHQiXT0xLFsiZnVsbE5hbWUiXT0ieGFsdC8wLjQuNCIsWyJsb2FkT3JkZXIiXT00LHByb3BUPXt9LFsic2hvcnQiXT0ieGFsdCIsWyJzdGF0dXMiXT0iYWN0aXZlIix9LH0sbXBhdGhBPXsiL29wdC9hcHBzL2djYzRfNy9tdmFwaWNoMl8xXzkvbW9kdWxlZmlsZXMiLCIvb3B0L2FwcHMv" MPISPAWN_GENERIC_NAME_75=__PERSONAL_PATH__ MPISPAWN_GENERIC_VALUE_75="1" MPISPAWN_GENERIC_NAME_76=LMOD_SYSTEM_DEFAULT_MODULES MPISPAWN_GENERIC_VALUE_76="TACC" MPISPAWN_GENERIC_NAME_77=SLURM_JOB_NAME MPISPAWN_GENERIC_VALUE_77="stampedeLargeScaleAllTS.sh" MPISPAWN_GENERIC_NAME_78=SLURM_SUBMIT_HOST MPISPAWN_GENERIC_VALUE_78="login4.stampede.tacc.utexas.edu" MPISPAWN_GENERIC_NAME_79=LD_LIBRARY_PATH MPISPAWN_GENERIC_VALUE_79="/opt/apps/gcc4_7/mvapich2/1.9/lib:/opt/apps/gcc4_7/mvapich2/1.9/lib/shared:/opt/apps/gcc/4.7.1/lib64:/opt/apps/gcc/4.7.1/lib:/opt/apps/xsede/gsi-openssh-5.7/lib64:/opt/apps/xsede/gsi-openssh-5.7/lib64" MPISPAWN_GENERIC_NAME_80=SLURM_QUEUE MPISPAWN_GENERIC_VALUE_80="normal" MPISPAWN_GENERIC_NAME_81=PKG_CONFIG_PATH MPISPAWN_GENERIC_VALUE_81="/opt/apps/gcc4_7/mvapich2/1.9/lib/pkgconfig" MPISPAWN_GENERIC_NAME_82=LMOD_PREPEND_BLOCK MPISPAWN_GENERIC_VALUE_82="normal" MPISPAWN_GENERIC_NAME_83=TACC_FAMILY_COMPILER MPISPAWN_GENERIC_VALUE_83="gcc" MPISPAWN_GENERIC_NAME_84=TACC_DOMAIN MPISPAWN_GENERIC_VALUE_84="stampede" MPISPAWN_GENERIC_NAME_85=MV2_HOMOGENEOUS_CLUSTER MPISPAWN_GENERIC_VALUE_85="1" MPISPAWN_GENERIC_NAME_86=LMOD_FAMILY_COMPILER MPISPAWN_GENERIC_VALUE_86="gcc" MPISPAWN_GENERIC_NAME_87=MV2_USE_UD_HYBRID MPISPAWN_GENERIC_VALUE_87="0" MPISPAWN_GENERIC_NAME_88=MPICH_HOME MPISPAWN_GENERIC_VALUE_88="/opt/apps/gcc4_7/mvapich2/1.9" MPISPAWN_GENERIC_NAME_89=SLURM_JOB_NODELIST MPISPAWN_GENERIC_VALUE_89="c468-601,c471-503,c472-[401-402],c473-[303-304,401,804,901],c474-[203-204,301,401,403-404],c478-303,c482-404,c483-[001-004,101,504,601,804,901],c484-[101,103-104,201,602,701],c485-502,c486-102,c490-[302,904],c491-004,c492-701,c493-[001,102,201],c494-[101,402-403,602,702-703,904],c495-[001-002,201],c497-[102,202,504,601-603],c498-[304,402,603],c501-[204,301,604,801],c502-104,c503-901,c504-[602,604,702,801],c505-[001,202-204,401,502-504,802,901-904],c506-[302,704],c507-[102,202,503,601,904],c508-[101,204,601,702-703,802,903-904],c509-[002,201,301,303,401,501-502],c510-[003,401,404,501,503],c511-[601-602,902-903],c536-[801,902],c537-902,c538-[001,003,104],c540-[104,201-202,804,901-904]" MPISPAWN_GENERIC_NAME_90=SLURM_PROCID MPISPAWN_GENERIC_VALUE_90="0" MPISPAWN_GENERIC_NAME_91=I_MPI_DAPL_PROVIDER MPISPAWN_GENERIC_VALUE_91="ofa-v2-mlx4_0-1u" MPISPAWN_GENERIC_NAME_92=GLOBUS_LOCATION MPISPAWN_GENERIC_VALUE_92="/opt/apps/xsede/gsi-openssh-5.7" MPISPAWN_GENERIC_NAME_93=LMOD_COLORIZE MPISPAWN_GENERIC_VALUE_93="yes" MPISPAWN_GENERIC_NAME_94=I_MPI_HYDRA_BRANCH_COUNT MPISPAWN_GENERIC_VALUE_94="6000" MPISPAWN_GENERIC_NAME_95=SLURM_NNODES MPISPAWN_GENERIC_VALUE_95="128" MPISPAWN_GENERIC_NAME_96=MV2_DEFAULT_TIME_OUT MPISPAWN_GENERIC_VALUE_96="23" MPISPAWN_GENERIC_NAME_97=SLURM_TACC_NCORES_SET MPISPAWN_GENERIC_VALUE_97="1" MPISPAWN_GENERIC_NAME_98=__Init_Default_Modules MPISPAWN_GENERIC_VALUE_98="1" MPISPAWN_GENERIC_NAME_99=BASH_ENV MPISPAWN_GENERIC_VALUE_99="/etc/tacc/tacc_functions" MPISPAWN_GENERIC_NAME_100=SLURM_TOPOLOGY_ADDR_PATTERN MPISPAWN_GENERIC_VALUE_100="node" MPISPAWN_GENERIC_NAME_101=MV2_USE_RING_STARTUP MPISPAWN_GENERIC_VALUE_101="0" MPISPAWN_GENERIC_NAME_102=TACC_XALT_BIN MPISPAWN_GENERIC_VALUE_102="/opt/apps/xalt/0.4.4/bin" MPISPAWN_GENERIC_NAME_103=X509_USER_PROXY MPISPAWN_GENERIC_VALUE_103="/tmp/x509up_p3084.file9zM3fu.1" MPISPAWN_GENERIC_NAME_104=TMPDIR MPISPAWN_GENERIC_VALUE_104="/tmp" MPISPAWN_GENERIC_NAME_105=LMOD_FULL_SETTARG_SUPPORT MPISPAWN_GENERIC_VALUE_105="no" MPISPAWN_ID=42 MPISPAWN_WORKING_DIR=/home1/03222/tg825207/parallelnbody MPISPAWN_MPIRUN_RANK_0=672 MPISPAWN_MPIRUN_RANK_1=673 MPISPAWN_MPIRUN_RANK_2=674 MPISPAWN_MPIRUN_RANK_3=675 MPISPAWN_MPIRUN_RANK_4=676 MPISPAWN_MPIRUN_RANK_5=677 MPISPAWN_MPIRUN_RANK_6=678 MPISPAWN_MPIRUN_RANK_7=679 MPISPAWN_MPIRUN_RANK_8=680 MPISPAWN_MPIRUN_RANK_9=681 MPISPAWN_MPIRUN_RANK_10=682 MPISPAWN_MPIRUN_RANK_11=683 MPISPAWN_MPIRUN_RANK_12=684 MPISPAWN_MPIRUN_RANK_13=685 MPISPAWN_MPIRUN_RANK_14=686 MPISPAWN_MPIRUN_RANK_15=687 /opt/apps/gcc4_7/mvapich2/1.9/bin/mpispawn 0
bash: line 1: 20646 Segmentation fault      /usr/bin/env LD_LIBRARY_PATH=/opt/apps/gcc4_7/mvapich2/1.9/lib:/opt/apps/gcc4_7/mvapich2/1.9/lib/shared:/opt/apps/gcc/4.7.1/lib64:/opt/apps/gcc/4.7.1/lib:/opt/apps/xsede/gsi-openssh-5.7/lib64:/opt/apps/xsede/gsi-openssh-5.7/lib64 MPISPAWN_MPIRUN_MPD=0 USE_LINEAR_SSH=1 MPISPAWN_MPIRUN_HOST=c468-601.stampede.tacc.utexas.edu MPISPAWN_MPIRUN_HOSTIP=129.114.76.165 MPIRUN_RSH_LAUNCH=1 MPISPAWN_CHECKIN_PORT=36101 MPISPAWN_MPIRUN_PORT=36101 MPISPAWN_NNODES=64 MPISPAWN_GLOBAL_NPROCS=1024 MPISPAWN_MPIRUN_ID=44295 MPISPAWN_ARGC=4 MPDMAN_KVS_TEMPLATE=kvs_513_c468-601.stampede.tacc.utexas.edu_44295 MPISPAWN_LOCAL_NPROCS=16 MPISPAWN_ARGV_0='/home1/03222/tg825207/parallelnbody/teamscatter' MPISPAWN_ARGV_1='256000' MPISPAWN_ARGV_2='-c' MPISPAWN_ARGV_3='2' MPISPAWN_ARGC=4 MPISPAWN_GENERIC_ENV_COUNT=106 MPISPAWN_GENERIC_NAME_0=TACC_ENVLEN MPISPAWN_GENERIC_VALUE_0=6578 MPISPAWN_GENERIC_NAME_1=SLURM_JOB_ID MPISPAWN_GENERIC_VALUE_1="4306264" MPISPAWN_GENERIC_NAME_2=SLURMD_NODENAME MPISPAWN_GENERIC_VALUE_2="c468-601" MPISPAWN_GENERIC_NAME_3=STOCKYARD MPISPAWN_GENERIC_VALUE_3="/work/03222/tg825207" MPISPAWN_GENERIC_NAME_4=LC_ALL MPISPAWN_GENERIC_VALUE_4="en_US.UTF-8" MPISPAWN_GENERIC_NAME_5=SLURM_CHECKPOINT_IMAGE_DIR MPISPAWN_GENERIC_VALUE_5="/home1/03222/tg825207/parallelnbody" MPISPAWN_GENERIC_NAME_6=LMOD_FAMILY_MPI MPISPAWN_GENERIC_VALUE_6="mvapich2" MPISPAWN_GENERIC_NAME_7=SLURM_JOBID MPISPAWN_GENERIC_VALUE_7="4306264" MPISPAWN_GENERIC_NAME_8=_ModuleTable001_ MPISPAWN_GENERIC_VALUE_8="X01vZHVsZVRhYmxlXz17WyJhY3RpdmVTaXplIl09OCxiYXNlTXBhdGhBPXsiL29wdC9hcHBzL3hzZWRlL21vZHVsZWZpbGVzIiwiL29wdC9hcHBzL21vZHVsZWZpbGVzIiwiL29wdC9tb2R1bGVmaWxlcyIsfSxbImNfcmVidWlsZFRpbWUiXT1mYWxzZSxbImNfc2hvcnRUaW1lIl09ZmFsc2UsZmFtaWx5PXtbIk1QSSJdPSJtdmFwaWNoMiIsWyJjb21waWxlciJdPSJnY2MiLH0saW5hY3RpdmU9e30sbVQ9e0xpbnV4PXtbIkZOIl09Ii9vcHQvbW9kdWxlZmlsZXMvTGludXgiLFsiZGVmYXVsdCJdPTAsWyJmdWxsTmFtZSJdPSJMaW51eCIsWyJsb2FkT3JkZXIiXT0yLHByb3BUPXt9LFsic2hvcnQiXT0iTGludXgiLFsic3RhdHVzIl09ImFjdGl2ZSIsfSxUQUNDPXtbIkZOIl09Ii9v" MPISPAWN_GENERIC_NAME_9=TACC_SYSTEM MPISPAWN_GENERIC_VALUE_9="stampede" MPISPAWN_GENERIC_NAME_10=WORK MPISPAWN_GENERIC_VALUE_10="/work/03222/tg825207" MPISPAWN_GENERIC_NAME_11=SLURM_GTIDS MPISPAWN_GENERIC_VALUE_11="0" MPISPAWN_GENERIC_NAME_12=GCC_LIB MPISPAWN_GENERIC_VALUE_12="/opt/apps/gcc/4.7.1/lib64" MPISPAWN_GENERIC_NAME_13=SLURM_PRIO_PROCESS MPISPAWN_GENERIC_VALUE_13="0" MPISPAWN_GENERIC_NAME_14=SCRATCH MPISPAWN_GENERIC_VALUE_14="/scratch/03222/tg825207" MPISPAWN_GENERIC_NAME_15=LOGNAME MPISPAWN_GENERIC_VALUE_15="tg825207" MPISPAWN_GENERIC_NAME_16=_ModuleTable002_ MPISPAWN_GENERIC_VALUE_16="cHQvbW9kdWxlZmlsZXMvVEFDQyIsWyJkZWZhdWx0Il09MCxbImZ1bGxOYW1lIl09IlRBQ0MiLFsibG9hZE9yZGVyIl09Nixwcm9wVD17fSxbInNob3J0Il09IlRBQ0MiLFsic3RhdHVzIl09ImFjdGl2ZSIsfSxbIlRBQ0MtcGF0aHMiXT17WyJGTiJdPSIvb3B0L21vZHVsZWZpbGVzL1RBQ0MtcGF0aHMiLFsiZGVmYXVsdCJdPTAsWyJmdWxsTmFtZSJdPSJUQUNDLXBhdGhzIixbImxvYWRPcmRlciJdPTEscHJvcFQ9e30sWyJzaG9ydCJdPSJUQUNDLXBhdGhzIixbInN0YXR1cyJdPSJhY3RpdmUiLH0sY2x1c3Rlcj17WyJGTiJdPSIvb3B0L21vZHVsZWZpbGVzL2NsdXN0ZXIiLFsiZGVmYXVsdCJdPTAsWyJmdWxsTmFtZSJdPSJjbHVzdGVyIixbImxvYWRPcmRlciJdPTUscHJvcFQ9" MPISPAWN_GENERIC_NAME_17=INPUTRC MPISPAWN_GENERIC_VALUE_17="/etc/inputrc" MPISPAWN_GENERIC_NAME_18=LMOD_CMD MPISPAWN_GENERIC_VALUE_18="/opt/apps/lmod/lmod/libexec/lmod" MPISPAWN_GENERIC_NAME_19=I_MPI_OFA_ADAPTER_NAME MPISPAWN_GENERIC_VALUE_19="mlx4_0" MPISPAWN_GENERIC_NAME_20=QTLIB MPISPAWN_GENERIC_VALUE_20="/usr/lib64/qt-3.3/lib" MPISPAWN_GENERIC_NAME_21=NODE_TASKS_PPN_INFO MPISPAWN_GENERIC_VALUE_21=""16,0_"" MPISPAWN_GENERIC_NAME_22=SLURM_CPUS_ON_NODE MPISPAWN_GENERIC_VALUE_22="16" MPISPAWN_GENERIC_NAME_23=PATH MPISPAWN_GENERIC_VALUE_23="/opt/apps/gcc4_7/mvapich2/1.9/bin:/opt/apps/gcc/4.7.1/bin:/opt/apps/xalt/0.4.4/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/opt/apps/xsede/gsi-openssh-5.7/bin:/usr/X11R6/bin:/opt/ofed/bin:/opt/ofed/sbin:." MPISPAWN_GENERIC_NAME_24=SLURM_TACC_JOBNAME MPISPAWN_GENERIC_VALUE_24="stampedeLargeScaleAllTS.sh" MPISPAWN_GENERIC_NAME_25=SLURM_NODEID MPISPAWN_GENERIC_VALUE_25="0" MPISPAWN_GENERIC_NAME_26=LMOD_PKG MPISPAWN_GENERIC_VALUE_26="/opt/apps/lmod/lmod" MPISPAWN_GENERIC_NAME_27=MIC_I_MPI_OFA_ADAPTER_NAME MPISPAWN_GENERIC_VALUE_27="mlx4_0" MPISPAWN_GENERIC_NAME_28=HISTSIZE MPISPAWN_GENERIC_VALUE_28="1000" MPISPAWN_GENERIC_NAME_29=SLURM_NTASKS MPISPAWN_GENERIC_VALUE_29="2048" MPISPAWN_GENERIC_NAME_30=SHLIB_PATH MPISPAWN_GENERIC_VALUE_30="/opt/apps/xsede/gsi-openssh-5.7/lib64" MPISPAWN_GENERIC_NAME_31=TACC_OVERRIDE_PROJECT MPISPAWN_GENERIC_VALUE_31="TG-ASC140020" MPISPAWN_GENERIC_NAME_32=_ModuleTable005_ MPISPAWN_GENERIC_VALUE_32="Z2NjNF83L21vZHVsZWZpbGVzIiwiL29wdC9hcHBzL3hzZWRlL21vZHVsZWZpbGVzIiwiL29wdC9hcHBzL21vZHVsZWZpbGVzIiwiL29wdC9tb2R1bGVmaWxlcyIsfSxbInN5c3RlbUJhc2VNUEFUSCJdPSIvb3B0L2FwcHMveHNlZGUvbW9kdWxlZmlsZXM6L29wdC9hcHBzL21vZHVsZWZpbGVzOi9vcHQvbW9kdWxlZmlsZXMiLFsidmVyc2lvbiJdPTIsfQ==" MPISPAWN_GENERIC_NAME_33=_ModuleTable_Sz_ MPISPAWN_GENERIC_VALUE_33="5" MPISPAWN_GENERIC_NAME_34=MAIL MPISPAWN_GENERIC_VALUE_34="/var/spool/mail/tg825207" MPISPAWN_GENERIC_NAME_35=MV2_USE_HUGEPAGES MPISPAWN_GENERIC_VALUE_35="0" MPISPAWN_GENERIC_NAME_36=SLURM_TOPOLOGY_ADDR MPISPAWN_GENERIC_VALUE_36="c468-601" MPISPAWN_GENERIC_NAME_37=TACC_XALT_DIR MPISPAWN_GENERIC_VALUE_37="/opt/apps/xalt/0.4.4/" MPISPAWN_GENERIC_NAME_38=QTDIR MPISPAWN_GENERIC_VALUE_38="/usr/lib64/qt-3.3" MPISPAWN_GENERIC_NAME_39=SYSTEM MPISPAWN_GENERIC_VALUE_39="linux" MPISPAWN_GENERIC_NAME_40=SLURM_TACC_RUNLIMIT_MINS MPISPAWN_GENERIC_VALUE_40="180" MPISPAWN_GENERIC_NAME_41=SLURM_SUBMIT_DIR MPISPAWN_GENERIC_VALUE_41="/home1/03222/tg825207/parallelnbody" MPISPAWN_GENERIC_NAME_42=LIBPATH MPISPAWN_GENERIC_VALUE_42="/opt/apps/xsede/gsi-openssh-5.7/lib64:/usr/lib:/lib" MPISPAWN_GENERIC_NAME_43=HOSTNAME MPISPAWN_GENERIC_VALUE_43="c468-601" MPISPAWN_GENERIC_NAME_44=LMOD_DEFAULT_MODULEPATH MPISPAWN_GENERIC_VALUE_44="/opt/apps/xsede/modulefiles:/opt/apps/modulefiles:/opt/modulefiles" MPISPAWN_GENERIC_NAME_45=ARCHIVE MPISPAWN_GENERIC_VALUE_45="/home/03222/tg825207" MPISPAWN_GENERIC_NAME_46=_ MPISPAWN_GENERIC_VALUE_46="/usr/local/bin/build_env.pl" MPISPAWN_GENERIC_NAME_47=__BASHRC_SOURCED__ MPISPAWN_GENERIC_VALUE_47="1" MPISPAWN_GENERIC_NAME_48=APPS MPISPAWN_GENERIC_VALUE_48="/opt/apps" MPISPAWN_GENERIC_NAME_49=SHELL MPISPAWN_GENERIC_VALUE_49="/bin/bash" MPISPAWN_GENERIC_NAME_50=ARMCI_OPENIB_DEVICE MPISPAWN_GENERIC_VALUE_50="mlx4_0" MPISPAWN_GENERIC_NAME_51=ENVIRONMENT MPISPAWN_GENERIC_VALUE_51="BATCH" MPISPAWN_GENERIC_NAME_52=TACC_FAMILY_MPI MPISPAWN_GENERIC_VALUE_52="mvapich2" MPISPAWN_GENERIC_NAME_53=MV2_IBA_HCA MPISPAWN_GENERIC_VALUE_53="mlx4_0" MPISPAWN_GENERIC_NAME_54=TACC_PUBLIC_MACHINE MPISPAWN_GENERIC_VALUE_54="1" MPISPAWN_GENERIC_NAME_55=SLURM_TACC_NODES MPISPAWN_GENERIC_VALUE_55="1" MPISPAWN_GENERIC_NAME_56=_ModuleTable003_ MPISPAWN_GENERIC_VALUE_56="e30sWyJzaG9ydCJdPSJjbHVzdGVyIixbInN0YXR1cyJdPSJhY3RpdmUiLH0sWyJjbHVzdGVyLXBhdGhzIl09e1siRk4iXT0iL29wdC9tb2R1bGVmaWxlcy9jbHVzdGVyLXBhdGhzIixbImRlZmF1bHQiXT0wLFsiZnVsbE5hbWUiXT0iY2x1c3Rlci1wYXRocyIsWyJsb2FkT3JkZXIiXT0zLHByb3BUPXt9LFsic2hvcnQiXT0iY2x1c3Rlci1wYXRocyIsWyJzdGF0dXMiXT0iYWN0aXZlIix9LGdjYz17WyJGTiJdPSIvb3B0L2FwcHMvbW9kdWxlZmlsZXMvZ2NjLzQuNy4xLmx1YSIsWyJkZWZhdWx0Il09MCxbImZ1bGxOYW1lIl09ImdjYy80LjcuMSIsWyJsb2FkT3JkZXIiXT03LHByb3BUPXt9LFsic2hvcnQiXT0iZ2NjIixbInN0YXR1cyJdPSJhY3RpdmUiLH0sbXZhcGljaDI9e1si" MPISPAWN_GENERIC_NAME_57=QTINC MPISPAWN_GENERIC_VALUE_57="/usr/lib64/qt-3.3/include" MPISPAWN_GENERIC_NAME_58=CVS_RSH MPISPAWN_GENERIC_VALUE_58="ssh" MPISPAWN_GENERIC_NAME_59=MV2_USE_OLD_BCAST MPISPAWN_GENERIC_VALUE_59="1" MPISPAWN_GENERIC_NAME_60=SLURM_JOB_NUM_NODES MPISPAWN_GENERIC_VALUE_60="128" MPISPAWN_GENERIC_NAME_61=SLURM_TASK_PID MPISPAWN_GENERIC_VALUE_61="31479" MPISPAWN_GENERIC_NAME_62=TACC_MPI_GETMODE MPISPAWN_GENERIC_VALUE_62="mvapich2_ssh" MPISPAWN_GENERIC_NAME_63=LANG MPISPAWN_GENERIC_VALUE_63="en_US.UTF-8" MPISPAWN_GENERIC_NAME_64=USER MPISPAWN_GENERIC_VALUE_64="tg825207" MPISPAWN_GENERIC_NAME_65=MIC_I_MPI_DAPL_PROVIDER MPISPAWN_GENERIC_VALUE_65="ofa-v2-mlx4_0-1u" MPISPAWN_GENERIC_NAME_66=G_BROKEN_FILENAMES MPISPAWN_GENERIC_VALUE_66="1" MPISPAWN_GENERIC_NAME_67=SLURM_LOCALID MPISPAWN_GENERIC_VALUE_67="0" MPISPAWN_GENERIC_NAME_68=SLURM_NPROCS MPISPAWN_GENERIC_VALUE_68="2048" MPISPAWN_GENERIC_NAME_69=SLURM_TACC_NNODES_SET MPISPAWN_GENERIC_VALUE_69="0" MPISPAWN_GENERIC_NAME_70=SLURM_TACC_CORES MPISPAWN_GENERIC_VALUE_70="2048" MPISPAWN_GENERIC_NAME_71=LMOD_DIR MPISPAWN_GENERIC_VALUE_71="/opt/apps/lmod/lmod/libexec" MPISPAWN_GENERIC_NAME_72=LMOD_SETTARG_CMD MPISPAWN_GENERIC_VALUE_72=":" MPISPAWN_GENERIC_NAME_73=OMP_NUM_THREADS MPISPAWN_GENERIC_VALUE_73="1" MPISPAWN_GENERIC_NAME_74=_ModuleTable004_ MPISPAWN_GENERIC_VALUE_74="Rk4iXT0iL29wdC9hcHBzL2djYzRfNy9tb2R1bGVmaWxlcy9tdmFwaWNoMi8xLjlhMiIsWyJkZWZhdWx0Il09MSxbImZ1bGxOYW1lIl09Im12YXBpY2gyLzEuOWEyIixbImxvYWRPcmRlciJdPTgscHJvcFQ9e30sWyJzaG9ydCJdPSJtdmFwaWNoMiIsWyJzdGF0dXMiXT0iYWN0aXZlIix9LHhhbHQ9e1siRk4iXT0iL29wdC9hcHBzL21vZHVsZWZpbGVzL3hhbHQvMC40LjQubHVhIixbImRlZmF1bHQiXT0xLFsiZnVsbE5hbWUiXT0ieGFsdC8wLjQuNCIsWyJsb2FkT3JkZXIiXT00LHByb3BUPXt9LFsic2hvcnQiXT0ieGFsdCIsWyJzdGF0dXMiXT0iYWN0aXZlIix9LH0sbXBhdGhBPXsiL29wdC9hcHBzL2djYzRfNy9tdmFwaWNoMl8xXzkvbW9kdWxlZmlsZXMiLCIvb3B0L2FwcHMv" MPISPAWN_GENERIC_NAME_75=__PERSONAL_PATH__ MPISPAWN_GENERIC_VALUE_75="1" MPISPAWN_GENERIC_NAME_76=LMOD_SYSTEM_DEFAULT_MODULES MPISPAWN_GENERIC_VALUE_76="TACC" MPISPAWN_GENERIC_NAME_77=SLURM_JOB_NAME MPISPAWN_GENERIC_VALUE_77="stampedeLargeScaleAllTS.sh" MPISPAWN_GENERIC_NAME_78=SLURM_SUBMIT_HOST MPISPAWN_GENERIC_VALUE_78="login4.stampede.tacc.utexas.edu" MPISPAWN_GENERIC_NAME_79=LD_LIBRARY_PATH MPISPAWN_GENERIC_VALUE_79="/opt/apps/gcc4_7/mvapich2/1.9/lib:/opt/apps/gcc4_7/mvapich2/1.9/lib/shared:/opt/apps/gcc/4.7.1/lib64:/opt/apps/gcc/4.7.1/lib:/opt/apps/xsede/gsi-openssh-5.7/lib64:/opt/apps/xsede/gsi-openssh-5.7/lib64" MPISPAWN_GENERIC_NAME_80=SLURM_QUEUE MPISPAWN_GENERIC_VALUE_80="normal" MPISPAWN_GENERIC_NAME_81=PKG_CONFIG_PATH MPISPAWN_GENERIC_VALUE_81="/opt/apps/gcc4_7/mvapich2/1.9/lib/pkgconfig" MPISPAWN_GENERIC_NAME_82=LMOD_PREPEND_BLOCK MPISPAWN_GENERIC_VALUE_82="normal" MPISPAWN_GENERIC_NAME_83=TACC_FAMILY_COMPILER MPISPAWN_GENERIC_VALUE_83="gcc" MPISPAWN_GENERIC_NAME_84=TACC_DOMAIN MPISPAWN_GENERIC_VALUE_84="stampede" MPISPAWN_GENERIC_NAME_85=MV2_HOMOGENEOUS_CLUSTER MPISPAWN_GENERIC_VALUE_85="1" MPISPAWN_GENERIC_NAME_86=LMOD_FAMILY_COMPILER MPISPAWN_GENERIC_VALUE_86="gcc" MPISPAWN_GENERIC_NAME_87=MV2_USE_UD_HYBRID MPISPAWN_GENERIC_VALUE_87="0" MPISPAWN_GENERIC_NAME_88=MPICH_HOME MPISPAWN_GENERIC_VALUE_88="/opt/apps/gcc4_7/mvapich2/1.9" MPISPAWN_GENERIC_NAME_89=SLURM_JOB_NODELIST MPISPAWN_GENERIC_VALUE_89="c468-601,c471-503,c472-[401-402],c473-[303-304,401,804,901],c474-[203-204,301,401,403-404],c478-303,c482-404,c483-[001-004,101,504,601,804,901],c484-[101,103-104,201,602,701],c485-502,c486-102,c490-[302,904],c491-004,c492-701,c493-[001,102,201],c494-[101,402-403,602,702-703,904],c495-[001-002,201],c497-[102,202,504,601-603],c498-[304,402,603],c501-[204,301,604,801],c502-104,c503-901,c504-[602,604,702,801],c505-[001,202-204,401,502-504,802,901-904],c506-[302,704],c507-[102,202,503,601,904],c508-[101,204,601,702-703,802,903-904],c509-[002,201,301,303,401,501-502],c510-[003,401,404,501,503],c511-[601-602,902-903],c536-[801,902],c537-902,c538-[001,003,104],c540-[104,201-202,804,901-904]" MPISPAWN_GENERIC_NAME_90=SLURM_PROCID MPISPAWN_GENERIC_VALUE_90="0" MPISPAWN_GENERIC_NAME_91=I_MPI_DAPL_PROVIDER MPISPAWN_GENERIC_VALUE_91="ofa-v2-mlx4_0-1u" MPISPAWN_GENERIC_NAME_92=GLOBUS_LOCATION MPISPAWN_GENERIC_VALUE_92="/opt/apps/xsede/gsi-openssh-5.7" MPISPAWN_GENERIC_NAME_93=LMOD_COLORIZE MPISPAWN_GENERIC_VALUE_93="yes" MPISPAWN_GENERIC_NAME_94=I_MPI_HYDRA_BRANCH_COUNT MPISPAWN_GENERIC_VALUE_94="6000" MPISPAWN_GENERIC_NAME_95=SLURM_NNODES MPISPAWN_GENERIC_VALUE_95="128" MPISPAWN_GENERIC_NAME_96=MV2_DEFAULT_TIME_OUT MPISPAWN_GENERIC_VALUE_96="23" MPISPAWN_GENERIC_NAME_97=SLURM_TACC_NCORES_SET MPISPAWN_GENERIC_VALUE_97="1" MPISPAWN_GENERIC_NAME_98=__Init_Default_Modules MPISPAWN_GENERIC_VALUE_98="1" MPISPAWN_GENERIC_NAME_99=BASH_ENV MPISPAWN_GENERIC_VALUE_99="/etc/tacc/tacc_functions" MPISPAWN_GENERIC_NAME_100=SLURM_TOPOLOGY_ADDR_PATTERN MPISPAWN_GENERIC_VALUE_100="node" MPISPAWN_GENERIC_NAME_101=MV2_USE_RING_STARTUP MPISPAWN_GENERIC_VALUE_101="0" MPISPAWN_GENERIC_NAME_102=TACC_XALT_BIN MPISPAWN_GENERIC_VALUE_102="/opt/apps/xalt/0.4.4/bin" MPISPAWN_GENERIC_NAME_103=X509_USER_PROXY MPISPAWN_GENERIC_VALUE_103="/tmp/x509up_p3084.file9zM3fu.1" MPISPAWN_GENERIC_NAME_104=TMPDIR MPISPAWN_GENERIC_VALUE_104="/tmp" MPISPAWN_GENERIC_NAME_105=LMOD_FULL_SETTARG_SUPPORT MPISPAWN_GENERIC_VALUE_105="no" MPISPAWN_ID=56 MPISPAWN_WORKING_DIR=/home1/03222/tg825207/parallelnbody MPISPAWN_MPIRUN_RANK_0=896 MPISPAWN_MPIRUN_RANK_1=897 MPISPAWN_MPIRUN_RANK_2=898 MPISPAWN_MPIRUN_RANK_3=899 MPISPAWN_MPIRUN_RANK_4=900 MPISPAWN_MPIRUN_RANK_5=901 MPISPAWN_MPIRUN_RANK_6=902 MPISPAWN_MPIRUN_RANK_7=903 MPISPAWN_MPIRUN_RANK_8=904 MPISPAWN_MPIRUN_RANK_9=905 MPISPAWN_MPIRUN_RANK_10=906 MPISPAWN_MPIRUN_RANK_11=907 MPISPAWN_MPIRUN_RANK_12=908 MPISPAWN_MPIRUN_RANK_13=909 MPISPAWN_MPIRUN_RANK_14=910 MPISPAWN_MPIRUN_RANK_15=911 /opt/apps/gcc4_7/mvapich2/1.9/bin/mpispawn 0
[c483-002.stampede.tacc.utexas.edu:mpispawn_18][error_sighandler] Caught error: Segmentation fault (signal 11)
[c483-901.stampede.tacc.utexas.edu:mpispawn_25][error_sighandler] Caught error: Segmentation fault (signal 11)
bash: line 1: 14323 Segmentation fault      /usr/bin/env LD_LIBRARY_PATH=/opt/apps/gcc4_7/mvapich2/1.9/lib:/opt/apps/gcc4_7/mvapich2/1.9/lib/shared:/opt/apps/gcc/4.7.1/lib64:/opt/apps/gcc/4.7.1/lib:/opt/apps/xsede/gsi-openssh-5.7/lib64:/opt/apps/xsede/gsi-openssh-5.7/lib64 MPISPAWN_MPIRUN_MPD=0 USE_LINEAR_SSH=1 MPISPAWN_MPIRUN_HOST=c468-601.stampede.tacc.utexas.edu MPISPAWN_MPIRUN_HOSTIP=129.114.76.165 MPIRUN_RSH_LAUNCH=1 MPISPAWN_CHECKIN_PORT=36101 MPISPAWN_MPIRUN_PORT=36101 MPISPAWN_NNODES=64 MPISPAWN_GLOBAL_NPROCS=1024 MPISPAWN_MPIRUN_ID=44295 MPISPAWN_ARGC=4 MPDMAN_KVS_TEMPLATE=kvs_513_c468-601.stampede.tacc.utexas.edu_44295 MPISPAWN_LOCAL_NPROCS=16 MPISPAWN_ARGV_0='/home1/03222/tg825207/parallelnbody/teamscatter' MPISPAWN_ARGV_1='256000' MPISPAWN_ARGV_2='-c' MPISPAWN_ARGV_3='2' MPISPAWN_ARGC=4 MPISPAWN_GENERIC_ENV_COUNT=106 MPISPAWN_GENERIC_NAME_0=TACC_ENVLEN MPISPAWN_GENERIC_VALUE_0=6578 MPISPAWN_GENERIC_NAME_1=SLURM_JOB_ID MPISPAWN_GENERIC_VALUE_1="4306264" MPISPAWN_GENERIC_NAME_2=SLURMD_NODENAME MPISPAWN_GENERIC_VALUE_2="c468-601" MPISPAWN_GENERIC_NAME_3=STOCKYARD MPISPAWN_GENERIC_VALUE_3="/work/03222/tg825207" MPISPAWN_GENERIC_NAME_4=LC_ALL MPISPAWN_GENERIC_VALUE_4="en_US.UTF-8" MPISPAWN_GENERIC_NAME_5=SLURM_CHECKPOINT_IMAGE_DIR MPISPAWN_GENERIC_VALUE_5="/home1/03222/tg825207/parallelnbody" MPISPAWN_GENERIC_NAME_6=LMOD_FAMILY_MPI MPISPAWN_GENERIC_VALUE_6="mvapich2" MPISPAWN_GENERIC_NAME_7=SLURM_JOBID MPISPAWN_GENERIC_VALUE_7="4306264" MPISPAWN_GENERIC_NAME_8=_ModuleTable001_ MPISPAWN_GENERIC_VALUE_8="X01vZHVsZVRhYmxlXz17WyJhY3RpdmVTaXplIl09OCxiYXNlTXBhdGhBPXsiL29wdC9hcHBzL3hzZWRlL21vZHVsZWZpbGVzIiwiL29wdC9hcHBzL21vZHVsZWZpbGVzIiwiL29wdC9tb2R1bGVmaWxlcyIsfSxbImNfcmVidWlsZFRpbWUiXT1mYWxzZSxbImNfc2hvcnRUaW1lIl09ZmFsc2UsZmFtaWx5PXtbIk1QSSJdPSJtdmFwaWNoMiIsWyJjb21waWxlciJdPSJnY2MiLH0saW5hY3RpdmU9e30sbVQ9e0xpbnV4PXtbIkZOIl09Ii9vcHQvbW9kdWxlZmlsZXMvTGludXgiLFsiZGVmYXVsdCJdPTAsWyJmdWxsTmFtZSJdPSJMaW51eCIsWyJsb2FkT3JkZXIiXT0yLHByb3BUPXt9LFsic2hvcnQiXT0iTGludXgiLFsic3RhdHVzIl09ImFjdGl2ZSIsfSxUQUNDPXtbIkZOIl09Ii9v" MPISPAWN_GENERIC_NAME_9=TACC_SYSTEM MPISPAWN_GENERIC_VALUE_9="stampede" MPISPAWN_GENERIC_NAME_10=WORK MPISPAWN_GENERIC_VALUE_10="/work/03222/tg825207" MPISPAWN_GENERIC_NAME_11=SLURM_GTIDS MPISPAWN_GENERIC_VALUE_11="0" MPISPAWN_GENERIC_NAME_12=GCC_LIB MPISPAWN_GENERIC_VALUE_12="/opt/apps/gcc/4.7.1/lib64" MPISPAWN_GENERIC_NAME_13=SLURM_PRIO_PROCESS MPISPAWN_GENERIC_VALUE_13="0" MPISPAWN_GENERIC_NAME_14=SCRATCH MPISPAWN_GENERIC_VALUE_14="/scratch/03222/tg825207" MPISPAWN_GENERIC_NAME_15=LOGNAME MPISPAWN_GENERIC_VALUE_15="tg825207" MPISPAWN_GENERIC_NAME_16=_ModuleTable002_ MPISPAWN_GENERIC_VALUE_16="cHQvbW9kdWxlZmlsZXMvVEFDQyIsWyJkZWZhdWx0Il09MCxbImZ1bGxOYW1lIl09IlRBQ0MiLFsibG9hZE9yZGVyIl09Nixwcm9wVD17fSxbInNob3J0Il09IlRBQ0MiLFsic3RhdHVzIl09ImFjdGl2ZSIsfSxbIlRBQ0MtcGF0aHMiXT17WyJGTiJdPSIvb3B0L21vZHVsZWZpbGVzL1RBQ0MtcGF0aHMiLFsiZGVmYXVsdCJdPTAsWyJmdWxsTmFtZSJdPSJUQUNDLXBhdGhzIixbImxvYWRPcmRlciJdPTEscHJvcFQ9e30sWyJzaG9ydCJdPSJUQUNDLXBhdGhzIixbInN0YXR1cyJdPSJhY3RpdmUiLH0sY2x1c3Rlcj17WyJGTiJdPSIvb3B0L21vZHVsZWZpbGVzL2NsdXN0ZXIiLFsiZGVmYXVsdCJdPTAsWyJmdWxsTmFtZSJdPSJjbHVzdGVyIixbImxvYWRPcmRlciJdPTUscHJvcFQ9" MPISPAWN_GENERIC_NAME_17=INPUTRC MPISPAWN_GENERIC_VALUE_17="/etc/inputrc" MPISPAWN_GENERIC_NAME_18=LMOD_CMD MPISPAWN_GENERIC_VALUE_18="/opt/apps/lmod/lmod/libexec/lmod" MPISPAWN_GENERIC_NAME_19=I_MPI_OFA_ADAPTER_NAME MPISPAWN_GENERIC_VALUE_19="mlx4_0" MPISPAWN_GENERIC_NAME_20=QTLIB MPISPAWN_GENERIC_VALUE_20="/usr/lib64/qt-3.3/lib" MPISPAWN_GENERIC_NAME_21=NODE_TASKS_PPN_INFO MPISPAWN_GENERIC_VALUE_21=""16,0_"" MPISPAWN_GENERIC_NAME_22=SLURM_CPUS_ON_NODE MPISPAWN_GENERIC_VALUE_22="16" MPISPAWN_GENERIC_NAME_23=PATH MPISPAWN_GENERIC_VALUE_23="/opt/apps/gcc4_7/mvapich2/1.9/bin:/opt/apps/gcc/4.7.1/bin:/opt/apps/xalt/0.4.4/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/opt/apps/xsede/gsi-openssh-5.7/bin:/usr/X11R6/bin:/opt/ofed/bin:/opt/ofed/sbin:." MPISPAWN_GENERIC_NAME_24=SLURM_TACC_JOBNAME MPISPAWN_GENERIC_VALUE_24="stampedeLargeScaleAllTS.sh" MPISPAWN_GENERIC_NAME_25=SLURM_NODEID MPISPAWN_GENERIC_VALUE_25="0" MPISPAWN_GENERIC_NAME_26=LMOD_PKG MPISPAWN_GENERIC_VALUE_26="/opt/apps/lmod/lmod" MPISPAWN_GENERIC_NAME_27=MIC_I_MPI_OFA_ADAPTER_NAME MPISPAWN_GENERIC_VALUE_27="mlx4_0" MPISPAWN_GENERIC_NAME_28=HISTSIZE MPISPAWN_GENERIC_VALUE_28="1000" MPISPAWN_GENERIC_NAME_29=SLURM_NTASKS MPISPAWN_GENERIC_VALUE_29="2048" MPISPAWN_GENERIC_NAME_30=SHLIB_PATH MPISPAWN_GENERIC_VALUE_30="/opt/apps/xsede/gsi-openssh-5.7/lib64" MPISPAWN_GENERIC_NAME_31=TACC_OVERRIDE_PROJECT MPISPAWN_GENERIC_VALUE_31="TG-ASC140020" MPISPAWN_GENERIC_NAME_32=_ModuleTable005_ MPISPAWN_GENERIC_VALUE_32="Z2NjNF83L21vZHVsZWZpbGVzIiwiL29wdC9hcHBzL3hzZWRlL21vZHVsZWZpbGVzIiwiL29wdC9hcHBzL21vZHVsZWZpbGVzIiwiL29wdC9tb2R1bGVmaWxlcyIsfSxbInN5c3RlbUJhc2VNUEFUSCJdPSIvb3B0L2FwcHMveHNlZGUvbW9kdWxlZmlsZXM6L29wdC9hcHBzL21vZHVsZWZpbGVzOi9vcHQvbW9kdWxlZmlsZXMiLFsidmVyc2lvbiJdPTIsfQ==" MPISPAWN_GENERIC_NAME_33=_ModuleTable_Sz_ MPISPAWN_GENERIC_VALUE_33="5" MPISPAWN_GENERIC_NAME_34=MAIL MPISPAWN_GENERIC_VALUE_34="/var/spool/mail/tg825207" MPISPAWN_GENERIC_NAME_35=MV2_USE_HUGEPAGES MPISPAWN_GENERIC_VALUE_35="0" MPISPAWN_GENERIC_NAME_36=SLURM_TOPOLOGY_ADDR MPISPAWN_GENERIC_VALUE_36="c468-601" MPISPAWN_GENERIC_NAME_37=TACC_XALT_DIR MPISPAWN_GENERIC_VALUE_37="/opt/apps/xalt/0.4.4/" MPISPAWN_GENERIC_NAME_38=QTDIR MPISPAWN_GENERIC_VALUE_38="/usr/lib64/qt-3.3" MPISPAWN_GENERIC_NAME_39=SYSTEM MPISPAWN_GENERIC_VALUE_39="linux" MPISPAWN_GENERIC_NAME_40=SLURM_TACC_RUNLIMIT_MINS MPISPAWN_GENERIC_VALUE_40="180" MPISPAWN_GENERIC_NAME_41=SLURM_SUBMIT_DIR MPISPAWN_GENERIC_VALUE_41="/home1/03222/tg825207/parallelnbody" MPISPAWN_GENERIC_NAME_42=LIBPATH MPISPAWN_GENERIC_VALUE_42="/opt/apps/xsede/gsi-openssh-5.7/lib64:/usr/lib:/lib" MPISPAWN_GENERIC_NAME_43=HOSTNAME MPISPAWN_GENERIC_VALUE_43="c468-601" MPISPAWN_GENERIC_NAME_44=LMOD_DEFAULT_MODULEPATH MPISPAWN_GENERIC_VALUE_44="/opt/apps/xsede/modulefiles:/opt/apps/modulefiles:/opt/modulefiles" MPISPAWN_GENERIC_NAME_45=ARCHIVE MPISPAWN_GENERIC_VALUE_45="/home/03222/tg825207" MPISPAWN_GENERIC_NAME_46=_ MPISPAWN_GENERIC_VALUE_46="/usr/local/bin/build_env.pl" MPISPAWN_GENERIC_NAME_47=__BASHRC_SOURCED__ MPISPAWN_GENERIC_VALUE_47="1" MPISPAWN_GENERIC_NAME_48=APPS MPISPAWN_GENERIC_VALUE_48="/opt/apps" MPISPAWN_GENERIC_NAME_49=SHELL MPISPAWN_GENERIC_VALUE_49="/bin/bash" MPISPAWN_GENERIC_NAME_50=ARMCI_OPENIB_DEVICE MPISPAWN_GENERIC_VALUE_50="mlx4_0" MPISPAWN_GENERIC_NAME_51=ENVIRONMENT MPISPAWN_GENERIC_VALUE_51="BATCH" MPISPAWN_GENERIC_NAME_52=TACC_FAMILY_MPI MPISPAWN_GENERIC_VALUE_52="mvapich2" MPISPAWN_GENERIC_NAME_53=MV2_IBA_HCA MPISPAWN_GENERIC_VALUE_53="mlx4_0" MPISPAWN_GENERIC_NAME_54=TACC_PUBLIC_MACHINE MPISPAWN_GENERIC_VALUE_54="1" MPISPAWN_GENERIC_NAME_55=SLURM_TACC_NODES MPISPAWN_GENERIC_VALUE_55="1" MPISPAWN_GENERIC_NAME_56=_ModuleTable003_ MPISPAWN_GENERIC_VALUE_56="e30sWyJzaG9ydCJdPSJjbHVzdGVyIixbInN0YXR1cyJdPSJhY3RpdmUiLH0sWyJjbHVzdGVyLXBhdGhzIl09e1siRk4iXT0iL29wdC9tb2R1bGVmaWxlcy9jbHVzdGVyLXBhdGhzIixbImRlZmF1bHQiXT0wLFsiZnVsbE5hbWUiXT0iY2x1c3Rlci1wYXRocyIsWyJsb2FkT3JkZXIiXT0zLHByb3BUPXt9LFsic2hvcnQiXT0iY2x1c3Rlci1wYXRocyIsWyJzdGF0dXMiXT0iYWN0aXZlIix9LGdjYz17WyJGTiJdPSIvb3B0L2FwcHMvbW9kdWxlZmlsZXMvZ2NjLzQuNy4xLmx1YSIsWyJkZWZhdWx0Il09MCxbImZ1bGxOYW1lIl09ImdjYy80LjcuMSIsWyJsb2FkT3JkZXIiXT03LHByb3BUPXt9LFsic2hvcnQiXT0iZ2NjIixbInN0YXR1cyJdPSJhY3RpdmUiLH0sbXZhcGljaDI9e1si" MPISPAWN_GENERIC_NAME_57=QTINC MPISPAWN_GENERIC_VALUE_57="/usr/lib64/qt-3.3/include" MPISPAWN_GENERIC_NAME_58=CVS_RSH MPISPAWN_GENERIC_VALUE_58="ssh" MPISPAWN_GENERIC_NAME_59=MV2_USE_OLD_BCAST MPISPAWN_GENERIC_VALUE_59="1" MPISPAWN_GENERIC_NAME_60=SLURM_JOB_NUM_NODES MPISPAWN_GENERIC_VALUE_60="128" MPISPAWN_GENERIC_NAME_61=SLURM_TASK_PID MPISPAWN_GENERIC_VALUE_61="31479" MPISPAWN_GENERIC_NAME_62=TACC_MPI_GETMODE MPISPAWN_GENERIC_VALUE_62="mvapich2_ssh" MPISPAWN_GENERIC_NAME_63=LANG MPISPAWN_GENERIC_VALUE_63="en_US.UTF-8" MPISPAWN_GENERIC_NAME_64=USER MPISPAWN_GENERIC_VALUE_64="tg825207" MPISPAWN_GENERIC_NAME_65=MIC_I_MPI_DAPL_PROVIDER MPISPAWN_GENERIC_VALUE_65="ofa-v2-mlx4_0-1u" MPISPAWN_GENERIC_NAME_66=G_BROKEN_FILENAMES MPISPAWN_GENERIC_VALUE_66="1" MPISPAWN_GENERIC_NAME_67=SLURM_LOCALID MPISPAWN_GENERIC_VALUE_67="0" MPISPAWN_GENERIC_NAME_68=SLURM_NPROCS MPISPAWN_GENERIC_VALUE_68="2048" MPISPAWN_GENERIC_NAME_69=SLURM_TACC_NNODES_SET MPISPAWN_GENERIC_VALUE_69="0" MPISPAWN_GENERIC_NAME_70=SLURM_TACC_CORES MPISPAWN_GENERIC_VALUE_70="2048" MPISPAWN_GENERIC_NAME_71=LMOD_DIR MPISPAWN_GENERIC_VALUE_71="/opt/apps/lmod/lmod/libexec" MPISPAWN_GENERIC_NAME_72=LMOD_SETTARG_CMD MPISPAWN_GENERIC_VALUE_72=":" MPISPAWN_GENERIC_NAME_73=OMP_NUM_THREADS MPISPAWN_GENERIC_VALUE_73="1" MPISPAWN_GENERIC_NAME_74=_ModuleTable004_ MPISPAWN_GENERIC_VALUE_74="Rk4iXT0iL29wdC9hcHBzL2djYzRfNy9tb2R1bGVmaWxlcy9tdmFwaWNoMi8xLjlhMiIsWyJkZWZhdWx0Il09MSxbImZ1bGxOYW1lIl09Im12YXBpY2gyLzEuOWEyIixbImxvYWRPcmRlciJdPTgscHJvcFQ9e30sWyJzaG9ydCJdPSJtdmFwaWNoMiIsWyJzdGF0dXMiXT0iYWN0aXZlIix9LHhhbHQ9e1siRk4iXT0iL29wdC9hcHBzL21vZHVsZWZpbGVzL3hhbHQvMC40LjQubHVhIixbImRlZmF1bHQiXT0xLFsiZnVsbE5hbWUiXT0ieGFsdC8wLjQuNCIsWyJsb2FkT3JkZXIiXT00LHByb3BUPXt9LFsic2hvcnQiXT0ieGFsdCIsWyJzdGF0dXMiXT0iYWN0aXZlIix9LH0sbXBhdGhBPXsiL29wdC9hcHBzL2djYzRfNy9tdmFwaWNoMl8xXzkvbW9kdWxlZmlsZXMiLCIvb3B0L2FwcHMv" MPISPAWN_GENERIC_NAME_75=__PERSONAL_PATH__ MPISPAWN_GENERIC_VALUE_75="1" MPISPAWN_GENERIC_NAME_76=LMOD_SYSTEM_DEFAULT_MODULES MPISPAWN_GENERIC_VALUE_76="TACC" MPISPAWN_GENERIC_NAME_77=SLURM_JOB_NAME MPISPAWN_GENERIC_VALUE_77="stampedeLargeScaleAllTS.sh" MPISPAWN_GENERIC_NAME_78=SLURM_SUBMIT_HOST MPISPAWN_GENERIC_VALUE_78="login4.stampede.tacc.utexas.edu" MPISPAWN_GENERIC_NAME_79=LD_LIBRARY_PATH MPISPAWN_GENERIC_VALUE_79="/opt/apps/gcc4_7/mvapich2/1.9/lib:/opt/apps/gcc4_7/mvapich2/1.9/lib/shared:/opt/apps/gcc/4.7.1/lib64:/opt/apps/gcc/4.7.1/lib:/opt/apps/xsede/gsi-openssh-5.7/lib64:/opt/apps/xsede/gsi-openssh-5.7/lib64" MPISPAWN_GENERIC_NAME_80=SLURM_QUEUE MPISPAWN_GENERIC_VALUE_80="normal" MPISPAWN_GENERIC_NAME_81=PKG_CONFIG_PATH MPISPAWN_GENERIC_VALUE_81="/opt/apps/gcc4_7/mvapich2/1.9/lib/pkgconfig" MPISPAWN_GENERIC_NAME_82=LMOD_PREPEND_BLOCK MPISPAWN_GENERIC_VALUE_82="normal" MPISPAWN_GENERIC_NAME_83=TACC_FAMILY_COMPILER MPISPAWN_GENERIC_VALUE_83="gcc" MPISPAWN_GENERIC_NAME_84=TACC_DOMAIN MPISPAWN_GENERIC_VALUE_84="stampede" MPISPAWN_GENERIC_NAME_85=MV2_HOMOGENEOUS_CLUSTER MPISPAWN_GENERIC_VALUE_85="1" MPISPAWN_GENERIC_NAME_86=LMOD_FAMILY_COMPILER MPISPAWN_GENERIC_VALUE_86="gcc" MPISPAWN_GENERIC_NAME_87=MV2_USE_UD_HYBRID MPISPAWN_GENERIC_VALUE_87="0" MPISPAWN_GENERIC_NAME_88=MPICH_HOME MPISPAWN_GENERIC_VALUE_88="/opt/apps/gcc4_7/mvapich2/1.9" MPISPAWN_GENERIC_NAME_89=SLURM_JOB_NODELIST MPISPAWN_GENERIC_VALUE_89="c468-601,c471-503,c472-[401-402],c473-[303-304,401,804,901],c474-[203-204,301,401,403-404],c478-303,c482-404,c483-[001-004,101,504,601,804,901],c484-[101,103-104,201,602,701],c485-502,c486-102,c490-[302,904],c491-004,c492-701,c493-[001,102,201],c494-[101,402-403,602,702-703,904],c495-[001-002,201],c497-[102,202,504,601-603],c498-[304,402,603],c501-[204,301,604,801],c502-104,c503-901,c504-[602,604,702,801],c505-[001,202-204,401,502-504,802,901-904],c506-[302,704],c507-[102,202,503,601,904],c508-[101,204,601,702-703,802,903-904],c509-[002,201,301,303,401,501-502],c510-[003,401,404,501,503],c511-[601-602,902-903],c536-[801,902],c537-902,c538-[001,003,104],c540-[104,201-202,804,901-904]" MPISPAWN_GENERIC_NAME_90=SLURM_PROCID MPISPAWN_GENERIC_VALUE_90="0" MPISPAWN_GENERIC_NAME_91=I_MPI_DAPL_PROVIDER MPISPAWN_GENERIC_VALUE_91="ofa-v2-mlx4_0-1u" MPISPAWN_GENERIC_NAME_92=GLOBUS_LOCATION MPISPAWN_GENERIC_VALUE_92="/opt/apps/xsede/gsi-openssh-5.7" MPISPAWN_GENERIC_NAME_93=LMOD_COLORIZE MPISPAWN_GENERIC_VALUE_93="yes" MPISPAWN_GENERIC_NAME_94=I_MPI_HYDRA_BRANCH_COUNT MPISPAWN_GENERIC_VALUE_94="6000" MPISPAWN_GENERIC_NAME_95=SLURM_NNODES MPISPAWN_GENERIC_VALUE_95="128" MPISPAWN_GENERIC_NAME_96=MV2_DEFAULT_TIME_OUT MPISPAWN_GENERIC_VALUE_96="23" MPISPAWN_GENERIC_NAME_97=SLURM_TACC_NCORES_SET MPISPAWN_GENERIC_VALUE_97="1" MPISPAWN_GENERIC_NAME_98=__Init_Default_Modules MPISPAWN_GENERIC_VALUE_98="1" MPISPAWN_GENERIC_NAME_99=BASH_ENV MPISPAWN_GENERIC_VALUE_99="/etc/tacc/tacc_functions" MPISPAWN_GENERIC_NAME_100=SLURM_TOPOLOGY_ADDR_PATTERN MPISPAWN_GENERIC_VALUE_100="node" MPISPAWN_GENERIC_NAME_101=MV2_USE_RING_STARTUP MPISPAWN_GENERIC_VALUE_101="0" MPISPAWN_GENERIC_NAME_102=TACC_XALT_BIN MPISPAWN_GENERIC_VALUE_102="/opt/apps/xalt/0.4.4/bin" MPISPAWN_GENERIC_NAME_103=X509_USER_PROXY MPISPAWN_GENERIC_VALUE_103="/tmp/x509up_p3084.file9zM3fu.1" MPISPAWN_GENERIC_NAME_104=TMPDIR MPISPAWN_GENERIC_VALUE_104="/tmp" MPISPAWN_GENERIC_NAME_105=LMOD_FULL_SETTARG_SUPPORT MPISPAWN_GENERIC_VALUE_105="no" MPISPAWN_ID=25 MPISPAWN_WORKING_DIR=/home1/03222/tg825207/parallelnbody MPISPAWN_MPIRUN_RANK_0=400 MPISPAWN_MPIRUN_RANK_1=401 MPISPAWN_MPIRUN_RANK_2=402 MPISPAWN_MPIRUN_RANK_3=403 MPISPAWN_MPIRUN_RANK_4=404 MPISPAWN_MPIRUN_RANK_5=405 MPISPAWN_MPIRUN_RANK_6=406 MPISPAWN_MPIRUN_RANK_7=407 MPISPAWN_MPIRUN_RANK_8=408 MPISPAWN_MPIRUN_RANK_9=409 MPISPAWN_MPIRUN_RANK_10=410 MPISPAWN_MPIRUN_RANK_11=411 MPISPAWN_MPIRUN_RANK_12=412 MPISPAWN_MPIRUN_RANK_13=413 MPISPAWN_MPIRUN_RANK_14=414 MPISPAWN_MPIRUN_RANK_15=415 /opt/apps/gcc4_7/mvapich2/1.9/bin/mpispawn 0
bash: line 1: 103803 Segmentation fault      /usr/bin/env LD_LIBRARY_PATH=/opt/apps/gcc4_7/mvapich2/1.9/lib:/opt/apps/gcc4_7/mvapich2/1.9/lib/shared:/opt/apps/gcc/4.7.1/lib64:/opt/apps/gcc/4.7.1/lib:/opt/apps/xsede/gsi-openssh-5.7/lib64:/opt/apps/xsede/gsi-openssh-5.7/lib64 MPISPAWN_MPIRUN_MPD=0 USE_LINEAR_SSH=1 MPISPAWN_MPIRUN_HOST=c468-601.stampede.tacc.utexas.edu MPISPAWN_MPIRUN_HOSTIP=129.114.76.165 MPIRUN_RSH_LAUNCH=1 MPISPAWN_CHECKIN_PORT=36101 MPISPAWN_MPIRUN_PORT=36101 MPISPAWN_NNODES=64 MPISPAWN_GLOBAL_NPROCS=1024 MPISPAWN_MPIRUN_ID=44295 MPISPAWN_ARGC=4 MPDMAN_KVS_TEMPLATE=kvs_513_c468-601.stampede.tacc.utexas.edu_44295 MPISPAWN_LOCAL_NPROCS=16 MPISPAWN_ARGV_0='/home1/03222/tg825207/parallelnbody/teamscatter' MPISPAWN_ARGV_1='256000' MPISPAWN_ARGV_2='-c' MPISPAWN_ARGV_3='2' MPISPAWN_ARGC=4 MPISPAWN_GENERIC_ENV_COUNT=106 MPISPAWN_GENERIC_NAME_0=TACC_ENVLEN MPISPAWN_GENERIC_VALUE_0=6578 MPISPAWN_GENERIC_NAME_1=SLURM_JOB_ID MPISPAWN_GENERIC_VALUE_1="4306264" MPISPAWN_GENERIC_NAME_2=SLURMD_NODENAME MPISPAWN_GENERIC_VALUE_2="c468-601" MPISPAWN_GENERIC_NAME_3=STOCKYARD MPISPAWN_GENERIC_VALUE_3="/work/03222/tg825207" MPISPAWN_GENERIC_NAME_4=LC_ALL MPISPAWN_GENERIC_VALUE_4="en_US.UTF-8" MPISPAWN_GENERIC_NAME_5=SLURM_CHECKPOINT_IMAGE_DIR MPISPAWN_GENERIC_VALUE_5="/home1/03222/tg825207/parallelnbody" MPISPAWN_GENERIC_NAME_6=LMOD_FAMILY_MPI MPISPAWN_GENERIC_VALUE_6="mvapich2" MPISPAWN_GENERIC_NAME_7=SLURM_JOBID MPISPAWN_GENERIC_VALUE_7="4306264" MPISPAWN_GENERIC_NAME_8=_ModuleTable001_ MPISPAWN_GENERIC_VALUE_8="X01vZHVsZVRhYmxlXz17WyJhY3RpdmVTaXplIl09OCxiYXNlTXBhdGhBPXsiL29wdC9hcHBzL3hzZWRlL21vZHVsZWZpbGVzIiwiL29wdC9hcHBzL21vZHVsZWZpbGVzIiwiL29wdC9tb2R1bGVmaWxlcyIsfSxbImNfcmVidWlsZFRpbWUiXT1mYWxzZSxbImNfc2hvcnRUaW1lIl09ZmFsc2UsZmFtaWx5PXtbIk1QSSJdPSJtdmFwaWNoMiIsWyJjb21waWxlciJdPSJnY2MiLH0saW5hY3RpdmU9e30sbVQ9e0xpbnV4PXtbIkZOIl09Ii9vcHQvbW9kdWxlZmlsZXMvTGludXgiLFsiZGVmYXVsdCJdPTAsWyJmdWxsTmFtZSJdPSJMaW51eCIsWyJsb2FkT3JkZXIiXT0yLHByb3BUPXt9LFsic2hvcnQiXT0iTGludXgiLFsic3RhdHVzIl09ImFjdGl2ZSIsfSxUQUNDPXtbIkZOIl09Ii9v" MPISPAWN_GENERIC_NAME_9=TACC_SYSTEM MPISPAWN_GENERIC_VALUE_9="stampede" MPISPAWN_GENERIC_NAME_10=WORK MPISPAWN_GENERIC_VALUE_10="/work/03222/tg825207" MPISPAWN_GENERIC_NAME_11=SLURM_GTIDS MPISPAWN_GENERIC_VALUE_11="0" MPISPAWN_GENERIC_NAME_12=GCC_LIB MPISPAWN_GENERIC_VALUE_12="/opt/apps/gcc/4.7.1/lib64" MPISPAWN_GENERIC_NAME_13=SLURM_PRIO_PROCESS MPISPAWN_GENERIC_VALUE_13="0" MPISPAWN_GENERIC_NAME_14=SCRATCH MPISPAWN_GENERIC_VALUE_14="/scratch/03222/tg825207" MPISPAWN_GENERIC_NAME_15=LOGNAME MPISPAWN_GENERIC_VALUE_15="tg825207" MPISPAWN_GENERIC_NAME_16=_ModuleTable002_ MPISPAWN_GENERIC_VALUE_16="cHQvbW9kdWxlZmlsZXMvVEFDQyIsWyJkZWZhdWx0Il09MCxbImZ1bGxOYW1lIl09IlRBQ0MiLFsibG9hZE9yZGVyIl09Nixwcm9wVD17fSxbInNob3J0Il09IlRBQ0MiLFsic3RhdHVzIl09ImFjdGl2ZSIsfSxbIlRBQ0MtcGF0aHMiXT17WyJGTiJdPSIvb3B0L21vZHVsZWZpbGVzL1RBQ0MtcGF0aHMiLFsiZGVmYXVsdCJdPTAsWyJmdWxsTmFtZSJdPSJUQUNDLXBhdGhzIixbImxvYWRPcmRlciJdPTEscHJvcFQ9e30sWyJzaG9ydCJdPSJUQUNDLXBhdGhzIixbInN0YXR1cyJdPSJhY3RpdmUiLH0sY2x1c3Rlcj17WyJGTiJdPSIvb3B0L21vZHVsZWZpbGVzL2NsdXN0ZXIiLFsiZGVmYXVsdCJdPTAsWyJmdWxsTmFtZSJdPSJjbHVzdGVyIixbImxvYWRPcmRlciJdPTUscHJvcFQ9" MPISPAWN_GENERIC_NAME_17=INPUTRC MPISPAWN_GENERIC_VALUE_17="/etc/inputrc" MPISPAWN_GENERIC_NAME_18=LMOD_CMD MPISPAWN_GENERIC_VALUE_18="/opt/apps/lmod/lmod/libexec/lmod" MPISPAWN_GENERIC_NAME_19=I_MPI_OFA_ADAPTER_NAME MPISPAWN_GENERIC_VALUE_19="mlx4_0" MPISPAWN_GENERIC_NAME_20=QTLIB MPISPAWN_GENERIC_VALUE_20="/usr/lib64/qt-3.3/lib" MPISPAWN_GENERIC_NAME_21=NODE_TASKS_PPN_INFO MPISPAWN_GENERIC_VALUE_21=""16,0_"" MPISPAWN_GENERIC_NAME_22=SLURM_CPUS_ON_NODE MPISPAWN_GENERIC_VALUE_22="16" MPISPAWN_GENERIC_NAME_23=PATH MPISPAWN_GENERIC_VALUE_23="/opt/apps/gcc4_7/mvapich2/1.9/bin:/opt/apps/gcc/4.7.1/bin:/opt/apps/xalt/0.4.4/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/opt/apps/xsede/gsi-openssh-5.7/bin:/usr/X11R6/bin:/opt/ofed/bin:/opt/ofed/sbin:." MPISPAWN_GENERIC_NAME_24=SLURM_TACC_JOBNAME MPISPAWN_GENERIC_VALUE_24="stampedeLargeScaleAllTS.sh" MPISPAWN_GENERIC_NAME_25=SLURM_NODEID MPISPAWN_GENERIC_VALUE_25="0" MPISPAWN_GENERIC_NAME_26=LMOD_PKG MPISPAWN_GENERIC_VALUE_26="/opt/apps/lmod/lmod" MPISPAWN_GENERIC_NAME_27=MIC_I_MPI_OFA_ADAPTER_NAME MPISPAWN_GENERIC_VALUE_27="mlx4_0" MPISPAWN_GENERIC_NAME_28=HISTSIZE MPISPAWN_GENERIC_VALUE_28="1000" MPISPAWN_GENERIC_NAME_29=SLURM_NTASKS MPISPAWN_GENERIC_VALUE_29="2048" MPISPAWN_GENERIC_NAME_30=SHLIB_PATH MPISPAWN_GENERIC_VALUE_30="/opt/apps/xsede/gsi-openssh-5.7/lib64" MPISPAWN_GENERIC_NAME_31=TACC_OVERRIDE_PROJECT MPISPAWN_GENERIC_VALUE_31="TG-ASC140020" MPISPAWN_GENERIC_NAME_32=_ModuleTable005_ MPISPAWN_GENERIC_VALUE_32="Z2NjNF83L21vZHVsZWZpbGVzIiwiL29wdC9hcHBzL3hzZWRlL21vZHVsZWZpbGVzIiwiL29wdC9hcHBzL21vZHVsZWZpbGVzIiwiL29wdC9tb2R1bGVmaWxlcyIsfSxbInN5c3RlbUJhc2VNUEFUSCJdPSIvb3B0L2FwcHMveHNlZGUvbW9kdWxlZmlsZXM6L29wdC9hcHBzL21vZHVsZWZpbGVzOi9vcHQvbW9kdWxlZmlsZXMiLFsidmVyc2lvbiJdPTIsfQ==" MPISPAWN_GENERIC_NAME_33=_ModuleTable_Sz_ MPISPAWN_GENERIC_VALUE_33="5" MPISPAWN_GENERIC_NAME_34=MAIL MPISPAWN_GENERIC_VALUE_34="/var/spool/mail/tg825207" MPISPAWN_GENERIC_NAME_35=MV2_USE_HUGEPAGES MPISPAWN_GENERIC_VALUE_35="0" MPISPAWN_GENERIC_NAME_36=SLURM_TOPOLOGY_ADDR MPISPAWN_GENERIC_VALUE_36="c468-601" MPISPAWN_GENERIC_NAME_37=TACC_XALT_DIR MPISPAWN_GENERIC_VALUE_37="/opt/apps/xalt/0.4.4/" MPISPAWN_GENERIC_NAME_38=QTDIR MPISPAWN_GENERIC_VALUE_38="/usr/lib64/qt-3.3" MPISPAWN_GENERIC_NAME_39=SYSTEM MPISPAWN_GENERIC_VALUE_39="linux" MPISPAWN_GENERIC_NAME_40=SLURM_TACC_RUNLIMIT_MINS MPISPAWN_GENERIC_VALUE_40="180" MPISPAWN_GENERIC_NAME_41=SLURM_SUBMIT_DIR MPISPAWN_GENERIC_VALUE_41="/home1/03222/tg825207/parallelnbody" MPISPAWN_GENERIC_NAME_42=LIBPATH MPISPAWN_GENERIC_VALUE_42="/opt/apps/xsede/gsi-openssh-5.7/lib64:/usr/lib:/lib" MPISPAWN_GENERIC_NAME_43=HOSTNAME MPISPAWN_GENERIC_VALUE_43="c468-601" MPISPAWN_GENERIC_NAME_44=LMOD_DEFAULT_MODULEPATH MPISPAWN_GENERIC_VALUE_44="/opt/apps/xsede/modulefiles:/opt/apps/modulefiles:/opt/modulefiles" MPISPAWN_GENERIC_NAME_45=ARCHIVE MPISPAWN_GENERIC_VALUE_45="/home/03222/tg825207" MPISPAWN_GENERIC_NAME_46=_ MPISPAWN_GENERIC_VALUE_46="/usr/local/bin/build_env.pl" MPISPAWN_GENERIC_NAME_47=__BASHRC_SOURCED__ MPISPAWN_GENERIC_VALUE_47="1" MPISPAWN_GENERIC_NAME_48=APPS MPISPAWN_GENERIC_VALUE_48="/opt/apps" MPISPAWN_GENERIC_NAME_49=SHELL MPISPAWN_GENERIC_VALUE_49="/bin/bash" MPISPAWN_GENERIC_NAME_50=ARMCI_OPENIB_DEVICE MPISPAWN_GENERIC_VALUE_50="mlx4_0" MPISPAWN_GENERIC_NAME_51=ENVIRONMENT MPISPAWN_GENERIC_VALUE_51="BATCH" MPISPAWN_GENERIC_NAME_52=TACC_FAMILY_MPI MPISPAWN_GENERIC_VALUE_52="mvapich2" MPISPAWN_GENERIC_NAME_53=MV2_IBA_HCA MPISPAWN_GENERIC_VALUE_53="mlx4_0" MPISPAWN_GENERIC_NAME_54=TACC_PUBLIC_MACHINE MPISPAWN_GENERIC_VALUE_54="1" MPISPAWN_GENERIC_NAME_55=SLURM_TACC_NODES MPISPAWN_GENERIC_VALUE_55="1" MPISPAWN_GENERIC_NAME_56=_ModuleTable003_ MPISPAWN_GENERIC_VALUE_56="e30sWyJzaG9ydCJdPSJjbHVzdGVyIixbInN0YXR1cyJdPSJhY3RpdmUiLH0sWyJjbHVzdGVyLXBhdGhzIl09e1siRk4iXT0iL29wdC9tb2R1bGVmaWxlcy9jbHVzdGVyLXBhdGhzIixbImRlZmF1bHQiXT0wLFsiZnVsbE5hbWUiXT0iY2x1c3Rlci1wYXRocyIsWyJsb2FkT3JkZXIiXT0zLHByb3BUPXt9LFsic2hvcnQiXT0iY2x1c3Rlci1wYXRocyIsWyJzdGF0dXMiXT0iYWN0aXZlIix9LGdjYz17WyJGTiJdPSIvb3B0L2FwcHMvbW9kdWxlZmlsZXMvZ2NjLzQuNy4xLmx1YSIsWyJkZWZhdWx0Il09MCxbImZ1bGxOYW1lIl09ImdjYy80LjcuMSIsWyJsb2FkT3JkZXIiXT03LHByb3BUPXt9LFsic2hvcnQiXT0iZ2NjIixbInN0YXR1cyJdPSJhY3RpdmUiLH0sbXZhcGljaDI9e1si" MPISPAWN_GENERIC_NAME_57=QTINC MPISPAWN_GENERIC_VALUE_57="/usr/lib64/qt-3.3/include" MPISPAWN_GENERIC_NAME_58=CVS_RSH MPISPAWN_GENERIC_VALUE_58="ssh" MPISPAWN_GENERIC_NAME_59=MV2_USE_OLD_BCAST MPISPAWN_GENERIC_VALUE_59="1" MPISPAWN_GENERIC_NAME_60=SLURM_JOB_NUM_NODES MPISPAWN_GENERIC_VALUE_60="128" MPISPAWN_GENERIC_NAME_61=SLURM_TASK_PID MPISPAWN_GENERIC_VALUE_61="31479" MPISPAWN_GENERIC_NAME_62=TACC_MPI_GETMODE MPISPAWN_GENERIC_VALUE_62="mvapich2_ssh" MPISPAWN_GENERIC_NAME_63=LANG MPISPAWN_GENERIC_VALUE_63="en_US.UTF-8" MPISPAWN_GENERIC_NAME_64=USER MPISPAWN_GENERIC_VALUE_64="tg825207" MPISPAWN_GENERIC_NAME_65=MIC_I_MPI_DAPL_PROVIDER MPISPAWN_GENERIC_VALUE_65="ofa-v2-mlx4_0-1u" MPISPAWN_GENERIC_NAME_66=G_BROKEN_FILENAMES MPISPAWN_GENERIC_VALUE_66="1" MPISPAWN_GENERIC_NAME_67=SLURM_LOCALID MPISPAWN_GENERIC_VALUE_67="0" MPISPAWN_GENERIC_NAME_68=SLURM_NPROCS MPISPAWN_GENERIC_VALUE_68="2048" MPISPAWN_GENERIC_NAME_69=SLURM_TACC_NNODES_SET MPISPAWN_GENERIC_VALUE_69="0" MPISPAWN_GENERIC_NAME_70=SLURM_TACC_CORES MPISPAWN_GENERIC_VALUE_70="2048" MPISPAWN_GENERIC_NAME_71=LMOD_DIR MPISPAWN_GENERIC_VALUE_71="/opt/apps/lmod/lmod/libexec" MPISPAWN_GENERIC_NAME_72=LMOD_SETTARG_CMD MPISPAWN_GENERIC_VALUE_72=":" MPISPAWN_GENERIC_NAME_73=OMP_NUM_THREADS MPISPAWN_GENERIC_VALUE_73="1" MPISPAWN_GENERIC_NAME_74=_ModuleTable004_ MPISPAWN_GENERIC_VALUE_74="Rk4iXT0iL29wdC9hcHBzL2djYzRfNy9tb2R1bGVmaWxlcy9tdmFwaWNoMi8xLjlhMiIsWyJkZWZhdWx0Il09MSxbImZ1bGxOYW1lIl09Im12YXBpY2gyLzEuOWEyIixbImxvYWRPcmRlciJdPTgscHJvcFQ9e30sWyJzaG9ydCJdPSJtdmFwaWNoMiIsWyJzdGF0dXMiXT0iYWN0aXZlIix9LHhhbHQ9e1siRk4iXT0iL29wdC9hcHBzL21vZHVsZWZpbGVzL3hhbHQvMC40LjQubHVhIixbImRlZmF1bHQiXT0xLFsiZnVsbE5hbWUiXT0ieGFsdC8wLjQuNCIsWyJsb2FkT3JkZXIiXT00LHByb3BUPXt9LFsic2hvcnQiXT0ieGFsdCIsWyJzdGF0dXMiXT0iYWN0aXZlIix9LH0sbXBhdGhBPXsiL29wdC9hcHBzL2djYzRfNy9tdmFwaWNoMl8xXzkvbW9kdWxlZmlsZXMiLCIvb3B0L2FwcHMv" MPISPAWN_GENERIC_NAME_75=__PERSONAL_PATH__ MPISPAWN_GENERIC_VALUE_75="1" MPISPAWN_GENERIC_NAME_76=LMOD_SYSTEM_DEFAULT_MODULES MPISPAWN_GENERIC_VALUE_76="TACC" MPISPAWN_GENERIC_NAME_77=SLURM_JOB_NAME MPISPAWN_GENERIC_VALUE_77="stampedeLargeScaleAllTS.sh" MPISPAWN_GENERIC_NAME_78=SLURM_SUBMIT_HOST MPISPAWN_GENERIC_VALUE_78="login4.stampede.tacc.utexas.edu" MPISPAWN_GENERIC_NAME_79=LD_LIBRARY_PATH MPISPAWN_GENERIC_VALUE_79="/opt/apps/gcc4_7/mvapich2/1.9/lib:/opt/apps/gcc4_7/mvapich2/1.9/lib/shared:/opt/apps/gcc/4.7.1/lib64:/opt/apps/gcc/4.7.1/lib:/opt/apps/xsede/gsi-openssh-5.7/lib64:/opt/apps/xsede/gsi-openssh-5.7/lib64" MPISPAWN_GENERIC_NAME_80=SLURM_QUEUE MPISPAWN_GENERIC_VALUE_80="normal" MPISPAWN_GENERIC_NAME_81=PKG_CONFIG_PATH MPISPAWN_GENERIC_VALUE_81="/opt/apps/gcc4_7/mvapich2/1.9/lib/pkgconfig" MPISPAWN_GENERIC_NAME_82=LMOD_PREPEND_BLOCK MPISPAWN_GENERIC_VALUE_82="normal" MPISPAWN_GENERIC_NAME_83=TACC_FAMILY_COMPILER MPISPAWN_GENERIC_VALUE_83="gcc" MPISPAWN_GENERIC_NAME_84=TACC_DOMAIN MPISPAWN_GENERIC_VALUE_84="stampede" MPISPAWN_GENERIC_NAME_85=MV2_HOMOGENEOUS_CLUSTER MPISPAWN_GENERIC_VALUE_85="1" MPISPAWN_GENERIC_NAME_86=LMOD_FAMILY_COMPILER MPISPAWN_GENERIC_VALUE_86="gcc" MPISPAWN_GENERIC_NAME_87=MV2_USE_UD_HYBRID MPISPAWN_GENERIC_VALUE_87="0" MPISPAWN_GENERIC_NAME_88=MPICH_HOME MPISPAWN_GENERIC_VALUE_88="/opt/apps/gcc4_7/mvapich2/1.9" MPISPAWN_GENERIC_NAME_89=SLURM_JOB_NODELIST MPISPAWN_GENERIC_VALUE_89="c468-601,c471-503,c472-[401-402],c473-[303-304,401,804,901],c474-[203-204,301,401,403-404],c478-303,c482-404,c483-[001-004,101,504,601,804,901],c484-[101,103-104,201,602,701],c485-502,c486-102,c490-[302,904],c491-004,c492-701,c493-[001,102,201],c494-[101,402-403,602,702-703,904],c495-[001-002,201],c497-[102,202,504,601-603],c498-[304,402,603],c501-[204,301,604,801],c502-104,c503-901,c504-[602,604,702,801],c505-[001,202-204,401,502-504,802,901-904],c506-[302,704],c507-[102,202,503,601,904],c508-[101,204,601,702-703,802,903-904],c509-[002,201,301,303,401,501-502],c510-[003,401,404,501,503],c511-[601-602,902-903],c536-[801,902],c537-902,c538-[001,003,104],c540-[104,201-202,804,901-904]" MPISPAWN_GENERIC_NAME_90=SLURM_PROCID MPISPAWN_GENERIC_VALUE_90="0" MPISPAWN_GENERIC_NAME_91=I_MPI_DAPL_PROVIDER MPISPAWN_GENERIC_VALUE_91="ofa-v2-mlx4_0-1u" MPISPAWN_GENERIC_NAME_92=GLOBUS_LOCATION MPISPAWN_GENERIC_VALUE_92="/opt/apps/xsede/gsi-openssh-5.7" MPISPAWN_GENERIC_NAME_93=LMOD_COLORIZE MPISPAWN_GENERIC_VALUE_93="yes" MPISPAWN_GENERIC_NAME_94=I_MPI_HYDRA_BRANCH_COUNT MPISPAWN_GENERIC_VALUE_94="6000" MPISPAWN_GENERIC_NAME_95=SLURM_NNODES MPISPAWN_GENERIC_VALUE_95="128" MPISPAWN_GENERIC_NAME_96=MV2_DEFAULT_TIME_OUT MPISPAWN_GENERIC_VALUE_96="23" MPISPAWN_GENERIC_NAME_97=SLURM_TACC_NCORES_SET MPISPAWN_GENERIC_VALUE_97="1" MPISPAWN_GENERIC_NAME_98=__Init_Default_Modules MPISPAWN_GENERIC_VALUE_98="1" MPISPAWN_GENERIC_NAME_99=BASH_ENV MPISPAWN_GENERIC_VALUE_99="/etc/tacc/tacc_functions" MPISPAWN_GENERIC_NAME_100=SLURM_TOPOLOGY_ADDR_PATTERN MPISPAWN_GENERIC_VALUE_100="node" MPISPAWN_GENERIC_NAME_101=MV2_USE_RING_STARTUP MPISPAWN_GENERIC_VALUE_101="0" MPISPAWN_GENERIC_NAME_102=TACC_XALT_BIN MPISPAWN_GENERIC_VALUE_102="/opt/apps/xalt/0.4.4/bin" MPISPAWN_GENERIC_NAME_103=X509_USER_PROXY MPISPAWN_GENERIC_VALUE_103="/tmp/x509up_p3084.file9zM3fu.1" MPISPAWN_GENERIC_NAME_104=TMPDIR MPISPAWN_GENERIC_VALUE_104="/tmp" MPISPAWN_GENERIC_NAME_105=LMOD_FULL_SETTARG_SUPPORT MPISPAWN_GENERIC_VALUE_105="no" MPISPAWN_ID=18 MPISPAWN_WORKING_DIR=/home1/03222/tg825207/parallelnbody MPISPAWN_MPIRUN_RANK_0=288 MPISPAWN_MPIRUN_RANK_1=289 MPISPAWN_MPIRUN_RANK_2=290 MPISPAWN_MPIRUN_RANK_3=291 MPISPAWN_MPIRUN_RANK_4=292 MPISPAWN_MPIRUN_RANK_5=293 MPISPAWN_MPIRUN_RANK_6=294 MPISPAWN_MPIRUN_RANK_7=295 MPISPAWN_MPIRUN_RANK_8=296 MPISPAWN_MPIRUN_RANK_9=297 MPISPAWN_MPIRUN_RANK_10=298 MPISPAWN_MPIRUN_RANK_11=299 MPISPAWN_MPIRUN_RANK_12=300 MPISPAWN_MPIRUN_RANK_13=301 MPISPAWN_MPIRUN_RANK_14=302 MPISPAWN_MPIRUN_RANK_15=303 /opt/apps/gcc4_7/mvapich2/1.9/bin/mpispawn 0
[cli_10]: [cli_14]: [cli_8]: [cli_12]: [cli_4]: [cli_2]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffc9ff3a90) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff17612fc0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffddc7f980) failed
PMPI_Comm_rank(66).: Null communicator

[cli_6]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff51454cb0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7ffffdb59c70) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffbf397130) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffa6937fd0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_16]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff8abe3d30) failed
PMPI_Comm_rank(66).: Null communicator

[c468-601.stampede.tacc.utexas.edu:mpispawn_0][readline] Unexpected End-Of-File on file descriptor 7. MPI process died?
[c468-601.stampede.tacc.utexas.edu:mpispawn_0][mtpmi_processops] Error while reading PMI socket. MPI process died?
[cli_32]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7ffff4d00230) failed
PMPI_Comm_rank(66).: Null communicator

[c471-503.stampede.tacc.utexas.edu:mpispawn_1][readline] Unexpected End-Of-File on file descriptor 5. MPI process died?
[c471-503.stampede.tacc.utexas.edu:mpispawn_1][mtpmi_processops] Error while reading PMI socket. MPI process died?
[cli_48]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff1d239250) failed
PMPI_Comm_rank(66).: Null communicator

[c472-401.stampede.tacc.utexas.edu:mpispawn_2][readline] Unexpected End-Of-File on file descriptor 5. MPI process died?
[c472-401.stampede.tacc.utexas.edu:mpispawn_2][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c471-503.stampede.tacc.utexas.edu:mpispawn_1][child_handler] MPI process (rank: 16, pid: 26486) exited with status 1
[c472-402.stampede.tacc.utexas.edu:mpispawn_3][readline] Unexpected End-Of-File on file descriptor 18. MPI process died?
[c472-402.stampede.tacc.utexas.edu:mpispawn_3][mtpmi_processops] Error while reading PMI socket. MPI process died?
[cli_64]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffe4dd2ee0) failed
PMPI_Comm_rank(66).: Null communicator

[c472-401.stampede.tacc.utexas.edu:mpispawn_2][child_handler] MPI process (rank: 32, pid: 106077) exited with status 1
[c473-303.stampede.tacc.utexas.edu:mpispawn_4][readline] Unexpected End-Of-File on file descriptor 5. MPI process died?
[c473-303.stampede.tacc.utexas.edu:mpispawn_4][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c472-402.stampede.tacc.utexas.edu:mpispawn_3][child_handler] MPI process (rank: 48, pid: 96584) exited with status 1
[c468-601.stampede.tacc.utexas.edu:mpispawn_0][child_handler] MPI process (rank: 8, pid: 47642) exited with status 1
[c473-303.stampede.tacc.utexas.edu:mpispawn_4][child_handler] MPI process (rank: 64, pid: 9324) exited with status 1
[cli_128]: [c468-601.stampede.tacc.utexas.edu:mpispawn_0][child_handler] MPI process (rank: 2, pid: 47636) exited with status 1
aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff140c6810) failed
PMPI_Comm_rank(66).: Null communicator

[c473-901.stampede.tacc.utexas.edu:mpispawn_8][readline] Unexpected End-Of-File on file descriptor 5. MPI process died?
[c473-901.stampede.tacc.utexas.edu:mpispawn_8][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c473-901.stampede.tacc.utexas.edu:mpispawn_8][child_handler] MPI process (rank: 128, pid: 13564) exited with status 1
[cli_192]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff0bd89380) failed
PMPI_Comm_rank(66).: Null communicator

[c474-401.stampede.tacc.utexas.edu:mpispawn_12][readline] Unexpected End-Of-File on file descriptor 5. MPI process died?
[c474-401.stampede.tacc.utexas.edu:mpispawn_12][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c474-401.stampede.tacc.utexas.edu:mpispawn_12][child_handler] MPI process (rank: 192, pid: 50408) exited with status 1
[cli_256]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff5b7e1760) failed
PMPI_Comm_rank(66).: Null communicator

[c482-404.stampede.tacc.utexas.edu:mpispawn_16][readline] Unexpected End-Of-File on file descriptor 5. MPI process died?
[c482-404.stampede.tacc.utexas.edu:mpispawn_16][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c468-601.stampede.tacc.utexas.edu:mpispawn_0][child_handler] MPI process (rank: 4, pid: 47638) exited with status 1
[c482-404.stampede.tacc.utexas.edu:mpispawn_16][child_handler] MPI process (rank: 256, pid: 25405) exited with status 1
[cli_512]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff18ca6450) failed
PMPI_Comm_rank(66).: Null communicator

[c485-502.stampede.tacc.utexas.edu:mpispawn_32][readline] Unexpected End-Of-File on file descriptor 6. MPI process died?
[c485-502.stampede.tacc.utexas.edu:mpispawn_32][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c485-502.stampede.tacc.utexas.edu:mpispawn_32][child_handler] MPI process (rank: 512, pid: 7351) exited with status 1
[c468-601.stampede.tacc.utexas.edu:mpispawn_0][child_handler] MPI process (rank: 6, pid: 47640) exited with status 1
[cli_768]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffcbd30110) failed
PMPI_Comm_rank(66).: Null communicator

[c495-001.stampede.tacc.utexas.edu:mpispawn_48][readline] Unexpected End-Of-File on file descriptor 5. MPI process died?
[c495-001.stampede.tacc.utexas.edu:mpispawn_48][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c495-001.stampede.tacc.utexas.edu:mpispawn_48][child_handler] MPI process (rank: 768, pid: 63167) exited with status 1
[cli_1024]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff25f11780) failed
PMPI_Comm_rank(66).: Null communicator

[c502-104.stampede.tacc.utexas.edu:mpispawn_64][readline] Unexpected End-Of-File on file descriptor 18. MPI process died?
[c502-104.stampede.tacc.utexas.edu:mpispawn_64][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c502-104.stampede.tacc.utexas.edu:mpispawn_64][child_handler] MPI process (rank: 1024, pid: 10414) exited with status 1
[c468-601.stampede.tacc.utexas.edu:mpispawn_0][child_handler] MPI process (rank: 10, pid: 47644) exited with status 1
[c468-601.stampede.tacc.utexas.edu:mpispawn_0][child_handler] MPI process (rank: 12, pid: 47646) exited with status 1
[c468-601.stampede.tacc.utexas.edu:mpispawn_0][child_handler] MPI process (rank: 14, pid: 47648) exited with status 1
[cli_22]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff136a27f0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_24]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff63d140c0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_18]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff092e5420) failed
PMPI_Comm_rank(66).: Null communicator

[cli_36]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffcf9b3a80) failed
PMPI_Comm_rank(66).: Null communicator

[cli_38]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffd371e200) failed
PMPI_Comm_rank(66).: Null communicator

[cli_40]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff21acd420) failed
PMPI_Comm_rank(66).: Null communicator

[cli_42]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffac334670) failed
PMPI_Comm_rank(66).: Null communicator

[cli_26]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffeac91640) failed
PMPI_Comm_rank(66).: Null communicator

[cli_28]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff80d285b0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_30]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffaffbfa50) failed
PMPI_Comm_rank(66).: Null communicator

[cli_34]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff80553470) failed
PMPI_Comm_rank(66).: Null communicator

[cli_20]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff190ac9a0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_44]: [cli_68]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff22d70320) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffbb62a020) failed
PMPI_Comm_rank(66).: Null communicator

[cli_46]: [cli_52]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff1e5c0460) failed
PMPI_Comm_rank(66).: Null communicator

[cli_54]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff39e06d20) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffa26aecd0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_96]: [cli_98]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffb2206fc0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_70]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffbfba45b0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_66]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffab55fbc0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_58]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff39562c00) failed
PMPI_Comm_rank(66).: Null communicator

[cli_50]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff68fa0ae0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_102]: [cli_100]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffea503080) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffb4a25c30) failed
PMPI_Comm_rank(66).: Null communicator

[cli_108]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff92cefd50) failed
PMPI_Comm_rank(66).: Null communicator

[cli_72]: [cli_78]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff3d0827d0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_74]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7ffff85c4670) failed
PMPI_Comm_rank(66).: Null communicator

[cli_76]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff253aec90) failed
PMPI_Comm_rank(66).: Null communicator

[cli_144]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffce62a840) failed
PMPI_Comm_rank(66).: Null communicator

[cli_104]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffc7a14bf0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_110]: [cli_62]: [cli_56]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffafd029f0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_60]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffd7cacb80) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff05b5b790) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffec5d3b00) failed
PMPI_Comm_rank(66).: Null communicator

[cli_106]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff3cce1c30) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7ffff487ef40) failed
PMPI_Comm_rank(66).: Null communicator

[cli_160]: [cli_164]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff32751ce0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_162]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffccbcdd70) failed
PMPI_Comm_rank(66).: Null communicator

[cli_166]: [cli_112]: [cli_114]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff144e4960) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffbd6d31f0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_118]: [cli_116]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7ffffb678890) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff0388def0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_120]: [cli_124]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffc8a589f0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_126]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7ffff360bd20) failed
PMPI_Comm_rank(66).: Null communicator

[cli_122]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff8c1b89b0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffa8fa7d50) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffc4bb2690) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff1dc53b80) failed
PMPI_Comm_rank(66).: Null communicator

[cli_176]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff316737d0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_180]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff09204670) failed
PMPI_Comm_rank(66).: Null communicator

[cli_178]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff7f3cda40) failed
PMPI_Comm_rank(66).: Null communicator

[cli_170]: [cli_168]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffa2d52db0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_174]: [cli_182]: [cli_184]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7ffff4316740) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff4e624840) failed
PMPI_Comm_rank(66).: Null communicator

[cli_172]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff0830e490) failed
PMPI_Comm_rank(66).: Null communicator

[cli_82]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffc1d0e2e0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_84]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff89ad7250) failed
PMPI_Comm_rank(66).: Null communicator

[cli_88]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffe9aa3d90) failed
PMPI_Comm_rank(66).: Null communicator

[cli_188]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffb8314d60) failed
PMPI_Comm_rank(66).: Null communicator

[cli_186]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff19f46710) failed
PMPI_Comm_rank(66).: Null communicator

[cli_190]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff2d008240) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffc43fa910) failed
PMPI_Comm_rank(66).: Null communicator

[cli_86]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffd3fdcf90) failed
PMPI_Comm_rank(66).: Null communicator

[cli_90]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff12fcdf00) failed
PMPI_Comm_rank(66).: Null communicator

[cli_92]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7ffff40fcc70) failed
PMPI_Comm_rank(66).: Null communicator

[cli_94]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffa9680020) failed
PMPI_Comm_rank(66).: Null communicator

[cli_80]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff51c42c50) failed
PMPI_Comm_rank(66).: Null communicator

[cli_148]: [cli_146]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff592db150) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffe4b8d260) failed
PMPI_Comm_rank(66).: Null communicator

[cli_150]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff0d5512c0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffedcfeda0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff058cbca0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_208]: [cli_196]: [cli_194]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffc38113d0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff3bc65b60) failed
PMPI_Comm_rank(66).: Null communicator

[cli_200]: [cli_198]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff5e4fd1c0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_204]: [cli_202]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffc96c5fd0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffc0040a50) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff795b86b0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_206]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffa37965a0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_154]: [cli_152]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff06c05990) failed
PMPI_Comm_rank(66).: Null communicator

[cli_156]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff9b90c870) failed
PMPI_Comm_rank(66).: Null communicator

[cli_158]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7ffff21d62a0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff750235c0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_132]: [cli_130]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff63b56270) failed
PMPI_Comm_rank(66).: Null communicator

[cli_210]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff8d3e7950) failed
PMPI_Comm_rank(66).: Null communicator

[cli_212]: [cli_214]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff31bd0a60) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff7862cb50) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff981ea080) failed
PMPI_Comm_rank(66).: Null communicator

[cli_224]: [cli_228]: [cli_218]: [cli_216]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffd5106900) failed
PMPI_Comm_rank(66).: Null communicator

[cli_222]: [cli_220]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff059a0610) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff25287130) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7ffffcac4f80) failed
PMPI_Comm_rank(66).: Null communicator

[cli_226]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff0d060740) failed
PMPI_Comm_rank(66).: Null communicator

[cli_230]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff7fc82750) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffc58b31d0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_260]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff845b52b0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_258]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff4825a8b0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_244]: [cli_242]: [cli_234]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff3ef6a710) failed
PMPI_Comm_rank(66).: Null communicator

[cli_262]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffbe326dd0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_266]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff56efa200) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff4bff3eb0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_252]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff01502140) failed
PMPI_Comm_rank(66).: Null communicator

[cli_246]: [cli_254]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff3816c0b0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff25dd1740) failed
PMPI_Comm_rank(66).: Null communicator

[cli_236]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7ffffa115020) failed
PMPI_Comm_rank(66).: Null communicator

[cli_238]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff12ff7940) failed
PMPI_Comm_rank(66).: Null communicator

[cli_232]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff47acd980) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff83b6b0d0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_288]: [cli_264]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffe8140b30) failed
PMPI_Comm_rank(66).: Null communicator

[cli_250]: [cli_248]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7ffff6d5a8d0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff59c2c540) failed
PMPI_Comm_rank(66).: Null communicator

[cli_268]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff4c8991a0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_270]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff602eb700) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffac675310) failed
PMPI_Comm_rank(66).: Null communicator

[cli_290]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7ffffb4f4bd0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7ffffe3933c0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_320]: [cli_324]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffb9499890) failed
PMPI_Comm_rank(66).: Null communicator

[cli_322]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffb6973f00) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff822947c0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_134]: [cli_272]: [cli_276]: [cli_274]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff8352d690) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff3685e270) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffc96401a0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_286]: [cli_278]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffb02eae20) failed
PMPI_Comm_rank(66).: Null communicator

[cli_282]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffb653a2f0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_284]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff8bc8c3e0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_280]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff7c787210) failed
PMPI_Comm_rank(66).: Null communicator

[cli_352]: [cli_356]: [cli_354]: [cli_326]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffe06f1a90) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff85c017c0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_292]: [cli_294]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffdfbd4ab0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff6144d8c0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff1f1ddce0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_330]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff40769520) failed
PMPI_Comm_rank(66).: Null communicator

[cli_332]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff56484130) failed
PMPI_Comm_rank(66).: Null communicator

[cli_334]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffa5456d40) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffc9ce95e0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff667c7c30) failed
PMPI_Comm_rank(66).: Null communicator

[cli_358]: [cli_304]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff6b971530) failed
PMPI_Comm_rank(66).: Null communicator

[cli_308]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff67a68f60) failed
PMPI_Comm_rank(66).: Null communicator

[cli_306]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff207e2dc0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_310]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffe7867460) failed
PMPI_Comm_rank(66).: Null communicator

[cli_318]: [cli_314]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff8b3171d0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_316]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffef1c1e70) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffe166b110) failed
PMPI_Comm_rank(66).: Null communicator

[cli_312]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff8510e7f0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_302]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffe1a96ff0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_298]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffa997a9c0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_300]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff0dfa18b0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_296]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffd5f24280) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff1e71c210) failed
PMPI_Comm_rank(66).: Null communicator

[cli_328]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff16f19920) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff6a23ed40) failed
PMPI_Comm_rank(66).: Null communicator

[cli_362]: [cli_364]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffd2f442a0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffd528ea10) failed
PMPI_Comm_rank(66).: Null communicator

[cli_366]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff88418450) failed
PMPI_Comm_rank(66).: Null communicator

[cli_360]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffa6f199f0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_400]: [cli_384]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffe5615100) failed
PMPI_Comm_rank(66).: Null communicator

[cli_388]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff308272d0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_386]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffa5aeba30) failed
PMPI_Comm_rank(66).: Null communicator

[cli_390]: [cli_392]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7ffff6de80e0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_394]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffcae7f880) failed
PMPI_Comm_rank(66).: Null communicator

[cli_336]: [cli_340]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffd0e12df0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_338]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffa47200d0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_342]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff855ce610) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff43a432d0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_344]: [cli_348]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff717daba0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_346]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffb36c3480) failed
PMPI_Comm_rank(66).: Null communicator

[cli_350]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff95fc70a0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffb540ca90) failed
PMPI_Comm_rank(66).: Null communicator

[cli_396]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffef2a1bf0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_398]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff580366d0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_404]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff09eef400) failed
PMPI_Comm_rank(66).: Null communicator

[cli_402]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff6b148320) failed
PMPI_Comm_rank(66).: Null communicator

[cli_406]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff5da22df0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff3310f850) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7ffff773d690) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff6c5aaad0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_408]: [cli_410]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffbe2d1580) failed
PMPI_Comm_rank(66).: Null communicator

[cli_412]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff93c47bd0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_416]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff213217b0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_414]: [cli_368]: [cli_370]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff27b27070) failed
PMPI_Comm_rank(66).: Null communicator

[cli_374]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff70286e10) failed
PMPI_Comm_rank(66).: Null communicator

[cli_372]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff62e21b00) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffa45fa520) failed
PMPI_Comm_rank(66).: Null communicator

[cli_382]: [cli_378]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff4fb034a0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_380]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffd2375dd0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_376]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffaae18bd0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff983ff830) failed
PMPI_Comm_rank(66).: Null communicator

[cli_418]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffab1b9a40) failed
PMPI_Comm_rank(66).: Null communicator

[cli_420]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffc528bdc0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff1a923790) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7ffff8f979e0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_432]: [cli_434]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff3db98650) failed
PMPI_Comm_rank(66).: Null communicator

[cli_438]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffcd8fc3b0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_436]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffeb3d5360) failed
PMPI_Comm_rank(66).: Null communicator

[cli_422]: [cli_428]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffe55fdea0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffcff28bc0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_424]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff9d972750) failed
PMPI_Comm_rank(66).: Null communicator

[cli_448]: [cli_452]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff5d58f090) failed
PMPI_Comm_rank(66).: Null communicator

[cli_450]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffcdd92060) failed
PMPI_Comm_rank(66).: Null communicator

[cli_430]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff191e89a0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_426]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff411963c0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffd75ea530) failed
PMPI_Comm_rank(66).: Null communicator

[cli_480]: [cli_496]: [cli_498]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff051d1150) failed
PMPI_Comm_rank(66).: Null communicator

[cli_444]: [cli_440]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff1ee22000) failed
PMPI_Comm_rank(66).: Null communicator

[cli_442]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff51ec21b0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffa58c1640) failed
PMPI_Comm_rank(66).: Null communicator

[cli_446]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffeee8cb50) failed
PMPI_Comm_rank(66).: Null communicator

[cli_142]: [cli_138]: [cli_140]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff6b8162a0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffa4591fd0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffdb41b010) failed
PMPI_Comm_rank(66).: Null communicator

[cli_482]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff8ccef310) failed
PMPI_Comm_rank(66).: Null communicator

[cli_486]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff4e721820) failed
PMPI_Comm_rank(66).: Null communicator

[cli_484]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff693cbd90) failed
PMPI_Comm_rank(66).: Null communicator

[cli_514]: [cli_518]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7ffff9a1cda0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_528]: [cli_530]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff764dd900) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffd6838790) failed
PMPI_Comm_rank(66).: Null communicator

[cli_534]: [cli_492]: [cli_494]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff3f553980) failed
PMPI_Comm_rank(66).: Null communicator

[cli_560]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff5cbf80c0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_522]: [cli_516]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff078221d0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff1e5f25c0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_520]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffb546b710) failed
PMPI_Comm_rank(66).: Null communicator

[cli_524]: [cli_526]: [cli_490]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffa9ca6820) failed
PMPI_Comm_rank(66).: Null communicator

[cli_488]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff0c18f2a0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff7ed5d420) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff529cd920) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff248c67c0) failed
PMPI_Comm_rank(66).: Null communicator

[c473-304.stampede.tacc.utexas.edu:mpispawn_5][readline] Unexpected End-Of-File on file descriptor 8. MPI process died?
[c473-304.stampede.tacc.utexas.edu:mpispawn_5][mtpmi_processops] Error while reading PMI socket. MPI process died?
aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffbce44270) failed
PMPI_Comm_rank(66).: Null communicator

[cli_532]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffa9e420e0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_540]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffe8717880) failed
PMPI_Comm_rank(66).: Null communicator

[cli_564]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffaa1cda80) failed
PMPI_Comm_rank(66).: Null communicator

[cli_562]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff53f80c50) failed
PMPI_Comm_rank(66).: Null communicator

[cli_566]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff029a9610) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff4a014a00) failed
PMPI_Comm_rank(66).: Null communicator

[c474-203.stampede.tacc.utexas.edu:mpispawn_9][readline] Unexpected End-Of-File on file descriptor 11. MPI process died?
[c474-203.stampede.tacc.utexas.edu:mpispawn_9][mtpmi_processops] Error while reading PMI socket. MPI process died?
[cli_640]: [c473-401.stampede.tacc.utexas.edu:mpispawn_6][readline] Unexpected End-Of-File on file descriptor 10. MPI process died?
[c473-401.stampede.tacc.utexas.edu:mpispawn_6][mtpmi_processops] Error while reading PMI socket. MPI process died?
[cli_544]: [cli_546]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff6fdd2750) failed
PMPI_Comm_rank(66).: Null communicator

[cli_550]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff82132a80) failed
PMPI_Comm_rank(66).: Null communicator

[cli_548]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff40e9cca0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff65ad3c20) failed
PMPI_Comm_rank(66).: Null communicator

[cli_558]: [cli_552]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffc493fb90) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffc83a3e40) failed
PMPI_Comm_rank(66).: Null communicator

[cli_554]: [cli_556]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7ffffc0d79b0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_136]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff72630eb0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_644]: [cli_642]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff38105c70) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff5f8b6230) failed
PMPI_Comm_rank(66).: Null communicator

[cli_646]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff8d06a4e0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_624]: [cli_626]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff30041cd0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_628]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffb380f000) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff623fbd50) failed
PMPI_Comm_rank(66).: Null communicator

[cli_630]: [cli_632]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff528c1e60) failed
PMPI_Comm_rank(66).: Null communicator

[cli_634]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff13f72010) failed
PMPI_Comm_rank(66).: Null communicator

[cli_576]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffa5acecb0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_580]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffc5f47860) failed
PMPI_Comm_rank(66).: Null communicator

[cli_578]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff80644130) failed
PMPI_Comm_rank(66).: Null communicator

[cli_582]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff10912170) failed
PMPI_Comm_rank(66).: Null communicator

[cli_588]: [cli_590]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffd9d56ad0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_584]: [cli_586]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffbf3ce310) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffdd9b3090) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff2f6b26f0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_688]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff3a431b50) failed
PMPI_Comm_rank(66).: Null communicator

[cli_690]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff6d034720) failed
PMPI_Comm_rank(66).: Null communicator

[cli_538]: [cli_542]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffd0c8f2f0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_536]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7ffff32351e0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7ffff6256da0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_672]: [cli_676]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff80fc3430) failed
PMPI_Comm_rank(66).: Null communicator

[cli_674]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffe2f34ab0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffe77451d0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_678]: [cli_684]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff47779480) failed
PMPI_Comm_rank(66).: Null communicator

[cli_682]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffce1b7050) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7ffff80c89c0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_692]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff19666c20) failed
PMPI_Comm_rank(66).: Null communicator

[c473-804.stampede.tacc.utexas.edu:mpispawn_7][readline] Unexpected End-Of-File on file descriptor 6. MPI process died?
[c473-804.stampede.tacc.utexas.edu:mpispawn_7][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c474-301.stampede.tacc.utexas.edu:mpispawn_11][readline] Unexpected End-Of-File on file descriptor 13. MPI process died?
[c474-301.stampede.tacc.utexas.edu:mpispawn_11][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c474-403.stampede.tacc.utexas.edu:mpispawn_13][readline] Unexpected End-Of-File on file descriptor 10. MPI process died?
[c474-403.stampede.tacc.utexas.edu:mpispawn_13][mtpmi_processops] Error while reading PMI socket. MPI process died?
[cli_686]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff681af610) failed
PMPI_Comm_rank(66).: Null communicator

[cli_680]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff867542e0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff096cbf70) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff69863720) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff9fbf6610) failed
PMPI_Comm_rank(66).: Null communicator

[cli_454]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff7cbe34b0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_464]: [cli_466]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffecd5d220) failed
PMPI_Comm_rank(66).: Null communicator

[cli_470]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffd32f9780) failed
PMPI_Comm_rank(66).: Null communicator

[cli_468]: [cli_456]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff23b0b070) failed
PMPI_Comm_rank(66).: Null communicator

[cli_458]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff9d0e66d0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_460]: [cli_462]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff3ed5dc20) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff7c0aed90) failed
PMPI_Comm_rank(66).: Null communicator

[cli_500]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffddf2d550) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff87aa6620) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff21df6b10) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff22487f30) failed
PMPI_Comm_rank(66).: Null communicator

[cli_476]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff406c7e80) failed
PMPI_Comm_rank(66).: Null communicator

[cli_474]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff30ab14f0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_478]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff16decfb0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_472]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffd8a449e0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_592]: [cli_502]: [cli_508]: [cli_506]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff05328630) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffcd2737e0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff88beb330) failed
PMPI_Comm_rank(66).: Null communicator

[cli_504]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff6495fa90) failed
PMPI_Comm_rank(66).: Null communicator

[cli_510]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff9777e5d0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff976f1ca0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_572]: [cli_570]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7ffff7472dd0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_574]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff42241220) failed
PMPI_Comm_rank(66).: Null communicator

[cli_568]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff33277600) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff04513480) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff3a053dd0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_594]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff0edfc0f0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_598]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7ffff10f3f20) failed
PMPI_Comm_rank(66).: Null communicator

[cli_596]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff6f2f5360) failed
PMPI_Comm_rank(66).: Null communicator

[cli_606]: [cli_602]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffabf72de0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_604]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffbb85cfb0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_600]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff42af64b0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7ffff8a685a0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_608]: [cli_610]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff71d150d0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_612]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff0fc2e500) failed
PMPI_Comm_rank(66).: Null communicator

[cli_614]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff6b4b0620) failed
PMPI_Comm_rank(66).: Null communicator

[c474-404.stampede.tacc.utexas.edu:mpispawn_14][readline] Unexpected End-Of-File on file descriptor 15. MPI process died?
[c474-404.stampede.tacc.utexas.edu:mpispawn_14][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c478-303.stampede.tacc.utexas.edu:mpispawn_15][readline] Unexpected End-Of-File on file descriptor 14. MPI process died?
[c478-303.stampede.tacc.utexas.edu:mpispawn_15][mtpmi_processops] Error while reading PMI socket. MPI process died?
[cli_616]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffad3cc590) failed
PMPI_Comm_rank(66).: Null communicator

[cli_620]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffde04ebd0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_622]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff66f84dc0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff24164e50) failed
PMPI_Comm_rank(66).: Null communicator

[cli_618]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff00a9f0d0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_656]: [cli_658]: [c474-204.stampede.tacc.utexas.edu:mpispawn_10][readline] Unexpected End-Of-File on file descriptor 13. MPI process died?
[c474-204.stampede.tacc.utexas.edu:mpispawn_10][mtpmi_processops] Error while reading PMI socket. MPI process died?
[cli_652]: [cli_650]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffa089fa80) failed
PMPI_Comm_rank(66).: Null communicator

[cli_654]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffd9f15a30) failed
PMPI_Comm_rank(66).: Null communicator

[cli_638]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7ffff7abc7e0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_636]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffb039a7f0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff4241c360) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff4eeb6b30) failed
PMPI_Comm_rank(66).: Null communicator

[cli_662]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff56043480) failed
PMPI_Comm_rank(66).: Null communicator

[cli_660]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff9a972180) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff80003600) failed
PMPI_Comm_rank(66).: Null communicator

[cli_670]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff77c02630) failed
PMPI_Comm_rank(66).: Null communicator

[cli_666]: [cli_668]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff8139a500) failed
PMPI_Comm_rank(66).: Null communicator

[cli_664]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7ffff6ab4500) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff8c380a00) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffc0a9fa20) failed
PMPI_Comm_rank(66).: Null communicator

[cli_648]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff4e5b8d40) failed
PMPI_Comm_rank(66).: Null communicator

[c483-504.stampede.tacc.utexas.edu:mpispawn_22][readline] Unexpected End-Of-File on file descriptor 9. MPI process died?
[c483-504.stampede.tacc.utexas.edu:mpispawn_22][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c483-003.stampede.tacc.utexas.edu:mpispawn_19][readline] Unexpected End-Of-File on file descriptor 15. MPI process died?
[c483-003.stampede.tacc.utexas.edu:mpispawn_19][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c483-001.stampede.tacc.utexas.edu:mpispawn_17][readline] Unexpected End-Of-File on file descriptor 8. MPI process died?
[c483-001.stampede.tacc.utexas.edu:mpispawn_17][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c483-002.stampede.tacc.utexas.edu:mpispawn_18][readline] Unexpected End-Of-File on file descriptor 7. MPI process died?
[c483-002.stampede.tacc.utexas.edu:mpispawn_18][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c483-004.stampede.tacc.utexas.edu:mpispawn_20][readline] Unexpected End-Of-File on file descriptor 10. MPI process died?
[c483-004.stampede.tacc.utexas.edu:mpispawn_20][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c483-601.stampede.tacc.utexas.edu:mpispawn_23][readline] Unexpected End-Of-File on file descriptor 18. MPI process died?
[c483-601.stampede.tacc.utexas.edu:mpispawn_23][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c483-804.stampede.tacc.utexas.edu:mpispawn_24][readline] Unexpected End-Of-File on file descriptor 6. MPI process died?
[c483-804.stampede.tacc.utexas.edu:mpispawn_24][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c483-101.stampede.tacc.utexas.edu:mpispawn_21][readline] Unexpected End-Of-File on file descriptor 8. MPI process died?
[c483-101.stampede.tacc.utexas.edu:mpispawn_21][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c483-901.stampede.tacc.utexas.edu:mpispawn_25][readline] Unexpected End-Of-File on file descriptor 5. MPI process died?
[c483-901.stampede.tacc.utexas.edu:mpispawn_25][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c484-201.stampede.tacc.utexas.edu:mpispawn_29][readline] Unexpected End-Of-File on file descriptor 8. MPI process died?
[c484-201.stampede.tacc.utexas.edu:mpispawn_29][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c484-103.stampede.tacc.utexas.edu:mpispawn_27][readline] Unexpected End-Of-File on file descriptor 12. MPI process died?
[c484-103.stampede.tacc.utexas.edu:mpispawn_27][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c484-602.stampede.tacc.utexas.edu:mpispawn_30][readline] Unexpected End-Of-File on file descriptor 9. MPI process died?
[c484-602.stampede.tacc.utexas.edu:mpispawn_30][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c484-701.stampede.tacc.utexas.edu:mpispawn_31][readline] Unexpected End-Of-File on file descriptor 17. MPI process died?
[c484-701.stampede.tacc.utexas.edu:mpispawn_31][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c484-101.stampede.tacc.utexas.edu:mpispawn_26][readline] Unexpected End-Of-File on file descriptor 6. MPI process died?
[c484-101.stampede.tacc.utexas.edu:mpispawn_26][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c486-102.stampede.tacc.utexas.edu:mpispawn_33][readline] Unexpected End-Of-File on file descriptor 7. MPI process died?
[c486-102.stampede.tacc.utexas.edu:mpispawn_33][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c484-104.stampede.tacc.utexas.edu:mpispawn_28][readline] Unexpected End-Of-File on file descriptor 7. MPI process died?
[c484-104.stampede.tacc.utexas.edu:mpispawn_28][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c494-403.stampede.tacc.utexas.edu:mpispawn_43][readline] Unexpected End-Of-File on file descriptor 6. MPI process died?
[c494-403.stampede.tacc.utexas.edu:mpispawn_43][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c490-302.stampede.tacc.utexas.edu:mpispawn_34][readline] Unexpected End-Of-File on file descriptor 9. MPI process died?
[c490-302.stampede.tacc.utexas.edu:mpispawn_34][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c491-004.stampede.tacc.utexas.edu:mpispawn_36][readline] Unexpected End-Of-File on file descriptor 13. MPI process died?
[c491-004.stampede.tacc.utexas.edu:mpispawn_36][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c493-201.stampede.tacc.utexas.edu:mpispawn_40][readline] Unexpected End-Of-File on file descriptor 8. MPI process died?
[c493-201.stampede.tacc.utexas.edu:mpispawn_40][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c490-904.stampede.tacc.utexas.edu:mpispawn_35][readline] Unexpected End-Of-File on file descriptor 8. MPI process died?
[c490-904.stampede.tacc.utexas.edu:mpispawn_35][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c493-001.stampede.tacc.utexas.edu:mpispawn_38][readline] Unexpected End-Of-File on file descriptor 12. MPI process died?
[c493-001.stampede.tacc.utexas.edu:mpispawn_38][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c494-101.stampede.tacc.utexas.edu:mpispawn_41][readline] Unexpected End-Of-File on file descriptor 10. MPI process died?
[c494-101.stampede.tacc.utexas.edu:mpispawn_41][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c493-102.stampede.tacc.utexas.edu:mpispawn_39][readline] Unexpected End-Of-File on file descriptor 17. MPI process died?
[c493-102.stampede.tacc.utexas.edu:mpispawn_39][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c492-701.stampede.tacc.utexas.edu:mpispawn_37][readline] Unexpected End-Of-File on file descriptor 11. MPI process died?
[c492-701.stampede.tacc.utexas.edu:mpispawn_37][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c494-402.stampede.tacc.utexas.edu:mpispawn_42][readline] Unexpected End-Of-File on file descriptor 8. MPI process died?
[c494-402.stampede.tacc.utexas.edu:mpispawn_42][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c471-503.stampede.tacc.utexas.edu:mpispawn_1][child_handler] MPI process (rank: 22, pid: 26492) exited with status 1
[c472-401.stampede.tacc.utexas.edu:mpispawn_2][child_handler] MPI process (rank: 36, pid: 106081) exited with status 1
[c494-403.stampede.tacc.utexas.edu:mpispawn_43][child_handler] MPI process (rank: 692, pid: 59042) exited with status 1
[c473-901.stampede.tacc.utexas.edu:mpispawn_8][child_handler] MPI process (rank: 134, pid: 13570) exited with status 1
[c473-804.stampede.tacc.utexas.edu:mpispawn_7][child_handler] MPI process (rank: 114, pid: 28964) exited with status 1
[c473-304.stampede.tacc.utexas.edu:mpispawn_5][child_handler] MPI process (rank: 94, pid: 9095) exited with status 1
[c474-203.stampede.tacc.utexas.edu:mpispawn_9][child_handler] MPI process (rank: 150, pid: 41687) exited with status 1
[c473-401.stampede.tacc.utexas.edu:mpispawn_6][child_handler] MPI process (rank: 102, pid: 36908) exited with status 1
[c473-303.stampede.tacc.utexas.edu:mpispawn_4][child_handler] MPI process (rank: 68, pid: 9328) exited with status 1
[c474-204.stampede.tacc.utexas.edu:mpispawn_10][child_handler] MPI process (rank: 170, pid: 10061) exited with status 1
[c474-403.stampede.tacc.utexas.edu:mpispawn_13][child_handler] MPI process (rank: 210, pid: 101932) exited with status 1
[c474-301.stampede.tacc.utexas.edu:mpispawn_11][child_handler] MPI process (rank: 188, pid: 13279) exited with status 1
[c484-602.stampede.tacc.utexas.edu:mpispawn_30][child_handler] MPI process (rank: 486, pid: 100644) exited with status 1
[cli_240]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff9e9aaa40) failed
PMPI_Comm_rank(66).: Null communicator

[c482-404.stampede.tacc.utexas.edu:mpispawn_16][child_handler] MPI process (rank: 266, pid: 25415) exited with status 1
[c478-303.stampede.tacc.utexas.edu:mpispawn_15][child_handler] MPI process (rank: 254, pid: 27423) exited with status 1
[c483-003.stampede.tacc.utexas.edu:mpispawn_19][child_handler] MPI process (rank: 314, pid: 99783) exited with status 1
[c484-101.stampede.tacc.utexas.edu:mpispawn_26][child_handler] MPI process (rank: 420, pid: 25375) exited with status 1
[c474-404.stampede.tacc.utexas.edu:mpispawn_14][child_handler] MPI process (rank: 228, pid: 20161) exited with status 1
[c493-201.stampede.tacc.utexas.edu:mpispawn_40][child_handler] MPI process (rank: 646, pid: 59258) exited with status 1
[c492-701.stampede.tacc.utexas.edu:mpispawn_37][child_handler] MPI process (rank: 598, pid: 119296) exited with status 1
[c484-201.stampede.tacc.utexas.edu:mpispawn_29][child_handler] MPI process (rank: 474, pid: 91137) exited with status 1
[c483-101.stampede.tacc.utexas.edu:mpispawn_21][child_handler] MPI process (rank: 350, pid: 80779) exited with status 1
[c483-804.stampede.tacc.utexas.edu:mpispawn_24][child_handler] MPI process (rank: 388, pid: 33668) exited with status 1
[c474-401.stampede.tacc.utexas.edu:mpispawn_12][child_handler] MPI process (rank: 194, pid: 50410) exited with status 1
[c484-701.stampede.tacc.utexas.edu:mpispawn_31][child_handler] MPI process (rank: 506, pid: 42890) exited with status 1
[c484-104.stampede.tacc.utexas.edu:mpispawn_28][child_handler] MPI process (rank: 452, pid: 88628) exited with status 1
[c486-102.stampede.tacc.utexas.edu:mpispawn_33][child_handler] MPI process (rank: 530, pid: 2809) exited with status 1
[c483-004.stampede.tacc.utexas.edu:mpispawn_20][child_handler] MPI process (rank: 330, pid: 10496) exited with status 1
[c494-101.stampede.tacc.utexas.edu:mpispawn_41][child_handler] MPI process (rank: 662, pid: 16462) exited with status 1
[c494-402.stampede.tacc.utexas.edu:mpispawn_42][child_handler] MPI process (rank: 676, pid: 78017) exited with status 1
[c493-102.stampede.tacc.utexas.edu:mpispawn_39][child_handler] MPI process (rank: 636, pid: 74000) exited with status 1
[c483-504.stampede.tacc.utexas.edu:mpispawn_22][child_handler] MPI process (rank: 364, pid: 12853) exited with status 1
[c490-302.stampede.tacc.utexas.edu:mpispawn_34][child_handler] MPI process (rank: 548, pid: 57444) exited with status 1
[c471-503.stampede.tacc.utexas.edu:mpispawn_1][child_handler] MPI process (rank: 18, pid: 26488) exited with status 1
[c483-001.stampede.tacc.utexas.edu:mpispawn_17][child_handler] MPI process (rank: 276, pid: 19651) exited with status 1
[c483-601.stampede.tacc.utexas.edu:mpispawn_23][child_handler] MPI process (rank: 372, pid: 15830) exited with status 1
[c483-002.stampede.tacc.utexas.edu:mpispawn_18][child_handler] MPI process (rank: 290, pid: 104553) exited with status 1
[c491-004.stampede.tacc.utexas.edu:mpispawn_36][child_handler] MPI process (rank: 590, pid: 117239) exited with status 1
[c472-401.stampede.tacc.utexas.edu:mpispawn_2][child_handler] MPI process (rank: 34, pid: 106079) exited with status 1
[c483-901.stampede.tacc.utexas.edu:mpispawn_25][child_handler] MPI process (rank: 414, pid: 15085) exited with status 1
[c484-103.stampede.tacc.utexas.edu:mpispawn_27][child_handler] MPI process (rank: 446, pid: 36239) exited with status 1
[c493-001.stampede.tacc.utexas.edu:mpispawn_38][child_handler] MPI process (rank: 620, pid: 24267) exited with status 1
[c494-403.stampede.tacc.utexas.edu:mpispawn_43][child_handler] MPI process (rank: 688, pid: 59038) exited with status 1
[c490-904.stampede.tacc.utexas.edu:mpispawn_35][child_handler] MPI process (rank: 574, pid: 36114) exited with status 1
[c473-804.stampede.tacc.utexas.edu:mpispawn_7][child_handler] MPI process (rank: 118, pid: 28968) exited with status 1
[c473-901.stampede.tacc.utexas.edu:mpispawn_8][child_handler] MPI process (rank: 130, pid: 13566) exited with status 1
[c482-404.stampede.tacc.utexas.edu:mpispawn_16][child_handler] MPI process (rank: 258, pid: 25407) exited with status 1
[c473-304.stampede.tacc.utexas.edu:mpispawn_5][child_handler] MPI process (rank: 80, pid: 9081) exited with status 1
[c472-402.stampede.tacc.utexas.edu:mpispawn_3][child_handler] MPI process (rank: 62, pid: 96598) exited with status 1
[c473-401.stampede.tacc.utexas.edu:mpispawn_6][child_handler] MPI process (rank: 96, pid: 36902) exited with status 1
[c472-402.stampede.tacc.utexas.edu:mpispawn_3][child_handler] MPI process (rank: 50, pid: 96586) exited with status 1
[c474-203.stampede.tacc.utexas.edu:mpispawn_9][child_handler] MPI process (rank: 144, pid: 41681) exited with status 1
[c473-303.stampede.tacc.utexas.edu:mpispawn_4][child_handler] MPI process (rank: 66, pid: 9326) exited with status 1
[c474-301.stampede.tacc.utexas.edu:mpispawn_11][child_handler] MPI process (rank: 178, pid: 13269) exited with status 1
[c485-502.stampede.tacc.utexas.edu:mpispawn_32][child_handler] MPI process (rank: 524, pid: 7363) exited with status 1
[c474-403.stampede.tacc.utexas.edu:mpispawn_13][child_handler] MPI process (rank: 208, pid: 101930) exited with status 1
[c474-204.stampede.tacc.utexas.edu:mpispawn_10][child_handler] MPI process (rank: 160, pid: 10051) exited with status 1
[c485-502.stampede.tacc.utexas.edu:mpispawn_32][child_handler] MPI process (rank: 514, pid: 7353) exited with status 1
[c484-602.stampede.tacc.utexas.edu:mpispawn_30][child_handler] MPI process (rank: 482, pid: 100640) exited with status 1
[c478-303.stampede.tacc.utexas.edu:mpispawn_15][child_handler] MPI process (rank: 240, pid: 27409) exited with status 1
[c484-101.stampede.tacc.utexas.edu:mpispawn_26][child_handler] MPI process (rank: 418, pid: 25373) exited with status 1
[c483-003.stampede.tacc.utexas.edu:mpispawn_19][child_handler] MPI process (rank: 306, pid: 99775) exited with status 1
[c474-404.stampede.tacc.utexas.edu:mpispawn_14][child_handler] MPI process (rank: 224, pid: 20157) exited with status 1
[c493-201.stampede.tacc.utexas.edu:mpispawn_40][child_handler] MPI process (rank: 642, pid: 59254) exited with status 1
[c474-401.stampede.tacc.utexas.edu:mpispawn_12][child_handler] MPI process (rank: 196, pid: 50412) exited with status 1
[c484-201.stampede.tacc.utexas.edu:mpispawn_29][child_handler] MPI process (rank: 464, pid: 91127) exited with status 1
[c483-804.stampede.tacc.utexas.edu:mpispawn_24][child_handler] MPI process (rank: 384, pid: 33664) exited with status 1
[c492-701.stampede.tacc.utexas.edu:mpispawn_37][child_handler] MPI process (rank: 594, pid: 119292) exited with status 1
[c483-101.stampede.tacc.utexas.edu:mpispawn_21][child_handler] MPI process (rank: 336, pid: 80765) exited with status 1
[c484-104.stampede.tacc.utexas.edu:mpispawn_28][child_handler] MPI process (rank: 450, pid: 88626) exited with status 1
[c483-002.stampede.tacc.utexas.edu:mpispawn_18][child_handler] MPI process (rank: 292, pid: 104555) exited with status 1
[c486-102.stampede.tacc.utexas.edu:mpispawn_33][child_handler] MPI process (rank: 528, pid: 2807) exited with status 1
[c494-101.stampede.tacc.utexas.edu:mpispawn_41][child_handler] MPI process (rank: 656, pid: 16456) exited with status 1
[c483-004.stampede.tacc.utexas.edu:mpispawn_20][child_handler] MPI process (rank: 320, pid: 10486) exited with status 1
[c494-402.stampede.tacc.utexas.edu:mpispawn_42][child_handler] MPI process (rank: 672, pid: 78013) exited with status 1
[c493-102.stampede.tacc.utexas.edu:mpispawn_39][child_handler] MPI process (rank: 624, pid: 73988) exited with status 1
[c471-503.stampede.tacc.utexas.edu:mpispawn_1][child_handler] MPI process (rank: 20, pid: 26490) exited with status 1
[c483-504.stampede.tacc.utexas.edu:mpispawn_22][child_handler] MPI process (rank: 352, pid: 12841) exited with status 1
[c490-302.stampede.tacc.utexas.edu:mpispawn_34][child_handler] MPI process (rank: 544, pid: 57440) exited with status 1
[c491-004.stampede.tacc.utexas.edu:mpispawn_36][child_handler] MPI process (rank: 576, pid: 117225) exited with status 1
[c483-001.stampede.tacc.utexas.edu:mpispawn_17][child_handler] MPI process (rank: 274, pid: 19649) exited with status 1
[c483-601.stampede.tacc.utexas.edu:mpispawn_23][child_handler] MPI process (rank: 368, pid: 15826) exited with status 1
[c472-401.stampede.tacc.utexas.edu:mpispawn_2][child_handler] MPI process (rank: 38, pid: 106083) exited with status 1
[c484-701.stampede.tacc.utexas.edu:mpispawn_31][child_handler] MPI process (rank: 498, pid: 42882) exited with status 1
[c483-901.stampede.tacc.utexas.edu:mpispawn_25][child_handler] MPI process (rank: 402, pid: 15073) exited with status 1
[c484-103.stampede.tacc.utexas.edu:mpispawn_27][child_handler] MPI process (rank: 432, pid: 36225) exited with status 1
[c494-403.stampede.tacc.utexas.edu:mpispawn_43][child_handler] MPI process (rank: 690, pid: 59040) exited with status 1
[c493-001.stampede.tacc.utexas.edu:mpispawn_38][child_handler] MPI process (rank: 608, pid: 24255) exited with status 1
[c490-904.stampede.tacc.utexas.edu:mpispawn_35][child_handler] MPI process (rank: 560, pid: 36100) exited with status 1
[c473-804.stampede.tacc.utexas.edu:mpispawn_7][child_handler] MPI process (rank: 112, pid: 28962) exited with status 1
[c482-404.stampede.tacc.utexas.edu:mpispawn_16][child_handler] MPI process (rank: 260, pid: 25409) exited with status 1
[c473-304.stampede.tacc.utexas.edu:mpispawn_5][child_handler] MPI process (rank: 82, pid: 9083) exited with status 1
[c473-303.stampede.tacc.utexas.edu:mpispawn_4][child_handler] MPI process (rank: 70, pid: 9330) exited with status 1
[c472-402.stampede.tacc.utexas.edu:mpispawn_3][child_handler] MPI process (rank: 52, pid: 96588) exited with status 1
[c473-401.stampede.tacc.utexas.edu:mpispawn_6][child_handler] MPI process (rank: 98, pid: 36904) exited with status 1
[c473-901.stampede.tacc.utexas.edu:mpispawn_8][child_handler] MPI process (rank: 132, pid: 13568) exited with status 1
[c474-203.stampede.tacc.utexas.edu:mpispawn_9][child_handler] MPI process (rank: 146, pid: 41683) exited with status 1
[c474-301.stampede.tacc.utexas.edu:mpispawn_11][child_handler] MPI process (rank: 176, pid: 13267) exited with status 1
[c485-502.stampede.tacc.utexas.edu:mpispawn_32][child_handler] MPI process (rank: 516, pid: 7355) exited with status 1
[c474-204.stampede.tacc.utexas.edu:mpispawn_10][child_handler] MPI process (rank: 162, pid: 10053) exited with status 1
[c478-303.stampede.tacc.utexas.edu:mpispawn_15][child_handler] MPI process (rank: 242, pid: 27411) exited with status 1
[c484-101.stampede.tacc.utexas.edu:mpispawn_26][child_handler] MPI process (rank: 416, pid: 25371) exited with status 1
[c483-003.stampede.tacc.utexas.edu:mpispawn_19][child_handler] MPI process (rank: 304, pid: 99773) exited with status 1
[c484-201.stampede.tacc.utexas.edu:mpispawn_29][child_handler] MPI process (rank: 466, pid: 91129) exited with status 1
[c483-804.stampede.tacc.utexas.edu:mpispawn_24][child_handler] MPI process (rank: 386, pid: 33666) exited with status 1
[c486-102.stampede.tacc.utexas.edu:mpispawn_33][child_handler] MPI process (rank: 532, pid: 2811) exited with status 1
[c483-004.stampede.tacc.utexas.edu:mpispawn_20][child_handler] MPI process (rank: 322, pid: 10488) exited with status 1
[c494-101.stampede.tacc.utexas.edu:mpispawn_41][child_handler] MPI process (rank: 658, pid: 16458) exited with status 1
[c483-504.stampede.tacc.utexas.edu:mpispawn_22][child_handler] MPI process (rank: 354, pid: 12843) exited with status 1
[c472-401.stampede.tacc.utexas.edu:mpispawn_2][child_handler] MPI process (rank: 40, pid: 106085) exited with status 1
[c483-001.stampede.tacc.utexas.edu:mpispawn_17][child_handler] MPI process (rank: 272, pid: 19647) exited with status 1
[c484-103.stampede.tacc.utexas.edu:mpispawn_27][child_handler] MPI process (rank: 434, pid: 36227) exited with status 1
[c483-101.stampede.tacc.utexas.edu:mpispawn_21][child_handler] MPI process (rank: 338, pid: 80767) exited with status 1
[c474-404.stampede.tacc.utexas.edu:mpispawn_14][child_handler] MPI process (rank: 226, pid: 20159) exited with status 1
[c484-701.stampede.tacc.utexas.edu:mpispawn_31][child_handler] MPI process (rank: 496, pid: 42880) exited with status 1
[c490-302.stampede.tacc.utexas.edu:mpispawn_34][child_handler] MPI process (rank: 546, pid: 57442) exited with status 1
[c474-401.stampede.tacc.utexas.edu:mpispawn_12][child_handler] MPI process (rank: 198, pid: 50414) exited with status 1
[c471-503.stampede.tacc.utexas.edu:mpispawn_1][child_handler] MPI process (rank: 24, pid: 26494) exited with status 1
[c483-002.stampede.tacc.utexas.edu:mpispawn_18][child_handler] MPI process (rank: 288, pid: 104551) exited with status 1
[c493-102.stampede.tacc.utexas.edu:mpispawn_39][child_handler] MPI process (rank: 626, pid: 73990) exited with status 1
[c483-901.stampede.tacc.utexas.edu:mpispawn_25][child_handler] MPI process (rank: 404, pid: 15075) exited with status 1
[c493-001.stampede.tacc.utexas.edu:mpispawn_38][child_handler] MPI process (rank: 610, pid: 24257) exited with status 1
[c492-701.stampede.tacc.utexas.edu:mpispawn_37][child_handler] MPI process (rank: 592, pid: 119290) exited with status 1
[c474-403.stampede.tacc.utexas.edu:mpispawn_13][child_handler] MPI process (rank: 212, pid: 101934) exited with status 1
[c494-402.stampede.tacc.utexas.edu:mpispawn_42][child_handler] MPI process (rank: 674, pid: 78015) exited with status 1
[c484-602.stampede.tacc.utexas.edu:mpispawn_30][child_handler] MPI process (rank: 480, pid: 100638) exited with status 1
[c490-904.stampede.tacc.utexas.edu:mpispawn_35][child_handler] MPI process (rank: 562, pid: 36102) exited with status 1
[c473-804.stampede.tacc.utexas.edu:mpispawn_7][child_handler] MPI process (rank: 116, pid: 28966) exited with status 1
[c473-304.stampede.tacc.utexas.edu:mpispawn_5][child_handler] MPI process (rank: 84, pid: 9085) exited with status 1
[c483-601.stampede.tacc.utexas.edu:mpispawn_23][child_handler] MPI process (rank: 370, pid: 15828) exited with status 1
[c484-104.stampede.tacc.utexas.edu:mpispawn_28][child_handler] MPI process (rank: 448, pid: 88624) exited with status 1
[c472-402.stampede.tacc.utexas.edu:mpispawn_3][child_handler] MPI process (rank: 54, pid: 96590) exited with status 1
[c473-303.stampede.tacc.utexas.edu:mpispawn_4][child_handler] MPI process (rank: 72, pid: 9332) exited with status 1
[c474-203.stampede.tacc.utexas.edu:mpispawn_9][child_handler] MPI process (rank: 148, pid: 41685) exited with status 1
[c473-401.stampede.tacc.utexas.edu:mpispawn_6][child_handler] MPI process (rank: 100, pid: 36906) exited with status 1
[c474-301.stampede.tacc.utexas.edu:mpispawn_11][child_handler] MPI process (rank: 180, pid: 13271) exited with status 1
[c491-004.stampede.tacc.utexas.edu:mpispawn_36][child_handler] MPI process (rank: 578, pid: 117227) exited with status 1
[c482-404.stampede.tacc.utexas.edu:mpispawn_16][child_handler] MPI process (rank: 262, pid: 25411) exited with status 1
[c493-201.stampede.tacc.utexas.edu:mpispawn_40][child_handler] MPI process (rank: 640, pid: 59252) exited with status 1
[c473-901.stampede.tacc.utexas.edu:mpispawn_8][child_handler] MPI process (rank: 136, pid: 13572) exited with status 1
[c485-502.stampede.tacc.utexas.edu:mpispawn_32][child_handler] MPI process (rank: 518, pid: 7357) exited with status 1
[c474-403.stampede.tacc.utexas.edu:mpispawn_13][child_handler] MPI process (rank: 214, pid: 101936) exited with status 1
[c474-204.stampede.tacc.utexas.edu:mpispawn_10][child_handler] MPI process (rank: 164, pid: 10055) exited with status 1
[c484-602.stampede.tacc.utexas.edu:mpispawn_30][child_handler] MPI process (rank: 484, pid: 100642) exited with status 1
[c478-303.stampede.tacc.utexas.edu:mpispawn_15][child_handler] MPI process (rank: 244, pid: 27413) exited with status 1
[c484-101.stampede.tacc.utexas.edu:mpispawn_26][child_handler] MPI process (rank: 422, pid: 25377) exited with status 1
[c474-404.stampede.tacc.utexas.edu:mpispawn_14][child_handler] MPI process (rank: 230, pid: 20163) exited with status 1
[c483-003.stampede.tacc.utexas.edu:mpispawn_19][child_handler] MPI process (rank: 308, pid: 99777) exited with status 1
[c493-201.stampede.tacc.utexas.edu:mpispawn_40][child_handler] MPI process (rank: 644, pid: 59256) exited with status 1
[c483-804.stampede.tacc.utexas.edu:mpispawn_24][child_handler] MPI process (rank: 390, pid: 33670) exited with status 1
[c474-401.stampede.tacc.utexas.edu:mpispawn_12][child_handler] MPI process (rank: 200, pid: 50416) exited with status 1
[c484-201.stampede.tacc.utexas.edu:mpispawn_29][child_handler] MPI process (rank: 468, pid: 91131) exited with status 1
[c483-101.stampede.tacc.utexas.edu:mpispawn_21][child_handler] MPI process (rank: 340, pid: 80769) exited with status 1
[c492-701.stampede.tacc.utexas.edu:mpispawn_37][child_handler] MPI process (rank: 596, pid: 119294) exited with status 1
[c484-104.stampede.tacc.utexas.edu:mpispawn_28][child_handler] MPI process (rank: 454, pid: 88630) exited with status 1
[c486-102.stampede.tacc.utexas.edu:mpispawn_33][child_handler] MPI process (rank: 534, pid: 2813) exited with status 1
[c483-002.stampede.tacc.utexas.edu:mpispawn_18][child_handler] MPI process (rank: 294, pid: 104557) exited with status 1
[cli_698]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff6a4e10d0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_700]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff49a02560) failed
PMPI_Comm_rank(66).: Null communicator

[cli_696]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff21aebf10) failed
PMPI_Comm_rank(66).: Null communicator

[cli_694]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff2d8ba820) failed
PMPI_Comm_rank(66).: Null communicator

[cli_712]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff89fcedb0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_714]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffb9096600) failed
PMPI_Comm_rank(66).: Null communicator

[cli_702]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff90b0ff70) failed
PMPI_Comm_rank(66).: Null communicator

[cli_704]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffece0ac00) failed
PMPI_Comm_rank(66).: Null communicator

[cli_706]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff7fa47bf0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_708]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffb3401b70) failed
PMPI_Comm_rank(66).: Null communicator

[cli_710]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffce64fcd0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_716]: [cli_718]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff4f974880) failed
PMPI_Comm_rank(66).: Null communicator

[cli_724]: [cli_720]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff836c94b0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_740]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff65325ad0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_726]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff8a58f3b0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffa1cadd90) failed
PMPI_Comm_rank(66).: Null communicator

[cli_722]: [cli_732]: [cli_734]: [cli_736]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffa7651e50) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff893f2c50) failed
PMPI_Comm_rank(66).: Null communicator

[cli_738]: [cli_742]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff55857b60) failed
PMPI_Comm_rank(66).: Null communicator

[cli_746]: [cli_748]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff145c5b80) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff5acc2ba0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff98230130) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffe3ec4e20) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff0b6ecb50) failed
PMPI_Comm_rank(66).: Null communicator

[cli_728]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff44bfea10) failed
PMPI_Comm_rank(66).: Null communicator

[cli_800]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffca84f730) failed
PMPI_Comm_rank(66).: Null communicator

[cli_750]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff14c54c30) failed
PMPI_Comm_rank(66).: Null communicator

[cli_744]: [cli_864]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff8a2f5660) failed
PMPI_Comm_rank(66).: Null communicator

[cli_804]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7ffff40d63e0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_802]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff1e282c90) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffd6f6dd20) failed
PMPI_Comm_rank(66).: Null communicator

[cli_806]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff3f652f10) failed
PMPI_Comm_rank(66).: Null communicator

[cli_808]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff5ff36380) failed
PMPI_Comm_rank(66).: Null communicator

[cli_810]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff63230250) failed
PMPI_Comm_rank(66).: Null communicator

[cli_868]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff1d146420) failed
PMPI_Comm_rank(66).: Null communicator

[cli_866]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7ffff9a296f0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_870]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff14358020) failed
PMPI_Comm_rank(66).: Null communicator

[cli_874]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff8c1baec0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_814]: [cli_812]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff59637510) failed
PMPI_Comm_rank(66).: Null communicator

[cli_912]: [cli_876]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7ffffaa6d4f0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff79128040) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff9fd69180) failed
PMPI_Comm_rank(66).: Null communicator

[cli_916]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff120423e0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_914]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffff76c500) failed
PMPI_Comm_rank(66).: Null communicator

[cli_918]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff62aca3d0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffbffb2a80) failed
PMPI_Comm_rank(66).: Null communicator

[cli_872]: [cli_878]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff463c62a0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff249f3880) failed
PMPI_Comm_rank(66).: Null communicator

[cli_772]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff5fa5b2e0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_770]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffdc43f1c0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_774]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff574d2e80) failed
PMPI_Comm_rank(66).: Null communicator

[cli_782]: [cli_780]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff896ca8f0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_778]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff9af56500) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffc847fb50) failed
PMPI_Comm_rank(66).: Null communicator

[cli_776]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffe0cf9a60) failed
PMPI_Comm_rank(66).: Null communicator

[cli_922]: [cli_924]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff9bf14d90) failed
PMPI_Comm_rank(66).: Null communicator

[cli_992]: [cli_964]: [cli_962]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff49218940) failed
PMPI_Comm_rank(66).: Null communicator

[cli_972]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff3d9474b0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_926]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffa87bcad0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff6e82af90) failed
PMPI_Comm_rank(66).: Null communicator

[cli_920]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffebc1e230) failed
PMPI_Comm_rank(66).: Null communicator

[cli_976]: [cli_980]: [cli_978]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff159839f0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff965f58d0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_982]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff367acab0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff918bfd20) failed
PMPI_Comm_rank(66).: Null communicator

[cli_988]: [cli_966]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff4ee932d0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_970]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffd3c61f30) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff71251430) failed
PMPI_Comm_rank(66).: Null communicator

[cli_968]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffac5bf010) failed
PMPI_Comm_rank(66).: Null communicator

[cli_974]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffac46d490) failed
PMPI_Comm_rank(66).: Null communicator

[cli_784]: [cli_786]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff7481db10) failed
PMPI_Comm_rank(66).: Null communicator

[cli_788]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffdbbe8600) failed
PMPI_Comm_rank(66).: Null communicator

[cli_796]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff22274eb0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_790]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff97cf1e80) failed
PMPI_Comm_rank(66).: Null communicator

[cli_798]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff3b2e83a0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff4a3322e0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_792]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff63054320) failed
PMPI_Comm_rank(66).: Null communicator

[cli_794]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffaf570bd0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_730]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffdb7e6930) failed
PMPI_Comm_rank(66).: Null communicator

[cli_986]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff86662460) failed
PMPI_Comm_rank(66).: Null communicator

[cli_1028]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffebfb4c40) failed
PMPI_Comm_rank(66).: Null communicator

[cli_1026]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff7af9a330) failed
PMPI_Comm_rank(66).: Null communicator

[cli_990]: [cli_984]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffc0fff940) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffc7e27c50) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff84558690) failed
PMPI_Comm_rank(66).: Null communicator

[cli_1030]: [cli_1038]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffa06bbbd0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff0cd0b3a0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_1034]: [cli_1036]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff8f0bb070) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffedc83570) failed
PMPI_Comm_rank(66).: Null communicator

[cli_1032]: [cli_944]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffd7666490) failed
PMPI_Comm_rank(66).: Null communicator

[cli_948]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff45064c10) failed
PMPI_Comm_rank(66).: Null communicator

[cli_946]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff3321d9e0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_950]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff093ce770) failed
PMPI_Comm_rank(66).: Null communicator

[cli_956]: [cli_954]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff4e9b33b0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_952]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffc82ed320) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff8f3657f0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_958]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff3bb755e0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_832]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff62735440) failed
PMPI_Comm_rank(66).: Null communicator

[cli_836]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff56e78290) failed
PMPI_Comm_rank(66).: Null communicator

[cli_834]: [cli_838]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff877fa6f0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_844]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffe35f9b70) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffb2f43bf0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_840]: [cli_842]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffdd729c10) failed
PMPI_Comm_rank(66).: Null communicator

[cli_846]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff5972df50) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffb7573b20) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff9a3de330) failed
PMPI_Comm_rank(66).: Null communicator

[cli_848]: [cli_852]: [cli_816]: [cli_818]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffa8d38fc0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_822]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff4bc4cbc0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_820]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffa1d47730) failed
PMPI_Comm_rank(66).: Null communicator

[cli_826]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffaa87c090) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff2c311300) failed
PMPI_Comm_rank(66).: Null communicator

[cli_828]: [cli_830]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff8c54fee0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_824]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffdfcd54c0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff79b0a8a0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff55e98730) failed
PMPI_Comm_rank(66).: Null communicator

[cli_850]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff897cb580) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff9f370de0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_854]: [cli_862]: [cli_858]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffdd563fa0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff4562d650) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff52347660) failed
PMPI_Comm_rank(66).: Null communicator

[cli_856]: [cli_860]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffcead79e0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff2f541480) failed
PMPI_Comm_rank(66).: Null communicator

[cli_880]: [cli_752]: [cli_754]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7ffff891dd40) failed
PMPI_Comm_rank(66).: Null communicator

[cli_756]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff896577b0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_758]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff39492380) failed
PMPI_Comm_rank(66).: Null communicator

[cli_764]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff15e1b2e0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffa5398d90) failed
PMPI_Comm_rank(66).: Null communicator

[cli_760]: [cli_762]: [cli_766]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff83950a30) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffabfaffa0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff10d9c660) failed
PMPI_Comm_rank(66).: Null communicator

[c494-602.stampede.tacc.utexas.edu:mpispawn_44][readline] Unexpected End-Of-File on file descriptor 14. MPI process died?
[c494-602.stampede.tacc.utexas.edu:mpispawn_44][mtpmi_processops] Error while reading PMI socket. MPI process died?
aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff06797150) failed
PMPI_Comm_rank(66).: Null communicator

[cli_884]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7ffff6aaa870) failed
PMPI_Comm_rank(66).: Null communicator

[cli_882]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff25e3ce50) failed
PMPI_Comm_rank(66).: Null communicator

[cli_886]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff4f2eca80) failed
PMPI_Comm_rank(66).: Null communicator

[cli_888]: [cli_894]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffb5002ec0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_890]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff066f98f0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff8ca6ea40) failed
PMPI_Comm_rank(66).: Null communicator

[cli_892]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff75dc9690) failed
PMPI_Comm_rank(66).: Null communicator

[cli_896]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff2a8dcd80) failed
PMPI_Comm_rank(66).: Null communicator

[cli_900]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff15aad960) failed
PMPI_Comm_rank(66).: Null communicator

[cli_898]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffa7027ba0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_902]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff43523860) failed
PMPI_Comm_rank(66).: Null communicator

[cli_910]: [cli_906]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7ffff896c830) failed
PMPI_Comm_rank(66).: Null communicator

[cli_908]: [cli_904]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff68e02cb0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffc8ac7a40) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff403c3e80) failed
PMPI_Comm_rank(66).: Null communicator

[cli_928]: [cli_930]: [cli_1008]: [cli_1010]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff8acacbf0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_994]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff512b4770) failed
PMPI_Comm_rank(66).: Null communicator

[cli_996]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff73b43e20) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff69e156d0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_934]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffb3c336e0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_932]: [cli_940]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff2a2b5570) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffc601a860) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff555c2480) failed
PMPI_Comm_rank(66).: Null communicator

[cli_942]: [cli_938]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffc1628780) failed
PMPI_Comm_rank(66).: Null communicator

[cli_936]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffac7d35f0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff7b3634a0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffba7bb380) failed
PMPI_Comm_rank(66).: Null communicator

[cli_1014]: [cli_1012]: [cli_1020]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffc66afe20) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff615460b0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffc74a85a0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_1018]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffeca43970) failed
PMPI_Comm_rank(66).: Null communicator

[cli_1022]: [cli_1016]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff3d0e9710) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffcc220270) failed
PMPI_Comm_rank(66).: Null communicator

[cli_998]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffc0cd8150) failed
PMPI_Comm_rank(66).: Null communicator

[cli_1002]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff53dfbc40) failed
PMPI_Comm_rank(66).: Null communicator

[cli_1004]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff402f1150) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffae9b3d10) failed
PMPI_Comm_rank(66).: Null communicator

[cli_1006]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffc8130460) failed
PMPI_Comm_rank(66).: Null communicator

[cli_1000]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff63e274f0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_1064]: [cli_1070]: [cli_1066]: [cli_1068]: [cli_1062]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7ffff2bacb50) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff5ca21d60) failed
PMPI_Comm_rank(66).: Null communicator

[cli_1058]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff8c064fb0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_1056]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7ffff3561020) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffd0f48160) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffa8891480) failed
PMPI_Comm_rank(66).: Null communicator

[cli_1060]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff3faf2ae0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff1335ffe0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_1100]: [cli_1094]: [cli_1088]: [cli_1090]: [cli_1098]: [cli_1102]: [cli_1092]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff2fe66c10) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffe682ae60) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff0d7376f0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff2bac6160) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff4487a460) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff3f181390) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7ffffc03c720) failed
PMPI_Comm_rank(66).: Null communicator

[cli_1096]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff56283770) failed
PMPI_Comm_rank(66).: Null communicator

[cli_1080]: [cli_1084]: [cli_1076]: [cli_1074]: [cli_1086]: [cli_1078]: [cli_1082]: [cli_1072]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffa3c81800) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffc1be0060) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffa6f9a900) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff1acb7820) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff90fbaee0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff530a7c50) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff32960e70) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff66c801d0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_1110]: [cli_1106]: [cli_1104]: [cli_1108]: [cli_1118]: [cli_1116]: [cli_1114]: [cli_1112]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff8a67caa0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff48c57f40) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff98d55fa0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff7f8f3bb0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff1a16ba80) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffc7ff7ff0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffd611d3d0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff19cd9be0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_1176]: [cli_1182]: [cli_1170]: [cli_1178]: [cli_1180]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff9ced53b0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7ffffcfe8240) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff71d8efb0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_1174]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff5edb4d90) failed
PMPI_Comm_rank(66).: Null communicator

[cli_1172]: [cli_1168]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffd5e47730) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffe91f69b0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffa26fbc70) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff0a231ce0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_1152]: [cli_1160]: [cli_1164]: [cli_1166]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff8fc02400) failed
PMPI_Comm_rank(66).: Null communicator

[cli_1156]: [cli_1162]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7ffffb8c57d0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_1154]: [cli_1158]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffdad41de0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff879d52e0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff9c098cf0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffbf6cfca0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff57873e60) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff555105a0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_1198]: [cli_1196]: [cli_1192]: [cli_1190]: [cli_1186]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7ffffdf04600) failed
PMPI_Comm_rank(66).: Null communicator

[cli_1194]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff896cc7f0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_1188]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffed01d3e0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff1dcae3a0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_1184]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffac47a0d0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff016b53f0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffee717cf0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff77492460) failed
PMPI_Comm_rank(66).: Null communicator

[cli_1128]: [cli_1134]: [cli_1122]: [cli_1132]: [cli_1124]: [cli_1120]: [cli_1126]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffe4bad850) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff2669bf40) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffcced05c0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff06e26d30) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff4cd7f560) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff608376a0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffd8f6fa40) failed
PMPI_Comm_rank(66).: Null communicator

[cli_1130]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff60e6d0f0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_1146]: [cli_1150]: [cli_1144]: [cli_1142]: [cli_1148]: [cli_1140]: [cli_1138]: [cli_1136]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffa06a1530) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff4988f6b0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffe3afa360) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff1f8d1570) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff72e8cb10) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff41934a90) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff263e8b60) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff5d69f140) failed
PMPI_Comm_rank(66).: Null communicator

[cli_1204]: [cli_1202]: [cli_1206]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff3c7816c0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff987bb8c0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffb19b5770) failed
PMPI_Comm_rank(66).: Null communicator

[c494-703.stampede.tacc.utexas.edu:mpispawn_46][readline] Unexpected End-Of-File on file descriptor 9. MPI process died?
[c494-703.stampede.tacc.utexas.edu:mpispawn_46][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c494-702.stampede.tacc.utexas.edu:mpispawn_45][readline] Unexpected End-Of-File on file descriptor 10. MPI process died?
[c494-702.stampede.tacc.utexas.edu:mpispawn_45][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c497-102.stampede.tacc.utexas.edu:mpispawn_51][readline] Unexpected End-Of-File on file descriptor 10. MPI process died?
[c497-102.stampede.tacc.utexas.edu:mpispawn_51][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c495-002.stampede.tacc.utexas.edu:mpispawn_49][readline] Unexpected End-Of-File on file descriptor 19. MPI process died?
[c495-002.stampede.tacc.utexas.edu:mpispawn_49][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c497-202.stampede.tacc.utexas.edu:mpispawn_52][readline] Unexpected End-Of-File on file descriptor 15. MPI process died?
[c497-202.stampede.tacc.utexas.edu:mpispawn_52][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c497-504.stampede.tacc.utexas.edu:mpispawn_53][readline] Unexpected End-Of-File on file descriptor 9. MPI process died?
[c497-504.stampede.tacc.utexas.edu:mpispawn_53][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c497-601.stampede.tacc.utexas.edu:mpispawn_54][readline] Unexpected End-Of-File on file descriptor 15. MPI process died?
[c497-601.stampede.tacc.utexas.edu:mpispawn_54][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c497-603.stampede.tacc.utexas.edu:mpispawn_56][readline] Unexpected End-Of-File on file descriptor 8. MPI process died?
[c497-603.stampede.tacc.utexas.edu:mpispawn_56][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c497-602.stampede.tacc.utexas.edu:mpispawn_55][readline] Unexpected End-Of-File on file descriptor 8. MPI process died?
[c497-602.stampede.tacc.utexas.edu:mpispawn_55][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c494-904.stampede.tacc.utexas.edu:mpispawn_47][readline] Unexpected End-Of-File on file descriptor 10. MPI process died?
[c494-904.stampede.tacc.utexas.edu:mpispawn_47][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c495-201.stampede.tacc.utexas.edu:mpispawn_50][readline] Unexpected End-Of-File on file descriptor 8. MPI process died?
[c495-201.stampede.tacc.utexas.edu:mpispawn_50][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c498-304.stampede.tacc.utexas.edu:mpispawn_57][readline] Unexpected End-Of-File on file descriptor 15. MPI process died?
[c498-304.stampede.tacc.utexas.edu:mpispawn_57][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c498-402.stampede.tacc.utexas.edu:mpispawn_58][readline] Unexpected End-Of-File on file descriptor 10. MPI process died?
[c498-402.stampede.tacc.utexas.edu:mpispawn_58][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c498-603.stampede.tacc.utexas.edu:mpispawn_59][readline] Unexpected End-Of-File on file descriptor 8. MPI process died?
[c498-603.stampede.tacc.utexas.edu:mpispawn_59][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c501-801.stampede.tacc.utexas.edu:mpispawn_63][readline] Unexpected End-Of-File on file descriptor 17. MPI process died?
[c501-801.stampede.tacc.utexas.edu:mpispawn_63][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c501-204.stampede.tacc.utexas.edu:mpispawn_60][readline] Unexpected End-Of-File on file descriptor 16. MPI process died?
[c501-204.stampede.tacc.utexas.edu:mpispawn_60][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c501-604.stampede.tacc.utexas.edu:mpispawn_62][readline] Unexpected End-Of-File on file descriptor 15. MPI process died?
[c501-604.stampede.tacc.utexas.edu:mpispawn_62][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c501-301.stampede.tacc.utexas.edu:mpispawn_61][readline] Unexpected End-Of-File on file descriptor 8. MPI process died?
[c501-301.stampede.tacc.utexas.edu:mpispawn_61][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c504-602.stampede.tacc.utexas.edu:mpispawn_66][readline] Unexpected End-Of-File on file descriptor 14. MPI process died?
[c504-602.stampede.tacc.utexas.edu:mpispawn_66][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c505-202.stampede.tacc.utexas.edu:mpispawn_71][readline] Unexpected End-Of-File on file descriptor 15. MPI process died?
[c505-202.stampede.tacc.utexas.edu:mpispawn_71][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c504-604.stampede.tacc.utexas.edu:mpispawn_67][readline] Unexpected End-Of-File on file descriptor 15. MPI process died?
[c504-604.stampede.tacc.utexas.edu:mpispawn_67][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c505-502.stampede.tacc.utexas.edu:mpispawn_75][readline] Unexpected End-Of-File on file descriptor 8. MPI process died?
[c505-502.stampede.tacc.utexas.edu:mpispawn_75][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c504-801.stampede.tacc.utexas.edu:mpispawn_69][readline] Unexpected End-Of-File on file descriptor 8. MPI process died?
[c504-801.stampede.tacc.utexas.edu:mpispawn_69][mtpmi_processops] Error while reading PMI socket. MPI process died?
[cli_1200]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff67e7a130) failed
PMPI_Comm_rank(66).: Null communicator

[c505-204.stampede.tacc.utexas.edu:mpispawn_73][readline] Unexpected End-Of-File on file descriptor 10. MPI process died?
[c505-204.stampede.tacc.utexas.edu:mpispawn_73][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c504-702.stampede.tacc.utexas.edu:mpispawn_68][readline] Unexpected End-Of-File on file descriptor 7. MPI process died?
[c504-702.stampede.tacc.utexas.edu:mpispawn_68][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c505-401.stampede.tacc.utexas.edu:mpispawn_74][readline] Unexpected End-Of-File on file descriptor 20. MPI process died?
[c505-401.stampede.tacc.utexas.edu:mpispawn_74][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c505-203.stampede.tacc.utexas.edu:mpispawn_72][readline] Unexpected End-Of-File on file descriptor 6. MPI process died?
[c505-203.stampede.tacc.utexas.edu:mpispawn_72][mtpmi_processops] Error while reading PMI socket. MPI process died?
[cli_960]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff6cf8d960) failed
PMPI_Comm_rank(66).: Null communicator

[c494-403.stampede.tacc.utexas.edu:mpispawn_43][child_handler] MPI process (rank: 696, pid: 59046) exited with status 1
[c505-502.stampede.tacc.utexas.edu:mpispawn_75][child_handler] MPI process (rank: 1204, pid: 24563) exited with status 1
[cli_1208]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff77ad0900) failed
PMPI_Comm_rank(66).: Null communicator

[c494-602.stampede.tacc.utexas.edu:mpispawn_44][child_handler] MPI process (rank: 712, pid: 14472) exited with status 1
[cli_1214]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff74700e10) failed
PMPI_Comm_rank(66).: Null communicator

[c495-001.stampede.tacc.utexas.edu:mpispawn_48][child_handler] MPI process (rank: 772, pid: 63171) exited with status 1
[c498-402.stampede.tacc.utexas.edu:mpispawn_58][child_handler] MPI process (rank: 934, pid: 16748) exited with status 1
[c497-504.stampede.tacc.utexas.edu:mpispawn_53][child_handler] MPI process (rank: 852, pid: 38580) exited with status 1
[cli_1212]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff77c0dc80) failed
PMPI_Comm_rank(66).: Null communicator

[cli_1210]: [cli_1218]: [cli_1216]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff04416bc0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff6d991880) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff048ef790) failed
PMPI_Comm_rank(66).: Null communicator

[cli_1228]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff31dd8d60) failed
PMPI_Comm_rank(66).: Null communicator

[cli_1230]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff8eebf860) failed
PMPI_Comm_rank(66).: Null communicator

[cli_1222]: [cli_1226]: [cli_1220]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff1aaba350) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff4aaf84d0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff97fad900) failed
PMPI_Comm_rank(66).: Null communicator

[c497-202.stampede.tacc.utexas.edu:mpispawn_52][child_handler] MPI process (rank: 844, pid: 9767) exited with status 1
[cli_1224]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7ffff0a39a40) failed
PMPI_Comm_rank(66).: Null communicator

[cli_1242]: [cli_1244]: [cli_1236]: [cli_1238]: [cli_1246]: [cli_1234]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff1b234340) failed
PMPI_Comm_rank(66).: Null communicator

[cli_1232]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff02e95980) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffd02cd020) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff8af67560) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff9eb9eb80) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffc909d8d0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffbef55000) failed
PMPI_Comm_rank(66).: Null communicator

[c497-102.stampede.tacc.utexas.edu:mpispawn_51][child_handler] MPI process (rank: 820, pid: 19308) exited with status 1
[c494-703.stampede.tacc.utexas.edu:mpispawn_46][child_handler] MPI process (rank: 750, pid: 59518) exited with status 1
[cli_1256]: [cli_1262]: [cli_1258]: [cli_1260]: [cli_1254]: [cli_1250]: [cli_1252]: [cli_1248]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff0c386c60) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff1967d260) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff706987a0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff52eac250) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffda9ce190) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff178af680) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffa5cb3290) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff8b1fa150) failed
PMPI_Comm_rank(66).: Null communicator

[cli_1288]: [cli_1280]: [cli_1294]: [cli_1292]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff50513c90) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff06c47f70) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff30f7ad50) failed
PMPI_Comm_rank(66).: Null communicator

[cli_1282]: [cli_1284]: [cli_1290]: [cli_1286]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff69ffb4d0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff2ba04b70) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff6a6c8e90) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff40e2fa80) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff56081e40) failed
PMPI_Comm_rank(66).: Null communicator

[cli_1306]: [cli_1304]: [cli_1302]: [cli_1298]: [cli_1310]: [cli_1308]: [cli_1300]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffd1abf4d0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff1fc23ea0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff33dc90a0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff5331d0a0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff07523910) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff9345ca90) failed
PMPI_Comm_rank(66).: Null communicator

[cli_1296]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff2351bda0) failed
PMPI_Comm_rank(66).: Null communicator

[c501-604.stampede.tacc.utexas.edu:mpispawn_62][child_handler] MPI process (rank: 1000, pid: 37203) exited with status 1
aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffcb260680) failed
PMPI_Comm_rank(66).: Null communicator

[cli_1326]: [cli_1322]: [cli_1324]: [cli_1316]: [cli_1320]: [cli_1314]: [cli_1312]: [cli_1318]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff2555bed0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffbae43380) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff0a0eecb0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff968909f0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffdb41bfe0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff5a32d290) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff5014c270) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff40d7b050) failed
PMPI_Comm_rank(66).: Null communicator

[cli_1340]: [cli_1342]: [cli_1338]: [cli_1334]: [cli_1336]: [cli_1328]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff92a56b40) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff76922a90) failed
PMPI_Comm_rank(66).: Null communicator

[cli_1332]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff68b12d50) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff80e6ed10) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffef552160) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff2e3863b0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_1330]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffe61ea830) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff52c62930) failed
PMPI_Comm_rank(66).: Null communicator

[cli_1350]: [cli_1346]: [cli_1356]: [cli_1352]: [cli_1358]: [cli_1348]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff0af1b200) failed
PMPI_Comm_rank(66).: Null communicator

[cli_1354]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff0237fd00) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7ffff2e78d90) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff14645a10) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffb7d7eef0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff477756b0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff6d433740) failed
PMPI_Comm_rank(66).: Null communicator

[c494-904.stampede.tacc.utexas.edu:mpispawn_47][child_handler] MPI process (rank: 766, pid: 87941) exited with status 1
[cli_1388]: [cli_1390]: [cli_1380]: [cli_1384]: [cli_1382]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffbe36b2a0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffca2f7df0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_1378]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7ffffe20d350) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff7471e9b0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_1376]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffc683b3a0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff06d68570) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffca4d9400) failed
PMPI_Comm_rank(66).: Null communicator

[cli_1386]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff699c88a0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_1396]: [cli_1392]: [cli_1398]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff81ac6040) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff60b016b0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff3870ef50) failed
PMPI_Comm_rank(66).: Null communicator

[cli_1402]: [cli_1406]: [cli_1400]: [cli_1404]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffa889e190) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff9769a410) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff4af671a0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff1ac81be0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_1394]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff4ceffcc0) failed
PMPI_Comm_rank(66).: Null communicator

[c501-301.stampede.tacc.utexas.edu:mpispawn_61][child_handler] MPI process (rank: 988, pid: 55819) exited with status 1
[cli_1240]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff8296e2f0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_1420]: [cli_1418]: [cli_1416]: [cli_1422]: [cli_1408]: [cli_1414]: [cli_1412]: [cli_1410]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7ffff6be0220) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff6c98a3e0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff80a4b820) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff627e9370) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffc67cdef0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffb0074040) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff44ffebb0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffe88c04b0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_1370]: [cli_1374]: [cli_1372]: [cli_1368]: [cli_1360]: [cli_1362]: [cli_1366]: [cli_1364]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff1c020cc0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7ffffff34880) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7ffffc57a7e0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff460aee40) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff79cdc3f0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7ffff93349e0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7ffff85a6ee0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffb2681040) failed
PMPI_Comm_rank(66).: Null communicator

[c501-204.stampede.tacc.utexas.edu:mpispawn_60][child_handler] MPI process (rank: 972, pid: 76420) exited with status 1
[cli_1444]: [cli_1440]: [cli_1446]: [cli_1452]: [cli_1448]: [cli_1450]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff68068b00) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff8bb4fcf0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7ffffbc378b0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff28ac1170) failed
PMPI_Comm_rank(66).: Null communicator

[cli_1442]: [cli_1454]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffc078e7a0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff91ee3c40) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffc71d45e0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff09f71450) failed
PMPI_Comm_rank(66).: Null communicator

[c505-204.stampede.tacc.utexas.edu:mpispawn_73][child_handler] MPI process (rank: 1178, pid: 23240) exited with status 1
[cli_1438]: [cli_1426]: [cli_1428]: [cli_1434]: [cli_1430]: [cli_1436]: [cli_1424]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffbdc68a30) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff81d15120) failed
PMPI_Comm_rank(66).: Null communicator

[cli_1432]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff9748abc0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff40b6e4c0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff12810cf0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff413cf120) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffcc3559f0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff74430f20) failed
PMPI_Comm_rank(66).: Null communicator

[cli_1466]: [cli_1470]: [cli_1460]: [cli_1468]: [cli_1464]: [cli_1458]: [cli_1456]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff0b650250) failed
PMPI_Comm_rank(66).: Null communicator

[cli_1462]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff8509e1f0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff0ec30840) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffda5ace20) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffb0183cd0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff547f9ee0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffdad95650) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff8fbd3730) failed
PMPI_Comm_rank(66).: Null communicator

[c504-604.stampede.tacc.utexas.edu:mpispawn_67][child_handler] MPI process (rank: 1082, pid: 16512) exited with status 1
[cli_1344]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffc5829190) failed
PMPI_Comm_rank(66).: Null communicator

[cli_1474]: [cli_1480]: [cli_1482]: [cli_1484]: [cli_1476]: [cli_1478]: [cli_1472]: [cli_1486]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff0808e830) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff43736340) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff9c6e3e40) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff2fdcf1b0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff54ce1140) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff511c2940) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff526e5720) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffe2dae210) failed
PMPI_Comm_rank(66).: Null communicator

[cli_1514]: [cli_1518]: [cli_1512]: [cli_1516]: [cli_1510]: [cli_1508]: [cli_1504]: [cli_1506]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff62f28210) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffc0f8da90) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffd77580c0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffff8762a0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff8cdde370) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff73700b10) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff3528c1f0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff4ba1e4c0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_1550]: [cli_1546]: [cli_1548]: [cli_1544]: [cli_1542]: [cli_1540]: [cli_1536]: [cli_1538]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff7e7b4ac0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff5a412b10) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff6c992da0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffaf972c00) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff3206f530) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff06470f30) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff601a94e0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffbb20b0e0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_1494]: [cli_1490]: [cli_1492]: [cli_1488]: [cli_1496]: [cli_1502]: [cli_1498]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff9b3b03f0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff3a6db340) failed
PMPI_Comm_rank(66).: Null communicator

[cli_1500]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff1a613e30) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff23cdb340) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffa24a44d0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff74fe6d40) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff659e4c50) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff4346c540) failed
PMPI_Comm_rank(66).: Null communicator

[cli_1554]: [cli_1560]: [cli_1556]: [cli_1566]: [cli_1562]: [cli_1558]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffbc90a580) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffd633bf40) failed
PMPI_Comm_rank(66).: Null communicator

[cli_1564]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff08145580) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff29706760) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffeaf07fb0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff8563ea70) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffc54f0d60) failed
PMPI_Comm_rank(66).: Null communicator

[cli_1552]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffa7f398a0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_1574]: [cli_1568]: [cli_1572]: [cli_1580]: [cli_1576]: [cli_1570]: [cli_1582]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff2d9e7ee0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff38450e90) failed
PMPI_Comm_rank(66).: Null communicator

[cli_1578]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff8a165890) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffb9a9bfb0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff00d3e8f0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7ffff3bc6ab0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffb6565af0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff29427f70) failed
PMPI_Comm_rank(66).: Null communicator

[c504-801.stampede.tacc.utexas.edu:mpispawn_69][child_handler] MPI process (rank: 1118, pid: 13407) exited with status 1
[cli_1596]: [cli_1590]: [cli_1598]: [cli_1584]: [cli_1586]: [cli_1588]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff3e34fd80) failed
PMPI_Comm_rank(66).: Null communicator

[cli_1594]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff35f36030) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff03d19170) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffb96ebfd0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff46dfe0d0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff5ab25770) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff05d67030) failed
PMPI_Comm_rank(66).: Null communicator

[cli_1592]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff7284ac40) failed
PMPI_Comm_rank(66).: Null communicator

[c498-304.stampede.tacc.utexas.edu:mpispawn_57][child_handler] MPI process (rank: 918, pid: 37111) exited with status 1
[cli_1628]: [cli_1630]: [cli_1626]: [cli_1624]: [cli_1620]: [cli_1622]: [cli_1618]: [cli_1616]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff92931b10) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff475808d0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffb418e710) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff01da4520) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff415f1850) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffac304b30) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffc3bd9040) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffe00b2650) failed
PMPI_Comm_rank(66).: Null communicator

[cli_1646]: [cli_1634]: [cli_1644]: [cli_1636]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff0c50cd30) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff636ab170) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff4a24a360) failed
PMPI_Comm_rank(66).: Null communicator

[cli_1640]: [cli_1638]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff2f9db720) failed
PMPI_Comm_rank(66).: Null communicator

[cli_1642]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff6fe77760) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff0d97cd80) failed
PMPI_Comm_rank(66).: Null communicator

[cli_1662]: [cli_1658]: [cli_1656]: [cli_1660]: [cli_1654]: [cli_1650]: [cli_1648]: [cli_1652]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff34701d40) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff3f87c810) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff6b82aae0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff8f97be80) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffaad6f3d0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff0a195ce0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff6f30a710) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff485102c0) failed
PMPI_Comm_rank(66).: Null communicator

[c505-503.stampede.tacc.utexas.edu:mpispawn_76][readline] Unexpected End-Of-File on file descriptor 7. MPI process died?
[c505-503.stampede.tacc.utexas.edu:mpispawn_76][mtpmi_processops] Error while reading PMI socket. MPI process died?
aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff102b50d0) failed
PMPI_Comm_rank(66).: Null communicator

[c495-201.stampede.tacc.utexas.edu:mpispawn_50][child_handler] MPI process (rank: 814, pid: 34854) exited with status 1
[cli_1712]: [cli_1718]: [cli_1714]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff36d7c550) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff613be430) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff95c22260) failed
PMPI_Comm_rank(66).: Null communicator

[cli_1682]: [cli_1694]: [cli_1692]: [cli_1680]: [cli_1684]: [cli_1690]: [cli_1686]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff7c46f480) failed
PMPI_Comm_rank(66).: Null communicator

[cli_1688]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffa29faf70) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffc2f55d90) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff873df350) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffdb8e1f80) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffac70cf30) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff42670330) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffd82c4bf0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_1520]: [cli_1530]: [cli_1526]: [cli_1532]: [cli_1522]: [cli_1534]: [cli_1528]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff30422770) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff12113c80) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffc170d470) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff0baa37c0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff00f5de50) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff087f0cf0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_1524]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7ffffbb89910) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff3cec7420) failed
PMPI_Comm_rank(66).: Null communicator

[c504-702.stampede.tacc.utexas.edu:mpispawn_68][child_handler] MPI process (rank: 1098, pid: 22896) exited with status 1
[c497-602.stampede.tacc.utexas.edu:mpispawn_55][child_handler] MPI process (rank: 882, pid: 84192) exited with status 1
[cli_1672]: [cli_1678]: [cli_1668]: [cli_1666]: [cli_1664]: [cli_1676]: [cli_1670]: [cli_1674]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff5b2b13a0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffacd60300) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffc1b31e90) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff3f292330) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffb7c15df0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff3bb8a890) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff99274700) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff6b5cd080) failed
PMPI_Comm_rank(66).: Null communicator

[cli_1632]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffba31edc0) failed
PMPI_Comm_rank(66).: Null communicator

[c505-203.stampede.tacc.utexas.edu:mpispawn_72][child_handler] MPI process (rank: 1156, pid: 31319) exited with status 1
[cli_1720]: [c498-603.stampede.tacc.utexas.edu:mpispawn_59][child_handler] MPI process (rank: 954, pid: 117243) exited with status 1
[cli_0]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffcea2ab60) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff09afccf0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_1054]: [cli_1046]: [cli_1048]: [cli_1040]: [cli_1050]: [cli_1052]: [cli_1042]: [cli_1044]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff3e304b00) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffe51b92e0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff6c3fef70) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffac4b1b10) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffe803ac60) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff6c9d2d60) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffcd8aa890) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff14aaa4b0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_1724]: [cli_1726]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffaabde600) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff4378f8f0) failed
PMPI_Comm_rank(66).: Null communicator

[c505-401.stampede.tacc.utexas.edu:mpispawn_74][child_handler] MPI process (rank: 1196, pid: 18477) exited with status 1
[c505-202.stampede.tacc.utexas.edu:mpispawn_71][child_handler] MPI process (rank: 1148, pid: 7414) exited with status 1
[cli_1716]: [c505-001.stampede.tacc.utexas.edu:mpispawn_70][readline] Unexpected End-Of-File on file descriptor 8. MPI process died?
[c505-001.stampede.tacc.utexas.edu:mpispawn_70][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c495-002.stampede.tacc.utexas.edu:mpispawn_49][child_handler] MPI process (rank: 796, pid: 124580) exited with status 1
aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffd374aa60) failed
PMPI_Comm_rank(66).: Null communicator

[c494-702.stampede.tacc.utexas.edu:mpispawn_45][child_handler] MPI process (rank: 734, pid: 57732) exited with status 1
[cli_1722]: [c505-903.stampede.tacc.utexas.edu:mpispawn_81][readline] Unexpected End-Of-File on file descriptor 15. MPI process died?
[c505-903.stampede.tacc.utexas.edu:mpispawn_81][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c505-904.stampede.tacc.utexas.edu:mpispawn_82][readline] Unexpected End-Of-File on file descriptor 9. MPI process died?
[c505-904.stampede.tacc.utexas.edu:mpispawn_82][mtpmi_processops] Error while reading PMI socket. MPI process died?
aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff5aea0800) failed
PMPI_Comm_rank(66).: Null communicator

[c505-902.stampede.tacc.utexas.edu:mpispawn_80][readline] Unexpected End-Of-File on file descriptor 7. MPI process died?
[c505-902.stampede.tacc.utexas.edu:mpispawn_80][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c497-601.stampede.tacc.utexas.edu:mpispawn_54][child_handler] MPI process (rank: 878, pid: 86018) exited with status 1
[c505-504.stampede.tacc.utexas.edu:mpispawn_77][readline] Unexpected End-Of-File on file descriptor 7. MPI process died?
[c505-504.stampede.tacc.utexas.edu:mpispawn_77][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c506-704.stampede.tacc.utexas.edu:mpispawn_84][readline] Unexpected End-Of-File on file descriptor 15. MPI process died?
[c506-704.stampede.tacc.utexas.edu:mpispawn_84][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c506-302.stampede.tacc.utexas.edu:mpispawn_83][readline] Unexpected End-Of-File on file descriptor 14. MPI process died?
[c506-302.stampede.tacc.utexas.edu:mpispawn_83][mtpmi_processops] Error while reading PMI socket. MPI process died?
[cli_1736]: [cli_1742]: [cli_1738]: [cli_1740]: [cli_1728]: [cli_1734]: [cli_1732]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7ffffde50f60) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffa7ae31c0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff3dabad80) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffd0b1ad40) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7ffff01f9580) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffa497ada0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff042f0ff0) failed
PMPI_Comm_rank(66).: Null communicator

[c507-503.stampede.tacc.utexas.edu:mpispawn_87][readline] Unexpected End-Of-File on file descriptor 8. MPI process died?
[c507-503.stampede.tacc.utexas.edu:mpispawn_87][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c507-904.stampede.tacc.utexas.edu:mpispawn_89][readline] Unexpected End-Of-File on file descriptor 16. MPI process died?
[c507-904.stampede.tacc.utexas.edu:mpispawn_89][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c505-802.stampede.tacc.utexas.edu:mpispawn_78][readline] Unexpected End-Of-File on file descriptor 9. MPI process died?
[c505-802.stampede.tacc.utexas.edu:mpispawn_78][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c507-202.stampede.tacc.utexas.edu:mpispawn_86][readline] Unexpected End-Of-File on file descriptor 9. MPI process died?
[c507-202.stampede.tacc.utexas.edu:mpispawn_86][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c507-102.stampede.tacc.utexas.edu:mpispawn_85][readline] Unexpected End-Of-File on file descriptor 14. MPI process died?
[c507-102.stampede.tacc.utexas.edu:mpispawn_85][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c508-702.stampede.tacc.utexas.edu:mpispawn_93][readline] Unexpected End-Of-File on file descriptor 5. MPI process died?
[c508-702.stampede.tacc.utexas.edu:mpispawn_93][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c508-204.stampede.tacc.utexas.edu:mpispawn_91][readline] Unexpected End-Of-File on file descriptor 9. MPI process died?
[c508-204.stampede.tacc.utexas.edu:mpispawn_91][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c509-201.stampede.tacc.utexas.edu:mpispawn_99][readline] Unexpected End-Of-File on file descriptor 18. MPI process died?
[c509-201.stampede.tacc.utexas.edu:mpispawn_99][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c507-601.stampede.tacc.utexas.edu:mpispawn_88][readline] Unexpected End-Of-File on file descriptor 11. MPI process died?
[c507-601.stampede.tacc.utexas.edu:mpispawn_88][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c508-904.stampede.tacc.utexas.edu:mpispawn_97][readline] Unexpected End-Of-File on file descriptor 14. MPI process died?
[c508-904.stampede.tacc.utexas.edu:mpispawn_97][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c508-601.stampede.tacc.utexas.edu:mpispawn_92][readline] Unexpected End-Of-File on file descriptor 9. MPI process died?
[c508-601.stampede.tacc.utexas.edu:mpispawn_92][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c508-101.stampede.tacc.utexas.edu:mpispawn_90][readline] Unexpected End-Of-File on file descriptor 9. MPI process died?
[c508-101.stampede.tacc.utexas.edu:mpispawn_90][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c497-603.stampede.tacc.utexas.edu:mpispawn_56][child_handler] MPI process (rank: 896, pid: 21394) exited with status 1
[cli_1730]: [c508-703.stampede.tacc.utexas.edu:mpispawn_94][readline] Unexpected End-Of-File on file descriptor 15. MPI process died?
[c508-703.stampede.tacc.utexas.edu:mpispawn_94][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c508-903.stampede.tacc.utexas.edu:mpispawn_96][readline] Unexpected End-Of-File on file descriptor 9. MPI process died?
[c508-903.stampede.tacc.utexas.edu:mpispawn_96][mtpmi_processops] Error while reading PMI socket. MPI process died?
aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff76af9ce0) failed
PMPI_Comm_rank(66).: Null communicator

[c508-802.stampede.tacc.utexas.edu:mpispawn_95][readline] Unexpected End-Of-File on file descriptor 15. MPI process died?
[c508-802.stampede.tacc.utexas.edu:mpispawn_95][mtpmi_processops] Error while reading PMI socket. MPI process died?
[cli_1776]: [cli_1782]: [cli_1790]: [cli_1786]: [cli_1784]: [cli_1788]: [cli_1778]: [cli_1780]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff301561e0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff09e58cf0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff1ec74420) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffc04172b0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff3052acc0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffab148870) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff59337610) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff7a35c160) failed
PMPI_Comm_rank(66).: Null communicator

[cli_1810]: [cli_1812]: [cli_1808]: [cli_1816]: [cli_1820]: [cli_1822]: [cli_1818]: [cli_1814]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffbd7e0b10) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff43b71020) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7ffff93672a0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff628d6060) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffd1433350) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff6f5d1880) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffd9f2ed00) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff4b976410) failed
PMPI_Comm_rank(66).: Null communicator

[cli_1774]: [cli_1770]: [cli_1762]: [cli_1772]: [cli_1766]: [cli_1760]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff9def1630) failed
PMPI_Comm_rank(66).: Null communicator

[cli_1768]: [cli_1764]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff2e22a050) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff9fce4190) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffce2bb360) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff4a51d9d0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffa1de9a10) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7ffffa5b4650) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7ffff61091e0) failed
PMPI_Comm_rank(66).: Null communicator

[c509-401.stampede.tacc.utexas.edu:mpispawn_102][readline] Unexpected End-Of-File on file descriptor 11. MPI process died?
[c509-401.stampede.tacc.utexas.edu:mpispawn_102][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c509-501.stampede.tacc.utexas.edu:mpispawn_103][readline] Unexpected End-Of-File on file descriptor 15. MPI process died?
[c509-501.stampede.tacc.utexas.edu:mpispawn_103][mtpmi_processops] Error while reading PMI socket. MPI process died?
[cli_1846]: [cli_1844]: [cli_1848]: [cli_1842]: [cli_1850]: [cli_1854]: [cli_1852]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff2196fa50) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff7981a0f0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7ffffc7b25e0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff114b3650) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff80dbbda0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff7a7ccf40) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff0baadfa0) failed
PMPI_Comm_rank(66).: Null communicator

[c509-002.stampede.tacc.utexas.edu:mpispawn_98][readline] Unexpected End-Of-File on file descriptor 14. MPI process died?
[c509-002.stampede.tacc.utexas.edu:mpispawn_98][mtpmi_processops] Error while reading PMI socket. MPI process died?
[cli_1880]: [cli_1878]: [cli_1884]: [cli_1882]: [cli_1872]: [cli_1874]: [cli_1876]: [cli_1886]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff463522f0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff41c94c00) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffca7db050) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff71008b20) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff5581f180) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff5ec03550) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7ffff6ebf090) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffedd4e020) failed
PMPI_Comm_rank(66).: Null communicator

[cli_1830]: [cli_1838]: [cli_1832]: [cli_1824]: [cli_1834]: [cli_1826]: [cli_1828]: [cli_1836]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffa42e5700) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff98f855b0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff60285050) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff586a2080) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffba5bab30) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff7d8c1780) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff10c375e0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffa4b12c80) failed
PMPI_Comm_rank(66).: Null communicator

[c501-801.stampede.tacc.utexas.edu:mpispawn_63][child_handler] MPI process (rank: 1018, pid: 61973) exited with status 1
[cli_1930]: [cli_1928]: [cli_1926]: [cli_1932]: [cli_1924]: [cli_1922]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffb0a8b240) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff46e1b010) failed
PMPI_Comm_rank(66).: Null communicator

[cli_1920]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffb4ae9430) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff8b646380) failed
PMPI_Comm_rank(66).: Null communicator

[cli_1934]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffbec2cf20) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff344426c0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff67aef7d0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff943dd330) failed
PMPI_Comm_rank(66).: Null communicator

[cli_1900]: [cli_1902]: [cli_1894]: [cli_1888]: [cli_1896]: [cli_1892]: [cli_1890]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffa9c26000) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffcb7d6fa0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7ffff409ea80) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff1e4ba7e0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff01f1a440) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffd7d24560) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff6641c860) failed
PMPI_Comm_rank(66).: Null communicator

[cli_1898]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffb87d4360) failed
PMPI_Comm_rank(66).: Null communicator

[cli_1860]: [cli_1858]: [cli_1866]: [cli_1870]: [cli_1868]: [cli_1862]: [cli_1856]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff0a724440) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff10dd0680) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff71a69f60) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff8e350a70) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff4b44aaf0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffad45a700) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff8265f4a0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_1864]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff306143a0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_1960]: [cli_1962]: [cli_1966]: [cli_1958]: [cli_1954]: [cli_1956]: [cli_1964]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff2ad54350) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff233a7bb0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_1952]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffa13f81d0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff016f1eb0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff01966420) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffac7236a0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffef414bf0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffcd206ac0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_1912]: [cli_1918]: [cli_1916]: [cli_1914]: [cli_1908]: [cli_1910]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff245d90c0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff3aaca6d0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff1fbd1d10) failed
PMPI_Comm_rank(66).: Null communicator

[cli_1906]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffc55028d0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff18890670) failed
PMPI_Comm_rank(66).: Null communicator

[cli_1904]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffae4f6010) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff5608d890) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffd8a8ec40) failed
PMPI_Comm_rank(66).: Null communicator

[cli_1976]: [cli_1970]: [cli_1978]: [cli_1972]: [cli_1982]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff993a3630) failed
PMPI_Comm_rank(66).: Null communicator

[cli_1974]: [cli_1968]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff984b9ae0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff786982f0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff5a271b70) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffd9e678f0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff5fedc080) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff432d5480) failed
PMPI_Comm_rank(66).: Null communicator

[cli_1980]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff16dac230) failed
PMPI_Comm_rank(66).: Null communicator

[cli_1990]: [cli_1994]: [cli_1984]: [cli_1996]: [cli_1998]: [cli_1992]: [cli_1988]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffd471c070) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff3537ee60) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff0f338910) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff52861a50) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff4946e7d0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff32b4dbe0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff1e33b950) failed
PMPI_Comm_rank(66).: Null communicator

[c510-003.stampede.tacc.utexas.edu:mpispawn_105][readline] Unexpected End-Of-File on file descriptor 16. MPI process died?
[c510-003.stampede.tacc.utexas.edu:mpispawn_105][mtpmi_processops] Error while reading PMI socket. MPI process died?
[cli_2008]: [cli_2000]: [cli_2004]: [cli_2014]: [cli_2002]: [cli_2006]: [cli_2010]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffd9d09a00) failed
PMPI_Comm_rank(66).: Null communicator

[cli_2012]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffea1aae20) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffa81e6850) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffc1e94270) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff72ca4020) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff54bd3660) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff1cadb990) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff835d3700) failed
PMPI_Comm_rank(66).: Null communicator

[c509-303.stampede.tacc.utexas.edu:mpispawn_101][readline] Unexpected End-Of-File on file descriptor 16. MPI process died?
[c509-303.stampede.tacc.utexas.edu:mpispawn_101][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c510-404.stampede.tacc.utexas.edu:mpispawn_107][readline] Unexpected End-Of-File on file descriptor 5. MPI process died?
[c510-404.stampede.tacc.utexas.edu:mpispawn_107][mtpmi_processops] Error while reading PMI socket. MPI process died?
[cli_2024]: [cli_2028]: [cli_2022]: [cli_2030]: [cli_2016]: [cli_2026]: [cli_2018]: [cli_2020]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff83657b70) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff13c91ec0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff5be59d60) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff0419fc40) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffff1c3690) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff9c74e2f0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffc5dfe3d0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffbdc212c0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_2042]: [cli_2044]: [cli_2036]: [cli_2046]: [cli_2038]: [cli_2034]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffe578c4c0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff66065350) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7ffff70e5d90) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7ffff62c6990) failed
PMPI_Comm_rank(66).: Null communicator

[cli_2032]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffd572c220) failed
PMPI_Comm_rank(66).: Null communicator

[cli_2040]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff7b277230) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff645fcd00) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff1a50bef0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_1840]: [c509-502.stampede.tacc.utexas.edu:mpispawn_104][readline] Unexpected End-Of-File on file descriptor 7. MPI process died?
[c509-502.stampede.tacc.utexas.edu:mpispawn_104][mtpmi_processops] Error while reading PMI socket. MPI process died?
aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffd5ea56e0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_1986]: [c505-001.stampede.tacc.utexas.edu:mpispawn_70][child_handler] MPI process (rank: 1130, pid: 23572) exited with status 1
aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff14f027d0) failed
PMPI_Comm_rank(66).: Null communicator

[c503-901.stampede.tacc.utexas.edu:mpispawn_65][readline] Unexpected End-Of-File on file descriptor 11. MPI process died?
[c503-901.stampede.tacc.utexas.edu:mpispawn_65][mtpmi_processops] Error while reading PMI socket. MPI process died?
[cli_1750]: [cli_1756]: [cli_1752]: [cli_1754]: [cli_1748]: [cli_1758]: [cli_1746]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff99f98160) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffee3493d0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff14b057b0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff883f44d0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_1744]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffa0759fe0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7ffffd0b54a0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff95e69c90) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffc7e66590) failed
PMPI_Comm_rank(66).: Null communicator

[cli_1946]: [cli_1950]: [cli_1948]: [cli_1936]: [cli_1938]: [cli_1942]: [cli_1940]: [cli_1944]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff6c6f43a0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff4d624c70) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff9ab45200) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff180da520) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff2554f960) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff12300ea0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffc8f65cb0) failed
PMPI_Comm_rank(66).: Null communicator

[c510-501.stampede.tacc.utexas.edu:mpispawn_108][readline] Unexpected End-Of-File on file descriptor 14. MPI process died?
[c510-501.stampede.tacc.utexas.edu:mpispawn_108][mtpmi_processops] Error while reading PMI socket. MPI process died?
aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffb61a4500) failed
PMPI_Comm_rank(66).: Null communicator

[c504-602.stampede.tacc.utexas.edu:mpispawn_66][child_handler] MPI process (rank: 1064, pid: 55800) exited with status 1
[c537-902.stampede.tacc.utexas.edu:mpispawn_116][readline] Unexpected End-Of-File on file descriptor 10. MPI process died?
[c537-902.stampede.tacc.utexas.edu:mpispawn_116][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c511-601.stampede.tacc.utexas.edu:mpispawn_110][readline] Unexpected End-Of-File on file descriptor 9. MPI process died?
[c511-601.stampede.tacc.utexas.edu:mpispawn_110][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c536-801.stampede.tacc.utexas.edu:mpispawn_114][readline] Unexpected End-Of-File on file descriptor 14. MPI process died?
[c536-801.stampede.tacc.utexas.edu:mpispawn_114][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c538-104.stampede.tacc.utexas.edu:mpispawn_119][readline] Unexpected End-Of-File on file descriptor 17. MPI process died?
[c538-104.stampede.tacc.utexas.edu:mpispawn_119][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c511-903.stampede.tacc.utexas.edu:mpispawn_113][readline] Unexpected End-Of-File on file descriptor 11. MPI process died?
[c511-903.stampede.tacc.utexas.edu:mpispawn_113][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c538-001.stampede.tacc.utexas.edu:mpispawn_117][readline] Unexpected End-Of-File on file descriptor 19. MPI process died?
[c538-001.stampede.tacc.utexas.edu:mpispawn_117][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c511-602.stampede.tacc.utexas.edu:mpispawn_111][readline] Unexpected End-Of-File on file descriptor 6. MPI process died?
[c511-602.stampede.tacc.utexas.edu:mpispawn_111][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c540-903.stampede.tacc.utexas.edu:mpispawn_126][readline] Unexpected End-Of-File on file descriptor 14. MPI process died?
[c540-903.stampede.tacc.utexas.edu:mpispawn_126][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c540-904.stampede.tacc.utexas.edu:mpispawn_127][readline] Unexpected End-Of-File on file descriptor 15. MPI process died?
[c540-904.stampede.tacc.utexas.edu:mpispawn_127][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c536-902.stampede.tacc.utexas.edu:mpispawn_115][readline] Unexpected End-Of-File on file descriptor 5. MPI process died?
[c536-902.stampede.tacc.utexas.edu:mpispawn_115][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c540-901.stampede.tacc.utexas.edu:mpispawn_124][readline] Unexpected End-Of-File on file descriptor 13. MPI process died?
[c540-901.stampede.tacc.utexas.edu:mpispawn_124][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c538-003.stampede.tacc.utexas.edu:mpispawn_118][readline] Unexpected End-Of-File on file descriptor 11. MPI process died?
[c538-003.stampede.tacc.utexas.edu:mpispawn_118][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c540-202.stampede.tacc.utexas.edu:mpispawn_122][readline] Unexpected End-Of-File on file descriptor 8. MPI process died?
[c540-202.stampede.tacc.utexas.edu:mpispawn_122][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c540-902.stampede.tacc.utexas.edu:mpispawn_125][readline] Unexpected End-Of-File on file descriptor 16. MPI process died?
[c540-902.stampede.tacc.utexas.edu:mpispawn_125][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c540-104.stampede.tacc.utexas.edu:mpispawn_120][readline] Unexpected End-Of-File on file descriptor 9. MPI process died?
[c540-104.stampede.tacc.utexas.edu:mpispawn_120][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c540-804.stampede.tacc.utexas.edu:mpispawn_123][readline] Unexpected End-Of-File on file descriptor 6. MPI process died?
[c540-804.stampede.tacc.utexas.edu:mpispawn_123][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c540-201.stampede.tacc.utexas.edu:mpispawn_121][readline] Unexpected End-Of-File on file descriptor 6. MPI process died?
[c540-201.stampede.tacc.utexas.edu:mpispawn_121][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c510-503.stampede.tacc.utexas.edu:mpispawn_109][readline] Unexpected End-Of-File on file descriptor 10. MPI process died?
[c510-503.stampede.tacc.utexas.edu:mpispawn_109][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c502-104.stampede.tacc.utexas.edu:mpispawn_64][child_handler] MPI process (rank: 1036, pid: 10426) exited with status 1
[c505-902.stampede.tacc.utexas.edu:mpispawn_80][child_handler] MPI process (rank: 1284, pid: 127985) exited with status 1
[c508-101.stampede.tacc.utexas.edu:mpispawn_90][child_handler] MPI process (rank: 1452, pid: 69859) exited with status 1
[c508-904.stampede.tacc.utexas.edu:mpispawn_97][child_handler] MPI process (rank: 1564, pid: 75838) exited with status 1
[c505-904.stampede.tacc.utexas.edu:mpispawn_82][child_handler] MPI process (rank: 1316, pid: 37334) exited with status 1
[c508-702.stampede.tacc.utexas.edu:mpispawn_93][child_handler] MPI process (rank: 1490, pid: 107840) exited with status 1
[c506-302.stampede.tacc.utexas.edu:mpispawn_83][child_handler] MPI process (rank: 1338, pid: 8795) exited with status 1
[c509-201.stampede.tacc.utexas.edu:mpispawn_99][child_handler] MPI process (rank: 1596, pid: 106785) exited with status 1
[c505-504.stampede.tacc.utexas.edu:mpispawn_77][child_handler] MPI process (rank: 1246, pid: 20415) exited with status 1
[c509-501.stampede.tacc.utexas.edu:mpispawn_103][child_handler] MPI process (rank: 1652, pid: 14145) exited with status 1
[c505-802.stampede.tacc.utexas.edu:mpispawn_78][child_handler] MPI process (rank: 1260, pid: 72430) exited with status 1
[c505-903.stampede.tacc.utexas.edu:mpispawn_81][child_handler] MPI process (rank: 1300, pid: 11266) exited with status 1
[c507-102.stampede.tacc.utexas.edu:mpispawn_85][child_handler] MPI process (rank: 1372, pid: 70335) exited with status 1
[c509-502.stampede.tacc.utexas.edu:mpispawn_104][child_handler] MPI process (rank: 1666, pid: 42636) exited with status 1
[c538-104.stampede.tacc.utexas.edu:mpispawn_119][child_handler] MPI process (rank: 1918, pid: 123472) exited with status 1
[c508-601.stampede.tacc.utexas.edu:mpispawn_92][child_handler] MPI process (rank: 1484, pid: 114791) exited with status 1
[c507-503.stampede.tacc.utexas.edu:mpispawn_87][child_handler] MPI process (rank: 1402, pid: 48709) exited with status 1
[c505-503.stampede.tacc.utexas.edu:mpispawn_76][child_handler] MPI process (rank: 1218, pid: 48993) exited with status 1
[c508-204.stampede.tacc.utexas.edu:mpispawn_91][child_handler] MPI process (rank: 1464, pid: 11600) exited with status 1
[c507-601.stampede.tacc.utexas.edu:mpispawn_88][child_handler] MPI process (rank: 1414, pid: 111340) exited with status 1
[c506-704.stampede.tacc.utexas.edu:mpispawn_84][child_handler] MPI process (rank: 1354, pid: 126186) exited with status 1
[c509-002.stampede.tacc.utexas.edu:mpispawn_98][child_handler] MPI process (rank: 1582, pid: 95528) exited with status 1
[c509-401.stampede.tacc.utexas.edu:mpispawn_102][child_handler] MPI process (rank: 1644, pid: 40851) exited with status 1
[c508-703.stampede.tacc.utexas.edu:mpispawn_94][child_handler] MPI process (rank: 1516, pid: 21576) exited with status 1
[cli_1264]: [cli_1278]: [cli_1276]: [cli_1266]: [cli_1268]: [cli_1272]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff1a453610) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffcc5dad50) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffacc4e930) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff18420ad0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff715a9bf0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_1270]: [cli_1274]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff4e8a2ed0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff84bf10c0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff39169d90) failed
PMPI_Comm_rank(66).: Null communicator

[c540-201.stampede.tacc.utexas.edu:mpispawn_121][child_handler] MPI process (rank: 1938, pid: 35491) exited with status 1
[c540-804.stampede.tacc.utexas.edu:mpispawn_123][child_handler] MPI process (rank: 1974, pid: 25524) exited with status 1
[c508-903.stampede.tacc.utexas.edu:mpispawn_96][child_handler] MPI process (rank: 1550, pid: 2853) exited with status 1
[c540-904.stampede.tacc.utexas.edu:mpispawn_127][child_handler] MPI process (rank: 2044, pid: 15708) exited with status 1
[c510-003.stampede.tacc.utexas.edu:mpispawn_105][child_handler] MPI process (rank: 1688, pid: 105917) exited with status 1
[c503-901.stampede.tacc.utexas.edu:mpispawn_65][child_handler] MPI process (rank: 1050, pid: 4603) exited with status 1
[c538-001.stampede.tacc.utexas.edu:mpispawn_117][child_handler] MPI process (rank: 1886, pid: 119528) exited with status 1
[c538-003.stampede.tacc.utexas.edu:mpispawn_118][child_handler] MPI process (rank: 1902, pid: 10806) exited with status 1
[c511-903.stampede.tacc.utexas.edu:mpispawn_113][child_handler] MPI process (rank: 1818, pid: 33024) exited with status 1
[c508-802.stampede.tacc.utexas.edu:mpispawn_95][child_handler] MPI process (rank: 1530, pid: 89103) exited with status 1
[c536-902.stampede.tacc.utexas.edu:mpispawn_115][child_handler] MPI process (rank: 1852, pid: 59848) exited with status 1
[c511-602.stampede.tacc.utexas.edu:mpispawn_111][child_handler] MPI process (rank: 1790, pid: 46898) exited with status 1
[c537-902.stampede.tacc.utexas.edu:mpispawn_116][child_handler] MPI process (rank: 1868, pid: 125584) exited with status 1
[c511-601.stampede.tacc.utexas.edu:mpispawn_110][child_handler] MPI process (rank: 1772, pid: 31302) exited with status 1
[c540-104.stampede.tacc.utexas.edu:mpispawn_120][child_handler] MPI process (rank: 1926, pid: 61654) exited with status 1
[c510-404.stampede.tacc.utexas.edu:mpispawn_107][child_handler] MPI process (rank: 1726, pid: 113628) exited with status 1
[c505-901.stampede.tacc.utexas.edu:mpispawn_79][readline] Unexpected End-Of-File on file descriptor 9. MPI process died?
[c505-901.stampede.tacc.utexas.edu:mpispawn_79][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c510-503.stampede.tacc.utexas.edu:mpispawn_109][child_handler] MPI process (rank: 1756, pid: 39270) exited with status 1
[c507-904.stampede.tacc.utexas.edu:mpispawn_89][child_handler] MPI process (rank: 1424, pid: 123060) exited with status 1
[c510-501.stampede.tacc.utexas.edu:mpispawn_108][child_handler] MPI process (rank: 1742, pid: 70727) exited with status 1
[c540-202.stampede.tacc.utexas.edu:mpispawn_122][child_handler] MPI process (rank: 1966, pid: 112162) exited with status 1
[c540-901.stampede.tacc.utexas.edu:mpispawn_124][child_handler] MPI process (rank: 1998, pid: 129147) exited with status 1
[c540-903.stampede.tacc.utexas.edu:mpispawn_126][child_handler] MPI process (rank: 2024, pid: 111971) exited with status 1
[c536-801.stampede.tacc.utexas.edu:mpispawn_114][child_handler] MPI process (rank: 1826, pid: 18488) exited with status 1
[c540-902.stampede.tacc.utexas.edu:mpispawn_125][child_handler] MPI process (rank: 2010, pid: 99319) exited with status 1
[c507-202.stampede.tacc.utexas.edu:mpispawn_86][child_handler] MPI process (rank: 1388, pid: 100720) exited with status 1
[c509-303.stampede.tacc.utexas.edu:mpispawn_101][child_handler] MPI process (rank: 1626, pid: 21879) exited with status 1
[cli_1706]: [cli_1710]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffd3cdea60) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff0c492eb0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_1700]: [cli_1696]: [cli_1702]: [cli_1698]: [cli_1708]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffe7ddd5a0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff875af500) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff62400830) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffefc16b70) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff7e21d690) failed
PMPI_Comm_rank(66).: Null communicator

[cli_1704]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff6b53a5b0) failed
PMPI_Comm_rank(66).: Null communicator

[cli_1608]: [cli_1602]: [cli_1604]: [cli_1606]: [cli_1610]: [cli_1600]: [cli_1612]: [cli_1614]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff1bba8780) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffdb043d30) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff66b33880) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffca48c4d0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff8ffe6d70) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff52a18320) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffbd9d9620) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff6ec15940) failed
PMPI_Comm_rank(66).: Null communicator

[c510-401.stampede.tacc.utexas.edu:mpispawn_106][readline] Unexpected End-Of-File on file descriptor 16. MPI process died?
[c510-401.stampede.tacc.utexas.edu:mpispawn_106][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c509-301.stampede.tacc.utexas.edu:mpispawn_100][readline] Unexpected End-Of-File on file descriptor 8. MPI process died?
[c509-301.stampede.tacc.utexas.edu:mpispawn_100][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c505-901.stampede.tacc.utexas.edu:mpispawn_79][child_handler] MPI process (rank: 1276, pid: 130178) exited with status 1
[cli_1802]: [cli_1806]: [cli_1800]: [cli_1792]: [cli_1796]: [cli_1794]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff6d634a90) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff38eca670) failed
PMPI_Comm_rank(66).: Null communicator

[cli_1804]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff21396260) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff7d72ab90) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffdbd3bc70) failed
PMPI_Comm_rank(66).: Null communicator

[cli_1798]: aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff6f6cbd90) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fffe05808f0) failed
PMPI_Comm_rank(66).: Null communicator

aborting job:
Fatal error in PMPI_Comm_rank:
Invalid communicator, error stack:
PMPI_Comm_rank(108): MPI_Comm_rank(MPI_COMM_NULL, rank=0x7fff8ff12930) failed
PMPI_Comm_rank(66).: Null communicator

[c511-902.stampede.tacc.utexas.edu:mpispawn_112][readline] Unexpected End-Of-File on file descriptor 9. MPI process died?
[c511-902.stampede.tacc.utexas.edu:mpispawn_112][mtpmi_processops] Error while reading PMI socket. MPI process died?
[c510-401.stampede.tacc.utexas.edu:mpispawn_106][child_handler] MPI process (rank: 1698, pid: 44951) exited with status 1
[c509-301.stampede.tacc.utexas.edu:mpispawn_100][child_handler] MPI process (rank: 1614, pid: 8558) exited with status 1
[c511-902.stampede.tacc.utexas.edu:mpispawn_112][child_handler] MPI process (rank: 1792, pid: 38284) exited with status 1
[c468-601.stampede.tacc.utexas.edu:mpispawn_0][child_handler] MPI process (rank: 0, pid: 47634) exited with status 1
[c494-101.stampede.tacc.utexas.edu:mpispawn_41][error_sighandler] Caught error: Segmentation fault (signal 11)
bash: line 1: 16454 Segmentation fault      /usr/bin/env LD_LIBRARY_PATH=/opt/apps/gcc4_7/mvapich2/1.9/lib:/opt/apps/gcc4_7/mvapich2/1.9/lib/shared:/opt/apps/gcc/4.7.1/lib64:/opt/apps/gcc/4.7.1/lib:/opt/apps/xsede/gsi-openssh-5.7/lib64:/opt/apps/xsede/gsi-openssh-5.7/lib64 MPISPAWN_MPIRUN_MPD=0 USE_LINEAR_SSH=1 MPISPAWN_MPIRUN_HOST=c468-601.stampede.tacc.utexas.edu MPISPAWN_MPIRUN_HOSTIP=129.114.76.165 MPIRUN_RSH_LAUNCH=1 MPISPAWN_CHECKIN_PORT=56630 MPISPAWN_MPIRUN_PORT=56630 MPISPAWN_NNODES=128 MPISPAWN_GLOBAL_NPROCS=2048 MPISPAWN_MPIRUN_ID=47451 MPISPAWN_ARGC=4 MPDMAN_KVS_TEMPLATE=kvs_437_c468-601.stampede.tacc.utexas.edu_47451 MPISPAWN_LOCAL_NPROCS=16 MPISPAWN_ARGV_0='/home1/03222/tg825207/parallelnbody/teamscatter' MPISPAWN_ARGV_1='256000' MPISPAWN_ARGV_2='-c' MPISPAWN_ARGV_3='2' MPISPAWN_ARGC=4 MPISPAWN_GENERIC_ENV_COUNT=106 MPISPAWN_GENERIC_NAME_0=TACC_ENVLEN MPISPAWN_GENERIC_VALUE_0=6578 MPISPAWN_GENERIC_NAME_1=SLURM_JOB_ID MPISPAWN_GENERIC_VALUE_1="4306264" MPISPAWN_GENERIC_NAME_2=SLURMD_NODENAME MPISPAWN_GENERIC_VALUE_2="c468-601" MPISPAWN_GENERIC_NAME_3=STOCKYARD MPISPAWN_GENERIC_VALUE_3="/work/03222/tg825207" MPISPAWN_GENERIC_NAME_4=LC_ALL MPISPAWN_GENERIC_VALUE_4="en_US.UTF-8" MPISPAWN_GENERIC_NAME_5=SLURM_CHECKPOINT_IMAGE_DIR MPISPAWN_GENERIC_VALUE_5="/home1/03222/tg825207/parallelnbody" MPISPAWN_GENERIC_NAME_6=LMOD_FAMILY_MPI MPISPAWN_GENERIC_VALUE_6="mvapich2" MPISPAWN_GENERIC_NAME_7=SLURM_JOBID MPISPAWN_GENERIC_VALUE_7="4306264" MPISPAWN_GENERIC_NAME_8=_ModuleTable001_ MPISPAWN_GENERIC_VALUE_8="X01vZHVsZVRhYmxlXz17WyJhY3RpdmVTaXplIl09OCxiYXNlTXBhdGhBPXsiL29wdC9hcHBzL3hzZWRlL21vZHVsZWZpbGVzIiwiL29wdC9hcHBzL21vZHVsZWZpbGVzIiwiL29wdC9tb2R1bGVmaWxlcyIsfSxbImNfcmVidWlsZFRpbWUiXT1mYWxzZSxbImNfc2hvcnRUaW1lIl09ZmFsc2UsZmFtaWx5PXtbIk1QSSJdPSJtdmFwaWNoMiIsWyJjb21waWxlciJdPSJnY2MiLH0saW5hY3RpdmU9e30sbVQ9e0xpbnV4PXtbIkZOIl09Ii9vcHQvbW9kdWxlZmlsZXMvTGludXgiLFsiZGVmYXVsdCJdPTAsWyJmdWxsTmFtZSJdPSJMaW51eCIsWyJsb2FkT3JkZXIiXT0yLHByb3BUPXt9LFsic2hvcnQiXT0iTGludXgiLFsic3RhdHVzIl09ImFjdGl2ZSIsfSxUQUNDPXtbIkZOIl09Ii9v" MPISPAWN_GENERIC_NAME_9=TACC_SYSTEM MPISPAWN_GENERIC_VALUE_9="stampede" MPISPAWN_GENERIC_NAME_10=WORK MPISPAWN_GENERIC_VALUE_10="/work/03222/tg825207" MPISPAWN_GENERIC_NAME_11=SLURM_GTIDS MPISPAWN_GENERIC_VALUE_11="0" MPISPAWN_GENERIC_NAME_12=GCC_LIB MPISPAWN_GENERIC_VALUE_12="/opt/apps/gcc/4.7.1/lib64" MPISPAWN_GENERIC_NAME_13=SLURM_PRIO_PROCESS MPISPAWN_GENERIC_VALUE_13="0" MPISPAWN_GENERIC_NAME_14=SCRATCH MPISPAWN_GENERIC_VALUE_14="/scratch/03222/tg825207" MPISPAWN_GENERIC_NAME_15=LOGNAME MPISPAWN_GENERIC_VALUE_15="tg825207" MPISPAWN_GENERIC_NAME_16=_ModuleTable002_ MPISPAWN_GENERIC_VALUE_16="cHQvbW9kdWxlZmlsZXMvVEFDQyIsWyJkZWZhdWx0Il09MCxbImZ1bGxOYW1lIl09IlRBQ0MiLFsibG9hZE9yZGVyIl09Nixwcm9wVD17fSxbInNob3J0Il09IlRBQ0MiLFsic3RhdHVzIl09ImFjdGl2ZSIsfSxbIlRBQ0MtcGF0aHMiXT17WyJGTiJdPSIvb3B0L21vZHVsZWZpbGVzL1RBQ0MtcGF0aHMiLFsiZGVmYXVsdCJdPTAsWyJmdWxsTmFtZSJdPSJUQUNDLXBhdGhzIixbImxvYWRPcmRlciJdPTEscHJvcFQ9e30sWyJzaG9ydCJdPSJUQUNDLXBhdGhzIixbInN0YXR1cyJdPSJhY3RpdmUiLH0sY2x1c3Rlcj17WyJGTiJdPSIvb3B0L21vZHVsZWZpbGVzL2NsdXN0ZXIiLFsiZGVmYXVsdCJdPTAsWyJmdWxsTmFtZSJdPSJjbHVzdGVyIixbImxvYWRPcmRlciJdPTUscHJvcFQ9" MPISPAWN_GENERIC_NAME_17=INPUTRC MPISPAWN_GENERIC_VALUE_17="/etc/inputrc" MPISPAWN_GENERIC_NAME_18=LMOD_CMD MPISPAWN_GENERIC_VALUE_18="/opt/apps/lmod/lmod/libexec/lmod" MPISPAWN_GENERIC_NAME_19=I_MPI_OFA_ADAPTER_NAME MPISPAWN_GENERIC_VALUE_19="mlx4_0" MPISPAWN_GENERIC_NAME_20=QTLIB MPISPAWN_GENERIC_VALUE_20="/usr/lib64/qt-3.3/lib" MPISPAWN_GENERIC_NAME_21=NODE_TASKS_PPN_INFO MPISPAWN_GENERIC_VALUE_21=""16,0_"" MPISPAWN_GENERIC_NAME_22=SLURM_CPUS_ON_NODE MPISPAWN_GENERIC_VALUE_22="16" MPISPAWN_GENERIC_NAME_23=PATH MPISPAWN_GENERIC_VALUE_23="/opt/apps/gcc4_7/mvapich2/1.9/bin:/opt/apps/gcc/4.7.1/bin:/opt/apps/xalt/0.4.4/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/opt/apps/xsede/gsi-openssh-5.7/bin:/usr/X11R6/bin:/opt/ofed/bin:/opt/ofed/sbin:." MPISPAWN_GENERIC_NAME_24=SLURM_TACC_JOBNAME MPISPAWN_GENERIC_VALUE_24="stampedeLargeScaleAllTS.sh" MPISPAWN_GENERIC_NAME_25=SLURM_NODEID MPISPAWN_GENERIC_VALUE_25="0" MPISPAWN_GENERIC_NAME_26=LMOD_PKG MPISPAWN_GENERIC_VALUE_26="/opt/apps/lmod/lmod" MPISPAWN_GENERIC_NAME_27=MIC_I_MPI_OFA_ADAPTER_NAME MPISPAWN_GENERIC_VALUE_27="mlx4_0" MPISPAWN_GENERIC_NAME_28=HISTSIZE MPISPAWN_GENERIC_VALUE_28="1000" MPISPAWN_GENERIC_NAME_29=SLURM_NTASKS MPISPAWN_GENERIC_VALUE_29="2048" MPISPAWN_GENERIC_NAME_30=SHLIB_PATH MPISPAWN_GENERIC_VALUE_30="/opt/apps/xsede/gsi-openssh-5.7/lib64" MPISPAWN_GENERIC_NAME_31=TACC_OVERRIDE_PROJECT MPISPAWN_GENERIC_VALUE_31="TG-ASC140020" MPISPAWN_GENERIC_NAME_32=_ModuleTable005_ MPISPAWN_GENERIC_VALUE_32="Z2NjNF83L21vZHVsZWZpbGVzIiwiL29wdC9hcHBzL3hzZWRlL21vZHVsZWZpbGVzIiwiL29wdC9hcHBzL21vZHVsZWZpbGVzIiwiL29wdC9tb2R1bGVmaWxlcyIsfSxbInN5c3RlbUJhc2VNUEFUSCJdPSIvb3B0L2FwcHMveHNlZGUvbW9kdWxlZmlsZXM6L29wdC9hcHBzL21vZHVsZWZpbGVzOi9vcHQvbW9kdWxlZmlsZXMiLFsidmVyc2lvbiJdPTIsfQ==" MPISPAWN_GENERIC_NAME_33=_ModuleTable_Sz_ MPISPAWN_GENERIC_VALUE_33="5" MPISPAWN_GENERIC_NAME_34=MAIL MPISPAWN_GENERIC_VALUE_34="/var/spool/mail/tg825207" MPISPAWN_GENERIC_NAME_35=MV2_USE_HUGEPAGES MPISPAWN_GENERIC_VALUE_35="0" MPISPAWN_GENERIC_NAME_36=SLURM_TOPOLOGY_ADDR MPISPAWN_GENERIC_VALUE_36="c468-601" MPISPAWN_GENERIC_NAME_37=TACC_XALT_DIR MPISPAWN_GENERIC_VALUE_37="/opt/apps/xalt/0.4.4/" MPISPAWN_GENERIC_NAME_38=QTDIR MPISPAWN_GENERIC_VALUE_38="/usr/lib64/qt-3.3" MPISPAWN_GENERIC_NAME_39=SYSTEM MPISPAWN_GENERIC_VALUE_39="linux" MPISPAWN_GENERIC_NAME_40=SLURM_TACC_RUNLIMIT_MINS MPISPAWN_GENERIC_VALUE_40="180" MPISPAWN_GENERIC_NAME_41=SLURM_SUBMIT_DIR MPISPAWN_GENERIC_VALUE_41="/home1/03222/tg825207/parallelnbody" MPISPAWN_GENERIC_NAME_42=LIBPATH MPISPAWN_GENERIC_VALUE_42="/opt/apps/xsede/gsi-openssh-5.7/lib64:/usr/lib:/lib" MPISPAWN_GENERIC_NAME_43=HOSTNAME MPISPAWN_GENERIC_VALUE_43="c468-601" MPISPAWN_GENERIC_NAME_44=LMOD_DEFAULT_MODULEPATH MPISPAWN_GENERIC_VALUE_44="/opt/apps/xsede/modulefiles:/opt/apps/modulefiles:/opt/modulefiles" MPISPAWN_GENERIC_NAME_45=ARCHIVE MPISPAWN_GENERIC_VALUE_45="/home/03222/tg825207" MPISPAWN_GENERIC_NAME_46=_ MPISPAWN_GENERIC_VALUE_46="/usr/local/bin/build_env.pl" MPISPAWN_GENERIC_NAME_47=__BASHRC_SOURCED__ MPISPAWN_GENERIC_VALUE_47="1" MPISPAWN_GENERIC_NAME_48=APPS MPISPAWN_GENERIC_VALUE_48="/opt/apps" MPISPAWN_GENERIC_NAME_49=SHELL MPISPAWN_GENERIC_VALUE_49="/bin/bash" MPISPAWN_GENERIC_NAME_50=ARMCI_OPENIB_DEVICE MPISPAWN_GENERIC_VALUE_50="mlx4_0" MPISPAWN_GENERIC_NAME_51=ENVIRONMENT MPISPAWN_GENERIC_VALUE_51="BATCH" MPISPAWN_GENERIC_NAME_52=TACC_FAMILY_MPI MPISPAWN_GENERIC_VALUE_52="mvapich2" MPISPAWN_GENERIC_NAME_53=MV2_IBA_HCA MPISPAWN_GENERIC_VALUE_53="mlx4_0" MPISPAWN_GENERIC_NAME_54=TACC_PUBLIC_MACHINE MPISPAWN_GENERIC_VALUE_54="1" MPISPAWN_GENERIC_NAME_55=SLURM_TACC_NODES MPISPAWN_GENERIC_VALUE_55="1" MPISPAWN_GENERIC_NAME_56=_ModuleTable003_ MPISPAWN_GENERIC_VALUE_56="e30sWyJzaG9ydCJdPSJjbHVzdGVyIixbInN0YXR1cyJdPSJhY3RpdmUiLH0sWyJjbHVzdGVyLXBhdGhzIl09e1siRk4iXT0iL29wdC9tb2R1bGVmaWxlcy9jbHVzdGVyLXBhdGhzIixbImRlZmF1bHQiXT0wLFsiZnVsbE5hbWUiXT0iY2x1c3Rlci1wYXRocyIsWyJsb2FkT3JkZXIiXT0zLHByb3BUPXt9LFsic2hvcnQiXT0iY2x1c3Rlci1wYXRocyIsWyJzdGF0dXMiXT0iYWN0aXZlIix9LGdjYz17WyJGTiJdPSIvb3B0L2FwcHMvbW9kdWxlZmlsZXMvZ2NjLzQuNy4xLmx1YSIsWyJkZWZhdWx0Il09MCxbImZ1bGxOYW1lIl09ImdjYy80LjcuMSIsWyJsb2FkT3JkZXIiXT03LHByb3BUPXt9LFsic2hvcnQiXT0iZ2NjIixbInN0YXR1cyJdPSJhY3RpdmUiLH0sbXZhcGljaDI9e1si" MPISPAWN_GENERIC_NAME_57=QTINC MPISPAWN_GENERIC_VALUE_57="/usr/lib64/qt-3.3/include" MPISPAWN_GENERIC_NAME_58=CVS_RSH MPISPAWN_GENERIC_VALUE_58="ssh" MPISPAWN_GENERIC_NAME_59=MV2_USE_OLD_BCAST MPISPAWN_GENERIC_VALUE_59="1" MPISPAWN_GENERIC_NAME_60=SLURM_JOB_NUM_NODES MPISPAWN_GENERIC_VALUE_60="128" MPISPAWN_GENERIC_NAME_61=SLURM_TASK_PID MPISPAWN_GENERIC_VALUE_61="31479" MPISPAWN_GENERIC_NAME_62=TACC_MPI_GETMODE MPISPAWN_GENERIC_VALUE_62="mvapich2_ssh" MPISPAWN_GENERIC_NAME_63=LANG MPISPAWN_GENERIC_VALUE_63="en_US.UTF-8" MPISPAWN_GENERIC_NAME_64=USER MPISPAWN_GENERIC_VALUE_64="tg825207" MPISPAWN_GENERIC_NAME_65=MIC_I_MPI_DAPL_PROVIDER MPISPAWN_GENERIC_VALUE_65="ofa-v2-mlx4_0-1u" MPISPAWN_GENERIC_NAME_66=G_BROKEN_FILENAMES MPISPAWN_GENERIC_VALUE_66="1" MPISPAWN_GENERIC_NAME_67=SLURM_LOCALID MPISPAWN_GENERIC_VALUE_67="0" MPISPAWN_GENERIC_NAME_68=SLURM_NPROCS MPISPAWN_GENERIC_VALUE_68="2048" MPISPAWN_GENERIC_NAME_69=SLURM_TACC_NNODES_SET MPISPAWN_GENERIC_VALUE_69="0" MPISPAWN_GENERIC_NAME_70=SLURM_TACC_CORES MPISPAWN_GENERIC_VALUE_70="2048" MPISPAWN_GENERIC_NAME_71=LMOD_DIR MPISPAWN_GENERIC_VALUE_71="/opt/apps/lmod/lmod/libexec" MPISPAWN_GENERIC_NAME_72=LMOD_SETTARG_CMD MPISPAWN_GENERIC_VALUE_72=":" MPISPAWN_GENERIC_NAME_73=OMP_NUM_THREADS MPISPAWN_GENERIC_VALUE_73="1" MPISPAWN_GENERIC_NAME_74=_ModuleTable004_ MPISPAWN_GENERIC_VALUE_74="Rk4iXT0iL29wdC9hcHBzL2djYzRfNy9tb2R1bGVmaWxlcy9tdmFwaWNoMi8xLjlhMiIsWyJkZWZhdWx0Il09MSxbImZ1bGxOYW1lIl09Im12YXBpY2gyLzEuOWEyIixbImxvYWRPcmRlciJdPTgscHJvcFQ9e30sWyJzaG9ydCJdPSJtdmFwaWNoMiIsWyJzdGF0dXMiXT0iYWN0aXZlIix9LHhhbHQ9e1siRk4iXT0iL29wdC9hcHBzL21vZHVsZWZpbGVzL3hhbHQvMC40LjQubHVhIixbImRlZmF1bHQiXT0xLFsiZnVsbE5hbWUiXT0ieGFsdC8wLjQuNCIsWyJsb2FkT3JkZXIiXT00LHByb3BUPXt9LFsic2hvcnQiXT0ieGFsdCIsWyJzdGF0dXMiXT0iYWN0aXZlIix9LH0sbXBhdGhBPXsiL29wdC9hcHBzL2djYzRfNy9tdmFwaWNoMl8xXzkvbW9kdWxlZmlsZXMiLCIvb3B0L2FwcHMv" MPISPAWN_GENERIC_NAME_75=__PERSONAL_PATH__ MPISPAWN_GENERIC_VALUE_75="1" MPISPAWN_GENERIC_NAME_76=LMOD_SYSTEM_DEFAULT_MODULES MPISPAWN_GENERIC_VALUE_76="TACC" MPISPAWN_GENERIC_NAME_77=SLURM_JOB_NAME MPISPAWN_GENERIC_VALUE_77="stampedeLargeScaleAllTS.sh" MPISPAWN_GENERIC_NAME_78=SLURM_SUBMIT_HOST MPISPAWN_GENERIC_VALUE_78="login4.stampede.tacc.utexas.edu" MPISPAWN_GENERIC_NAME_79=LD_LIBRARY_PATH MPISPAWN_GENERIC_VALUE_79="/opt/apps/gcc4_7/mvapich2/1.9/lib:/opt/apps/gcc4_7/mvapich2/1.9/lib/shared:/opt/apps/gcc/4.7.1/lib64:/opt/apps/gcc/4.7.1/lib:/opt/apps/xsede/gsi-openssh-5.7/lib64:/opt/apps/xsede/gsi-openssh-5.7/lib64" MPISPAWN_GENERIC_NAME_80=SLURM_QUEUE MPISPAWN_GENERIC_VALUE_80="normal" MPISPAWN_GENERIC_NAME_81=PKG_CONFIG_PATH MPISPAWN_GENERIC_VALUE_81="/opt/apps/gcc4_7/mvapich2/1.9/lib/pkgconfig" MPISPAWN_GENERIC_NAME_82=LMOD_PREPEND_BLOCK MPISPAWN_GENERIC_VALUE_82="normal" MPISPAWN_GENERIC_NAME_83=TACC_FAMILY_COMPILER MPISPAWN_GENERIC_VALUE_83="gcc" MPISPAWN_GENERIC_NAME_84=TACC_DOMAIN MPISPAWN_GENERIC_VALUE_84="stampede" MPISPAWN_GENERIC_NAME_85=MV2_HOMOGENEOUS_CLUSTER MPISPAWN_GENERIC_VALUE_85="1" MPISPAWN_GENERIC_NAME_86=LMOD_FAMILY_COMPILER MPISPAWN_GENERIC_VALUE_86="gcc" MPISPAWN_GENERIC_NAME_87=MV2_USE_UD_HYBRID MPISPAWN_GENERIC_VALUE_87="0" MPISPAWN_GENERIC_NAME_88=MPICH_HOME MPISPAWN_GENERIC_VALUE_88="/opt/apps/gcc4_7/mvapich2/1.9" MPISPAWN_GENERIC_NAME_89=SLURM_JOB_NODELIST MPISPAWN_GENERIC_VALUE_89="c468-601,c471-503,c472-[401-402],c473-[303-304,401,804,901],c474-[203-204,301,401,403-404],c478-303,c482-404,c483-[001-004,101,504,601,804,901],c484-[101,103-104,201,602,701],c485-502,c486-102,c490-[302,904],c491-004,c492-701,c493-[001,102,201],c494-[101,402-403,602,702-703,904],c495-[001-002,201],c497-[102,202,504,601-603],c498-[304,402,603],c501-[204,301,604,801],c502-104,c503-901,c504-[602,604,702,801],c505-[001,202-204,401,502-504,802,901-904],c506-[302,704],c507-[102,202,503,601,904],c508-[101,204,601,702-703,802,903-904],c509-[002,201,301,303,401,501-502],c510-[003,401,404,501,503],c511-[601-602,902-903],c536-[801,902],c537-902,c538-[001,003,104],c540-[104,201-202,804,901-904]" MPISPAWN_GENERIC_NAME_90=SLURM_PROCID MPISPAWN_GENERIC_VALUE_90="0" MPISPAWN_GENERIC_NAME_91=I_MPI_DAPL_PROVIDER MPISPAWN_GENERIC_VALUE_91="ofa-v2-mlx4_0-1u" MPISPAWN_GENERIC_NAME_92=GLOBUS_LOCATION MPISPAWN_GENERIC_VALUE_92="/opt/apps/xsede/gsi-openssh-5.7" MPISPAWN_GENERIC_NAME_93=LMOD_COLORIZE MPISPAWN_GENERIC_VALUE_93="yes" MPISPAWN_GENERIC_NAME_94=I_MPI_HYDRA_BRANCH_COUNT MPISPAWN_GENERIC_VALUE_94="6000" MPISPAWN_GENERIC_NAME_95=SLURM_NNODES MPISPAWN_GENERIC_VALUE_95="128" MPISPAWN_GENERIC_NAME_96=MV2_DEFAULT_TIME_OUT MPISPAWN_GENERIC_VALUE_96="23" MPISPAWN_GENERIC_NAME_97=SLURM_TACC_NCORES_SET MPISPAWN_GENERIC_VALUE_97="1" MPISPAWN_GENERIC_NAME_98=__Init_Default_Modules MPISPAWN_GENERIC_VALUE_98="1" MPISPAWN_GENERIC_NAME_99=BASH_ENV MPISPAWN_GENERIC_VALUE_99="/etc/tacc/tacc_functions" MPISPAWN_GENERIC_NAME_100=SLURM_TOPOLOGY_ADDR_PATTERN MPISPAWN_GENERIC_VALUE_100="node" MPISPAWN_GENERIC_NAME_101=MV2_USE_RING_STARTUP MPISPAWN_GENERIC_VALUE_101="0" MPISPAWN_GENERIC_NAME_102=TACC_XALT_BIN MPISPAWN_GENERIC_VALUE_102="/opt/apps/xalt/0.4.4/bin" MPISPAWN_GENERIC_NAME_103=X509_USER_PROXY MPISPAWN_GENERIC_VALUE_103="/tmp/x509up_p3084.file9zM3fu.1" MPISPAWN_GENERIC_NAME_104=TMPDIR MPISPAWN_GENERIC_VALUE_104="/tmp" MPISPAWN_GENERIC_NAME_105=LMOD_FULL_SETTARG_SUPPORT MPISPAWN_GENERIC_VALUE_105="no" MPISPAWN_ID=41 MPISPAWN_WORKING_DIR=/home1/03222/tg825207/parallelnbody MPISPAWN_MPIRUN_RANK_0=656 MPISPAWN_MPIRUN_RANK_1=657 MPISPAWN_MPIRUN_RANK_2=658 MPISPAWN_MPIRUN_RANK_3=659 MPISPAWN_MPIRUN_RANK_4=660 MPISPAWN_MPIRUN_RANK_5=661 MPISPAWN_MPIRUN_RANK_6=662 MPISPAWN_MPIRUN_RANK_7=663 MPISPAWN_MPIRUN_RANK_8=664 MPISPAWN_MPIRUN_RANK_9=665 MPISPAWN_MPIRUN_RANK_10=666 MPISPAWN_MPIRUN_RANK_11=667 MPISPAWN_MPIRUN_RANK_12=668 MPISPAWN_MPIRUN_RANK_13=669 MPISPAWN_MPIRUN_RANK_14=670 MPISPAWN_MPIRUN_RANK_15=671 /opt/apps/gcc4_7/mvapich2/1.9/bin/mpispawn 0
[c497-504.stampede.tacc.utexas.edu:mpispawn_53][error_sighandler] Caught error: Segmentation fault (signal 11)
[c501-204.stampede.tacc.utexas.edu:mpispawn_60][error_sighandler] Caught error: Segmentation fault (signal 11)
bash: line 1: 37828 Segmentation fault      /usr/bin/env LD_LIBRARY_PATH=/opt/apps/gcc4_7/mvapich2/1.9/lib:/opt/apps/gcc4_7/mvapich2/1.9/lib/shared:/opt/apps/gcc/4.7.1/lib64:/opt/apps/gcc/4.7.1/lib:/opt/apps/xsede/gsi-openssh-5.7/lib64:/opt/apps/xsede/gsi-openssh-5.7/lib64 MPISPAWN_MPIRUN_MPD=0 USE_LINEAR_SSH=1 MPISPAWN_MPIRUN_HOST=c468-601.stampede.tacc.utexas.edu MPISPAWN_MPIRUN_HOSTIP=129.114.76.165 MPIRUN_RSH_LAUNCH=1 MPISPAWN_CHECKIN_PORT=36101 MPISPAWN_MPIRUN_PORT=36101 MPISPAWN_NNODES=64 MPISPAWN_GLOBAL_NPROCS=1024 MPISPAWN_MPIRUN_ID=44295 MPISPAWN_ARGC=4 MPDMAN_KVS_TEMPLATE=kvs_513_c468-601.stampede.tacc.utexas.edu_44295 MPISPAWN_LOCAL_NPROCS=16 MPISPAWN_ARGV_0='/home1/03222/tg825207/parallelnbody/teamscatter' MPISPAWN_ARGV_1='256000' MPISPAWN_ARGV_2='-c' MPISPAWN_ARGV_3='2' MPISPAWN_ARGC=4 MPISPAWN_GENERIC_ENV_COUNT=106 MPISPAWN_GENERIC_NAME_0=TACC_ENVLEN MPISPAWN_GENERIC_VALUE_0=6578 MPISPAWN_GENERIC_NAME_1=SLURM_JOB_ID MPISPAWN_GENERIC_VALUE_1="4306264" MPISPAWN_GENERIC_NAME_2=SLURMD_NODENAME MPISPAWN_GENERIC_VALUE_2="c468-601" MPISPAWN_GENERIC_NAME_3=STOCKYARD MPISPAWN_GENERIC_VALUE_3="/work/03222/tg825207" MPISPAWN_GENERIC_NAME_4=LC_ALL MPISPAWN_GENERIC_VALUE_4="en_US.UTF-8" MPISPAWN_GENERIC_NAME_5=SLURM_CHECKPOINT_IMAGE_DIR MPISPAWN_GENERIC_VALUE_5="/home1/03222/tg825207/parallelnbody" MPISPAWN_GENERIC_NAME_6=LMOD_FAMILY_MPI MPISPAWN_GENERIC_VALUE_6="mvapich2" MPISPAWN_GENERIC_NAME_7=SLURM_JOBID MPISPAWN_GENERIC_VALUE_7="4306264" MPISPAWN_GENERIC_NAME_8=_ModuleTable001_ MPISPAWN_GENERIC_VALUE_8="X01vZHVsZVRhYmxlXz17WyJhY3RpdmVTaXplIl09OCxiYXNlTXBhdGhBPXsiL29wdC9hcHBzL3hzZWRlL21vZHVsZWZpbGVzIiwiL29wdC9hcHBzL21vZHVsZWZpbGVzIiwiL29wdC9tb2R1bGVmaWxlcyIsfSxbImNfcmVidWlsZFRpbWUiXT1mYWxzZSxbImNfc2hvcnRUaW1lIl09ZmFsc2UsZmFtaWx5PXtbIk1QSSJdPSJtdmFwaWNoMiIsWyJjb21waWxlciJdPSJnY2MiLH0saW5hY3RpdmU9e30sbVQ9e0xpbnV4PXtbIkZOIl09Ii9vcHQvbW9kdWxlZmlsZXMvTGludXgiLFsiZGVmYXVsdCJdPTAsWyJmdWxsTmFtZSJdPSJMaW51eCIsWyJsb2FkT3JkZXIiXT0yLHByb3BUPXt9LFsic2hvcnQiXT0iTGludXgiLFsic3RhdHVzIl09ImFjdGl2ZSIsfSxUQUNDPXtbIkZOIl09Ii9v" MPISPAWN_GENERIC_NAME_9=TACC_SYSTEM MPISPAWN_GENERIC_VALUE_9="stampede" MPISPAWN_GENERIC_NAME_10=WORK MPISPAWN_GENERIC_VALUE_10="/work/03222/tg825207" MPISPAWN_GENERIC_NAME_11=SLURM_GTIDS MPISPAWN_GENERIC_VALUE_11="0" MPISPAWN_GENERIC_NAME_12=GCC_LIB MPISPAWN_GENERIC_VALUE_12="/opt/apps/gcc/4.7.1/lib64" MPISPAWN_GENERIC_NAME_13=SLURM_PRIO_PROCESS MPISPAWN_GENERIC_VALUE_13="0" MPISPAWN_GENERIC_NAME_14=SCRATCH MPISPAWN_GENERIC_VALUE_14="/scratch/03222/tg825207" MPISPAWN_GENERIC_NAME_15=LOGNAME MPISPAWN_GENERIC_VALUE_15="tg825207" MPISPAWN_GENERIC_NAME_16=_ModuleTable002_ MPISPAWN_GENERIC_VALUE_16="cHQvbW9kdWxlZmlsZXMvVEFDQyIsWyJkZWZhdWx0Il09MCxbImZ1bGxOYW1lIl09IlRBQ0MiLFsibG9hZE9yZGVyIl09Nixwcm9wVD17fSxbInNob3J0Il09IlRBQ0MiLFsic3RhdHVzIl09ImFjdGl2ZSIsfSxbIlRBQ0MtcGF0aHMiXT17WyJGTiJdPSIvb3B0L21vZHVsZWZpbGVzL1RBQ0MtcGF0aHMiLFsiZGVmYXVsdCJdPTAsWyJmdWxsTmFtZSJdPSJUQUNDLXBhdGhzIixbImxvYWRPcmRlciJdPTEscHJvcFQ9e30sWyJzaG9ydCJdPSJUQUNDLXBhdGhzIixbInN0YXR1cyJdPSJhY3RpdmUiLH0sY2x1c3Rlcj17WyJGTiJdPSIvb3B0L21vZHVsZWZpbGVzL2NsdXN0ZXIiLFsiZGVmYXVsdCJdPTAsWyJmdWxsTmFtZSJdPSJjbHVzdGVyIixbImxvYWRPcmRlciJdPTUscHJvcFQ9" MPISPAWN_GENERIC_NAME_17=INPUTRC MPISPAWN_GENERIC_VALUE_17="/etc/inputrc" MPISPAWN_GENERIC_NAME_18=LMOD_CMD MPISPAWN_GENERIC_VALUE_18="/opt/apps/lmod/lmod/libexec/lmod" MPISPAWN_GENERIC_NAME_19=I_MPI_OFA_ADAPTER_NAME MPISPAWN_GENERIC_VALUE_19="mlx4_0" MPISPAWN_GENERIC_NAME_20=QTLIB MPISPAWN_GENERIC_VALUE_20="/usr/lib64/qt-3.3/lib" MPISPAWN_GENERIC_NAME_21=NODE_TASKS_PPN_INFO MPISPAWN_GENERIC_VALUE_21=""16,0_"" MPISPAWN_GENERIC_NAME_22=SLURM_CPUS_ON_NODE MPISPAWN_GENERIC_VALUE_22="16" MPISPAWN_GENERIC_NAME_23=PATH MPISPAWN_GENERIC_VALUE_23="/opt/apps/gcc4_7/mvapich2/1.9/bin:/opt/apps/gcc/4.7.1/bin:/opt/apps/xalt/0.4.4/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/opt/apps/xsede/gsi-openssh-5.7/bin:/usr/X11R6/bin:/opt/ofed/bin:/opt/ofed/sbin:." MPISPAWN_GENERIC_NAME_24=SLURM_TACC_JOBNAME MPISPAWN_GENERIC_VALUE_24="stampedeLargeScaleAllTS.sh" MPISPAWN_GENERIC_NAME_25=SLURM_NODEID MPISPAWN_GENERIC_VALUE_25="0" MPISPAWN_GENERIC_NAME_26=LMOD_PKG MPISPAWN_GENERIC_VALUE_26="/opt/apps/lmod/lmod" MPISPAWN_GENERIC_NAME_27=MIC_I_MPI_OFA_ADAPTER_NAME MPISPAWN_GENERIC_VALUE_27="mlx4_0" MPISPAWN_GENERIC_NAME_28=HISTSIZE MPISPAWN_GENERIC_VALUE_28="1000" MPISPAWN_GENERIC_NAME_29=SLURM_NTASKS MPISPAWN_GENERIC_VALUE_29="2048" MPISPAWN_GENERIC_NAME_30=SHLIB_PATH MPISPAWN_GENERIC_VALUE_30="/opt/apps/xsede/gsi-openssh-5.7/lib64" MPISPAWN_GENERIC_NAME_31=TACC_OVERRIDE_PROJECT MPISPAWN_GENERIC_VALUE_31="TG-ASC140020" MPISPAWN_GENERIC_NAME_32=_ModuleTable005_ MPISPAWN_GENERIC_VALUE_32="Z2NjNF83L21vZHVsZWZpbGVzIiwiL29wdC9hcHBzL3hzZWRlL21vZHVsZWZpbGVzIiwiL29wdC9hcHBzL21vZHVsZWZpbGVzIiwiL29wdC9tb2R1bGVmaWxlcyIsfSxbInN5c3RlbUJhc2VNUEFUSCJdPSIvb3B0L2FwcHMveHNlZGUvbW9kdWxlZmlsZXM6L29wdC9hcHBzL21vZHVsZWZpbGVzOi9vcHQvbW9kdWxlZmlsZXMiLFsidmVyc2lvbiJdPTIsfQ==" MPISPAWN_GENERIC_NAME_33=_ModuleTable_Sz_ MPISPAWN_GENERIC_VALUE_33="5" MPISPAWN_GENERIC_NAME_34=MAIL MPISPAWN_GENERIC_VALUE_34="/var/spool/mail/tg825207" MPISPAWN_GENERIC_NAME_35=MV2_USE_HUGEPAGES MPISPAWN_GENERIC_VALUE_35="0" MPISPAWN_GENERIC_NAME_36=SLURM_TOPOLOGY_ADDR MPISPAWN_GENERIC_VALUE_36="c468-601" MPISPAWN_GENERIC_NAME_37=TACC_XALT_DIR MPISPAWN_GENERIC_VALUE_37="/opt/apps/xalt/0.4.4/" MPISPAWN_GENERIC_NAME_38=QTDIR MPISPAWN_GENERIC_VALUE_38="/usr/lib64/qt-3.3" MPISPAWN_GENERIC_NAME_39=SYSTEM MPISPAWN_GENERIC_VALUE_39="linux" MPISPAWN_GENERIC_NAME_40=SLURM_TACC_RUNLIMIT_MINS MPISPAWN_GENERIC_VALUE_40="180" MPISPAWN_GENERIC_NAME_41=SLURM_SUBMIT_DIR MPISPAWN_GENERIC_VALUE_41="/home1/03222/tg825207/parallelnbody" MPISPAWN_GENERIC_NAME_42=LIBPATH MPISPAWN_GENERIC_VALUE_42="/opt/apps/xsede/gsi-openssh-5.7/lib64:/usr/lib:/lib" MPISPAWN_GENERIC_NAME_43=HOSTNAME MPISPAWN_GENERIC_VALUE_43="c468-601" MPISPAWN_GENERIC_NAME_44=LMOD_DEFAULT_MODULEPATH MPISPAWN_GENERIC_VALUE_44="/opt/apps/xsede/modulefiles:/opt/apps/modulefiles:/opt/modulefiles" MPISPAWN_GENERIC_NAME_45=ARCHIVE MPISPAWN_GENERIC_VALUE_45="/home/03222/tg825207" MPISPAWN_GENERIC_NAME_46=_ MPISPAWN_GENERIC_VALUE_46="/usr/local/bin/build_env.pl" MPISPAWN_GENERIC_NAME_47=__BASHRC_SOURCED__ MPISPAWN_GENERIC_VALUE_47="1" MPISPAWN_GENERIC_NAME_48=APPS MPISPAWN_GENERIC_VALUE_48="/opt/apps" MPISPAWN_GENERIC_NAME_49=SHELL MPISPAWN_GENERIC_VALUE_49="/bin/bash" MPISPAWN_GENERIC_NAME_50=ARMCI_OPENIB_DEVICE MPISPAWN_GENERIC_VALUE_50="mlx4_0" MPISPAWN_GENERIC_NAME_51=ENVIRONMENT MPISPAWN_GENERIC_VALUE_51="BATCH" MPISPAWN_GENERIC_NAME_52=TACC_FAMILY_MPI MPISPAWN_GENERIC_VALUE_52="mvapich2" MPISPAWN_GENERIC_NAME_53=MV2_IBA_HCA MPISPAWN_GENERIC_VALUE_53="mlx4_0" MPISPAWN_GENERIC_NAME_54=TACC_PUBLIC_MACHINE MPISPAWN_GENERIC_VALUE_54="1" MPISPAWN_GENERIC_NAME_55=SLURM_TACC_NODES MPISPAWN_GENERIC_VALUE_55="1" MPISPAWN_GENERIC_NAME_56=_ModuleTable003_ MPISPAWN_GENERIC_VALUE_56="e30sWyJzaG9ydCJdPSJjbHVzdGVyIixbInN0YXR1cyJdPSJhY3RpdmUiLH0sWyJjbHVzdGVyLXBhdGhzIl09e1siRk4iXT0iL29wdC9tb2R1bGVmaWxlcy9jbHVzdGVyLXBhdGhzIixbImRlZmF1bHQiXT0wLFsiZnVsbE5hbWUiXT0iY2x1c3Rlci1wYXRocyIsWyJsb2FkT3JkZXIiXT0zLHByb3BUPXt9LFsic2hvcnQiXT0iY2x1c3Rlci1wYXRocyIsWyJzdGF0dXMiXT0iYWN0aXZlIix9LGdjYz17WyJGTiJdPSIvb3B0L2FwcHMvbW9kdWxlZmlsZXMvZ2NjLzQuNy4xLmx1YSIsWyJkZWZhdWx0Il09MCxbImZ1bGxOYW1lIl09ImdjYy80LjcuMSIsWyJsb2FkT3JkZXIiXT03LHByb3BUPXt9LFsic2hvcnQiXT0iZ2NjIixbInN0YXR1cyJdPSJhY3RpdmUiLH0sbXZhcGljaDI9e1si" MPISPAWN_GENERIC_NAME_57=QTINC MPISPAWN_GENERIC_VALUE_57="/usr/lib64/qt-3.3/include" MPISPAWN_GENERIC_NAME_58=CVS_RSH MPISPAWN_GENERIC_VALUE_58="ssh" MPISPAWN_GENERIC_NAME_59=MV2_USE_OLD_BCAST MPISPAWN_GENERIC_VALUE_59="1" MPISPAWN_GENERIC_NAME_60=SLURM_JOB_NUM_NODES MPISPAWN_GENERIC_VALUE_60="128" MPISPAWN_GENERIC_NAME_61=SLURM_TASK_PID MPISPAWN_GENERIC_VALUE_61="31479" MPISPAWN_GENERIC_NAME_62=TACC_MPI_GETMODE MPISPAWN_GENERIC_VALUE_62="mvapich2_ssh" MPISPAWN_GENERIC_NAME_63=LANG MPISPAWN_GENERIC_VALUE_63="en_US.UTF-8" MPISPAWN_GENERIC_NAME_64=USER MPISPAWN_GENERIC_VALUE_64="tg825207" MPISPAWN_GENERIC_NAME_65=MIC_I_MPI_DAPL_PROVIDER MPISPAWN_GENERIC_VALUE_65="ofa-v2-mlx4_0-1u" MPISPAWN_GENERIC_NAME_66=G_BROKEN_FILENAMES MPISPAWN_GENERIC_VALUE_66="1" MPISPAWN_GENERIC_NAME_67=SLURM_LOCALID MPISPAWN_GENERIC_VALUE_67="0" MPISPAWN_GENERIC_NAME_68=SLURM_NPROCS MPISPAWN_GENERIC_VALUE_68="2048" MPISPAWN_GENERIC_NAME_69=SLURM_TACC_NNODES_SET MPISPAWN_GENERIC_VALUE_69="0" MPISPAWN_GENERIC_NAME_70=SLURM_TACC_CORES MPISPAWN_GENERIC_VALUE_70="2048" MPISPAWN_GENERIC_NAME_71=LMOD_DIR MPISPAWN_GENERIC_VALUE_71="/opt/apps/lmod/lmod/libexec" MPISPAWN_GENERIC_NAME_72=LMOD_SETTARG_CMD MPISPAWN_GENERIC_VALUE_72=":" MPISPAWN_GENERIC_NAME_73=OMP_NUM_THREADS MPISPAWN_GENERIC_VALUE_73="1" MPISPAWN_GENERIC_NAME_74=_ModuleTable004_ MPISPAWN_GENERIC_VALUE_74="Rk4iXT0iL29wdC9hcHBzL2djYzRfNy9tb2R1bGVmaWxlcy9tdmFwaWNoMi8xLjlhMiIsWyJkZWZhdWx0Il09MSxbImZ1bGxOYW1lIl09Im12YXBpY2gyLzEuOWEyIixbImxvYWRPcmRlciJdPTgscHJvcFQ9e30sWyJzaG9ydCJdPSJtdmFwaWNoMiIsWyJzdGF0dXMiXT0iYWN0aXZlIix9LHhhbHQ9e1siRk4iXT0iL29wdC9hcHBzL21vZHVsZWZpbGVzL3hhbHQvMC40LjQubHVhIixbImRlZmF1bHQiXT0xLFsiZnVsbE5hbWUiXT0ieGFsdC8wLjQuNCIsWyJsb2FkT3JkZXIiXT00LHByb3BUPXt9LFsic2hvcnQiXT0ieGFsdCIsWyJzdGF0dXMiXT0iYWN0aXZlIix9LH0sbXBhdGhBPXsiL29wdC9hcHBzL2djYzRfNy9tdmFwaWNoMl8xXzkvbW9kdWxlZmlsZXMiLCIvb3B0L2FwcHMv" MPISPAWN_GENERIC_NAME_75=__PERSONAL_PATH__ MPISPAWN_GENERIC_VALUE_75="1" MPISPAWN_GENERIC_NAME_76=LMOD_SYSTEM_DEFAULT_MODULES MPISPAWN_GENERIC_VALUE_76="TACC" MPISPAWN_GENERIC_NAME_77=SLURM_JOB_NAME MPISPAWN_GENERIC_VALUE_77="stampedeLargeScaleAllTS.sh" MPISPAWN_GENERIC_NAME_78=SLURM_SUBMIT_HOST MPISPAWN_GENERIC_VALUE_78="login4.stampede.tacc.utexas.edu" MPISPAWN_GENERIC_NAME_79=LD_LIBRARY_PATH MPISPAWN_GENERIC_VALUE_79="/opt/apps/gcc4_7/mvapich2/1.9/lib:/opt/apps/gcc4_7/mvapich2/1.9/lib/shared:/opt/apps/gcc/4.7.1/lib64:/opt/apps/gcc/4.7.1/lib:/opt/apps/xsede/gsi-openssh-5.7/lib64:/opt/apps/xsede/gsi-openssh-5.7/lib64" MPISPAWN_GENERIC_NAME_80=SLURM_QUEUE MPISPAWN_GENERIC_VALUE_80="normal" MPISPAWN_GENERIC_NAME_81=PKG_CONFIG_PATH MPISPAWN_GENERIC_VALUE_81="/opt/apps/gcc4_7/mvapich2/1.9/lib/pkgconfig" MPISPAWN_GENERIC_NAME_82=LMOD_PREPEND_BLOCK MPISPAWN_GENERIC_VALUE_82="normal" MPISPAWN_GENERIC_NAME_83=TACC_FAMILY_COMPILER MPISPAWN_GENERIC_VALUE_83="gcc" MPISPAWN_GENERIC_NAME_84=TACC_DOMAIN MPISPAWN_GENERIC_VALUE_84="stampede" MPISPAWN_GENERIC_NAME_85=MV2_HOMOGENEOUS_CLUSTER MPISPAWN_GENERIC_VALUE_85="1" MPISPAWN_GENERIC_NAME_86=LMOD_FAMILY_COMPILER MPISPAWN_GENERIC_VALUE_86="gcc" MPISPAWN_GENERIC_NAME_87=MV2_USE_UD_HYBRID MPISPAWN_GENERIC_VALUE_87="0" MPISPAWN_GENERIC_NAME_88=MPICH_HOME MPISPAWN_GENERIC_VALUE_88="/opt/apps/gcc4_7/mvapich2/1.9" MPISPAWN_GENERIC_NAME_89=SLURM_JOB_NODELIST MPISPAWN_GENERIC_VALUE_89="c468-601,c471-503,c472-[401-402],c473-[303-304,401,804,901],c474-[203-204,301,401,403-404],c478-303,c482-404,c483-[001-004,101,504,601,804,901],c484-[101,103-104,201,602,701],c485-502,c486-102,c490-[302,904],c491-004,c492-701,c493-[001,102,201],c494-[101,402-403,602,702-703,904],c495-[001-002,201],c497-[102,202,504,601-603],c498-[304,402,603],c501-[204,301,604,801],c502-104,c503-901,c504-[602,604,702,801],c505-[001,202-204,401,502-504,802,901-904],c506-[302,704],c507-[102,202,503,601,904],c508-[101,204,601,702-703,802,903-904],c509-[002,201,301,303,401,501-502],c510-[003,401,404,501,503],c511-[601-602,902-903],c536-[801,902],c537-902,c538-[001,003,104],c540-[104,201-202,804,901-904]" MPISPAWN_GENERIC_NAME_90=SLURM_PROCID MPISPAWN_GENERIC_VALUE_90="0" MPISPAWN_GENERIC_NAME_91=I_MPI_DAPL_PROVIDER MPISPAWN_GENERIC_VALUE_91="ofa-v2-mlx4_0-1u" MPISPAWN_GENERIC_NAME_92=GLOBUS_LOCATION MPISPAWN_GENERIC_VALUE_92="/opt/apps/xsede/gsi-openssh-5.7" MPISPAWN_GENERIC_NAME_93=LMOD_COLORIZE MPISPAWN_GENERIC_VALUE_93="yes" MPISPAWN_GENERIC_NAME_94=I_MPI_HYDRA_BRANCH_COUNT MPISPAWN_GENERIC_VALUE_94="6000" MPISPAWN_GENERIC_NAME_95=SLURM_NNODES MPISPAWN_GENERIC_VALUE_95="128" MPISPAWN_GENERIC_NAME_96=MV2_DEFAULT_TIME_OUT MPISPAWN_GENERIC_VALUE_96="23" MPISPAWN_GENERIC_NAME_97=SLURM_TACC_NCORES_SET MPISPAWN_GENERIC_VALUE_97="1" MPISPAWN_GENERIC_NAME_98=__Init_Default_Modules MPISPAWN_GENERIC_VALUE_98="1" MPISPAWN_GENERIC_NAME_99=BASH_ENV MPISPAWN_GENERIC_VALUE_99="/etc/tacc/tacc_functions" MPISPAWN_GENERIC_NAME_100=SLURM_TOPOLOGY_ADDR_PATTERN MPISPAWN_GENERIC_VALUE_100="node" MPISPAWN_GENERIC_NAME_101=MV2_USE_RING_STARTUP MPISPAWN_GENERIC_VALUE_101="0" MPISPAWN_GENERIC_NAME_102=TACC_XALT_BIN MPISPAWN_GENERIC_VALUE_102="/opt/apps/xalt/0.4.4/bin" MPISPAWN_GENERIC_NAME_103=X509_USER_PROXY MPISPAWN_GENERIC_VALUE_103="/tmp/x509up_p3084.file9zM3fu.1" MPISPAWN_GENERIC_NAME_104=TMPDIR MPISPAWN_GENERIC_VALUE_104="/tmp" MPISPAWN_GENERIC_NAME_105=LMOD_FULL_SETTARG_SUPPORT MPISPAWN_GENERIC_VALUE_105="no" MPISPAWN_ID=53 MPISPAWN_WORKING_DIR=/home1/03222/tg825207/parallelnbody MPISPAWN_MPIRUN_RANK_0=848 MPISPAWN_MPIRUN_RANK_1=849 MPISPAWN_MPIRUN_RANK_2=850 MPISPAWN_MPIRUN_RANK_3=851 MPISPAWN_MPIRUN_RANK_4=852 MPISPAWN_MPIRUN_RANK_5=853 MPISPAWN_MPIRUN_RANK_6=854 MPISPAWN_MPIRUN_RANK_7=855 MPISPAWN_MPIRUN_RANK_8=856 MPISPAWN_MPIRUN_RANK_9=857 MPISPAWN_MPIRUN_RANK_10=858 MPISPAWN_MPIRUN_RANK_11=859 MPISPAWN_MPIRUN_RANK_12=860 MPISPAWN_MPIRUN_RANK_13=861 MPISPAWN_MPIRUN_RANK_14=862 MPISPAWN_MPIRUN_RANK_15=863 /opt/apps/gcc4_7/mvapich2/1.9/bin/mpispawn 0
bash: line 1: 75660 Segmentation fault      /usr/bin/env LD_LIBRARY_PATH=/opt/apps/gcc4_7/mvapich2/1.9/lib:/opt/apps/gcc4_7/mvapich2/1.9/lib/shared:/opt/apps/gcc/4.7.1/lib64:/opt/apps/gcc/4.7.1/lib:/opt/apps/xsede/gsi-openssh-5.7/lib64:/opt/apps/xsede/gsi-openssh-5.7/lib64 MPISPAWN_MPIRUN_MPD=0 USE_LINEAR_SSH=1 MPISPAWN_MPIRUN_HOST=c468-601.stampede.tacc.utexas.edu MPISPAWN_MPIRUN_HOSTIP=129.114.76.165 MPIRUN_RSH_LAUNCH=1 MPISPAWN_CHECKIN_PORT=36101 MPISPAWN_MPIRUN_PORT=36101 MPISPAWN_NNODES=64 MPISPAWN_GLOBAL_NPROCS=1024 MPISPAWN_MPIRUN_ID=44295 MPISPAWN_ARGC=4 MPDMAN_KVS_TEMPLATE=kvs_513_c468-601.stampede.tacc.utexas.edu_44295 MPISPAWN_LOCAL_NPROCS=16 MPISPAWN_ARGV_0='/home1/03222/tg825207/parallelnbody/teamscatter' MPISPAWN_ARGV_1='256000' MPISPAWN_ARGV_2='-c' MPISPAWN_ARGV_3='2' MPISPAWN_ARGC=4 MPISPAWN_GENERIC_ENV_COUNT=106 MPISPAWN_GENERIC_NAME_0=TACC_ENVLEN MPISPAWN_GENERIC_VALUE_0=6578 MPISPAWN_GENERIC_NAME_1=SLURM_JOB_ID MPISPAWN_GENERIC_VALUE_1="4306264" MPISPAWN_GENERIC_NAME_2=SLURMD_NODENAME MPISPAWN_GENERIC_VALUE_2="c468-601" MPISPAWN_GENERIC_NAME_3=STOCKYARD MPISPAWN_GENERIC_VALUE_3="/work/03222/tg825207" MPISPAWN_GENERIC_NAME_4=LC_ALL MPISPAWN_GENERIC_VALUE_4="en_US.UTF-8" MPISPAWN_GENERIC_NAME_5=SLURM_CHECKPOINT_IMAGE_DIR MPISPAWN_GENERIC_VALUE_5="/home1/03222/tg825207/parallelnbody" MPISPAWN_GENERIC_NAME_6=LMOD_FAMILY_MPI MPISPAWN_GENERIC_VALUE_6="mvapich2" MPISPAWN_GENERIC_NAME_7=SLURM_JOBID MPISPAWN_GENERIC_VALUE_7="4306264" MPISPAWN_GENERIC_NAME_8=_ModuleTable001_ MPISPAWN_GENERIC_VALUE_8="X01vZHVsZVRhYmxlXz17WyJhY3RpdmVTaXplIl09OCxiYXNlTXBhdGhBPXsiL29wdC9hcHBzL3hzZWRlL21vZHVsZWZpbGVzIiwiL29wdC9hcHBzL21vZHVsZWZpbGVzIiwiL29wdC9tb2R1bGVmaWxlcyIsfSxbImNfcmVidWlsZFRpbWUiXT1mYWxzZSxbImNfc2hvcnRUaW1lIl09ZmFsc2UsZmFtaWx5PXtbIk1QSSJdPSJtdmFwaWNoMiIsWyJjb21waWxlciJdPSJnY2MiLH0saW5hY3RpdmU9e30sbVQ9e0xpbnV4PXtbIkZOIl09Ii9vcHQvbW9kdWxlZmlsZXMvTGludXgiLFsiZGVmYXVsdCJdPTAsWyJmdWxsTmFtZSJdPSJMaW51eCIsWyJsb2FkT3JkZXIiXT0yLHByb3BUPXt9LFsic2hvcnQiXT0iTGludXgiLFsic3RhdHVzIl09ImFjdGl2ZSIsfSxUQUNDPXtbIkZOIl09Ii9v" MPISPAWN_GENERIC_NAME_9=TACC_SYSTEM MPISPAWN_GENERIC_VALUE_9="stampede" MPISPAWN_GENERIC_NAME_10=WORK MPISPAWN_GENERIC_VALUE_10="/work/03222/tg825207" MPISPAWN_GENERIC_NAME_11=SLURM_GTIDS MPISPAWN_GENERIC_VALUE_11="0" MPISPAWN_GENERIC_NAME_12=GCC_LIB MPISPAWN_GENERIC_VALUE_12="/opt/apps/gcc/4.7.1/lib64" MPISPAWN_GENERIC_NAME_13=SLURM_PRIO_PROCESS MPISPAWN_GENERIC_VALUE_13="0" MPISPAWN_GENERIC_NAME_14=SCRATCH MPISPAWN_GENERIC_VALUE_14="/scratch/03222/tg825207" MPISPAWN_GENERIC_NAME_15=LOGNAME MPISPAWN_GENERIC_VALUE_15="tg825207" MPISPAWN_GENERIC_NAME_16=_ModuleTable002_ MPISPAWN_GENERIC_VALUE_16="cHQvbW9kdWxlZmlsZXMvVEFDQyIsWyJkZWZhdWx0Il09MCxbImZ1bGxOYW1lIl09IlRBQ0MiLFsibG9hZE9yZGVyIl09Nixwcm9wVD17fSxbInNob3J0Il09IlRBQ0MiLFsic3RhdHVzIl09ImFjdGl2ZSIsfSxbIlRBQ0MtcGF0aHMiXT17WyJGTiJdPSIvb3B0L21vZHVsZWZpbGVzL1RBQ0MtcGF0aHMiLFsiZGVmYXVsdCJdPTAsWyJmdWxsTmFtZSJdPSJUQUNDLXBhdGhzIixbImxvYWRPcmRlciJdPTEscHJvcFQ9e30sWyJzaG9ydCJdPSJUQUNDLXBhdGhzIixbInN0YXR1cyJdPSJhY3RpdmUiLH0sY2x1c3Rlcj17WyJGTiJdPSIvb3B0L21vZHVsZWZpbGVzL2NsdXN0ZXIiLFsiZGVmYXVsdCJdPTAsWyJmdWxsTmFtZSJdPSJjbHVzdGVyIixbImxvYWRPcmRlciJdPTUscHJvcFQ9" MPISPAWN_GENERIC_NAME_17=INPUTRC MPISPAWN_GENERIC_VALUE_17="/etc/inputrc" MPISPAWN_GENERIC_NAME_18=LMOD_CMD MPISPAWN_GENERIC_VALUE_18="/opt/apps/lmod/lmod/libexec/lmod" MPISPAWN_GENERIC_NAME_19=I_MPI_OFA_ADAPTER_NAME MPISPAWN_GENERIC_VALUE_19="mlx4_0" MPISPAWN_GENERIC_NAME_20=QTLIB MPISPAWN_GENERIC_VALUE_20="/usr/lib64/qt-3.3/lib" MPISPAWN_GENERIC_NAME_21=NODE_TASKS_PPN_INFO MPISPAWN_GENERIC_VALUE_21=""16,0_"" MPISPAWN_GENERIC_NAME_22=SLURM_CPUS_ON_NODE MPISPAWN_GENERIC_VALUE_22="16" MPISPAWN_GENERIC_NAME_23=PATH MPISPAWN_GENERIC_VALUE_23="/opt/apps/gcc4_7/mvapich2/1.9/bin:/opt/apps/gcc/4.7.1/bin:/opt/apps/xalt/0.4.4/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/opt/apps/xsede/gsi-openssh-5.7/bin:/usr/X11R6/bin:/opt/ofed/bin:/opt/ofed/sbin:." MPISPAWN_GENERIC_NAME_24=SLURM_TACC_JOBNAME MPISPAWN_GENERIC_VALUE_24="stampedeLargeScaleAllTS.sh" MPISPAWN_GENERIC_NAME_25=SLURM_NODEID MPISPAWN_GENERIC_VALUE_25="0" MPISPAWN_GENERIC_NAME_26=LMOD_PKG MPISPAWN_GENERIC_VALUE_26="/opt/apps/lmod/lmod" MPISPAWN_GENERIC_NAME_27=MIC_I_MPI_OFA_ADAPTER_NAME MPISPAWN_GENERIC_VALUE_27="mlx4_0" MPISPAWN_GENERIC_NAME_28=HISTSIZE MPISPAWN_GENERIC_VALUE_28="1000" MPISPAWN_GENERIC_NAME_29=SLURM_NTASKS MPISPAWN_GENERIC_VALUE_29="2048" MPISPAWN_GENERIC_NAME_30=SHLIB_PATH MPISPAWN_GENERIC_VALUE_30="/opt/apps/xsede/gsi-openssh-5.7/lib64" MPISPAWN_GENERIC_NAME_31=TACC_OVERRIDE_PROJECT MPISPAWN_GENERIC_VALUE_31="TG-ASC140020" MPISPAWN_GENERIC_NAME_32=_ModuleTable005_ MPISPAWN_GENERIC_VALUE_32="Z2NjNF83L21vZHVsZWZpbGVzIiwiL29wdC9hcHBzL3hzZWRlL21vZHVsZWZpbGVzIiwiL29wdC9hcHBzL21vZHVsZWZpbGVzIiwiL29wdC9tb2R1bGVmaWxlcyIsfSxbInN5c3RlbUJhc2VNUEFUSCJdPSIvb3B0L2FwcHMveHNlZGUvbW9kdWxlZmlsZXM6L29wdC9hcHBzL21vZHVsZWZpbGVzOi9vcHQvbW9kdWxlZmlsZXMiLFsidmVyc2lvbiJdPTIsfQ==" MPISPAWN_GENERIC_NAME_33=_ModuleTable_Sz_ MPISPAWN_GENERIC_VALUE_33="5" MPISPAWN_GENERIC_NAME_34=MAIL MPISPAWN_GENERIC_VALUE_34="/var/spool/mail/tg825207" MPISPAWN_GENERIC_NAME_35=MV2_USE_HUGEPAGES MPISPAWN_GENERIC_VALUE_35="0" MPISPAWN_GENERIC_NAME_36=SLURM_TOPOLOGY_ADDR MPISPAWN_GENERIC_VALUE_36="c468-601" MPISPAWN_GENERIC_NAME_37=TACC_XALT_DIR MPISPAWN_GENERIC_VALUE_37="/opt/apps/xalt/0.4.4/" MPISPAWN_GENERIC_NAME_38=QTDIR MPISPAWN_GENERIC_VALUE_38="/usr/lib64/qt-3.3" MPISPAWN_GENERIC_NAME_39=SYSTEM MPISPAWN_GENERIC_VALUE_39="linux" MPISPAWN_GENERIC_NAME_40=SLURM_TACC_RUNLIMIT_MINS MPISPAWN_GENERIC_VALUE_40="180" MPISPAWN_GENERIC_NAME_41=SLURM_SUBMIT_DIR MPISPAWN_GENERIC_VALUE_41="/home1/03222/tg825207/parallelnbody" MPISPAWN_GENERIC_NAME_42=LIBPATH MPISPAWN_GENERIC_VALUE_42="/opt/apps/xsede/gsi-openssh-5.7/lib64:/usr/lib:/lib" MPISPAWN_GENERIC_NAME_43=HOSTNAME MPISPAWN_GENERIC_VALUE_43="c468-601" MPISPAWN_GENERIC_NAME_44=LMOD_DEFAULT_MODULEPATH MPISPAWN_GENERIC_VALUE_44="/opt/apps/xsede/modulefiles:/opt/apps/modulefiles:/opt/modulefiles" MPISPAWN_GENERIC_NAME_45=ARCHIVE MPISPAWN_GENERIC_VALUE_45="/home/03222/tg825207" MPISPAWN_GENERIC_NAME_46=_ MPISPAWN_GENERIC_VALUE_46="/usr/local/bin/build_env.pl" MPISPAWN_GENERIC_NAME_47=__BASHRC_SOURCED__ MPISPAWN_GENERIC_VALUE_47="1" MPISPAWN_GENERIC_NAME_48=APPS MPISPAWN_GENERIC_VALUE_48="/opt/apps" MPISPAWN_GENERIC_NAME_49=SHELL MPISPAWN_GENERIC_VALUE_49="/bin/bash" MPISPAWN_GENERIC_NAME_50=ARMCI_OPENIB_DEVICE MPISPAWN_GENERIC_VALUE_50="mlx4_0" MPISPAWN_GENERIC_NAME_51=ENVIRONMENT MPISPAWN_GENERIC_VALUE_51="BATCH" MPISPAWN_GENERIC_NAME_52=TACC_FAMILY_MPI MPISPAWN_GENERIC_VALUE_52="mvapich2" MPISPAWN_GENERIC_NAME_53=MV2_IBA_HCA MPISPAWN_GENERIC_VALUE_53="mlx4_0" MPISPAWN_GENERIC_NAME_54=TACC_PUBLIC_MACHINE MPISPAWN_GENERIC_VALUE_54="1" MPISPAWN_GENERIC_NAME_55=SLURM_TACC_NODES MPISPAWN_GENERIC_VALUE_55="1" MPISPAWN_GENERIC_NAME_56=_ModuleTable003_ MPISPAWN_GENERIC_VALUE_56="e30sWyJzaG9ydCJdPSJjbHVzdGVyIixbInN0YXR1cyJdPSJhY3RpdmUiLH0sWyJjbHVzdGVyLXBhdGhzIl09e1siRk4iXT0iL29wdC9tb2R1bGVmaWxlcy9jbHVzdGVyLXBhdGhzIixbImRlZmF1bHQiXT0wLFsiZnVsbE5hbWUiXT0iY2x1c3Rlci1wYXRocyIsWyJsb2FkT3JkZXIiXT0zLHByb3BUPXt9LFsic2hvcnQiXT0iY2x1c3Rlci1wYXRocyIsWyJzdGF0dXMiXT0iYWN0aXZlIix9LGdjYz17WyJGTiJdPSIvb3B0L2FwcHMvbW9kdWxlZmlsZXMvZ2NjLzQuNy4xLmx1YSIsWyJkZWZhdWx0Il09MCxbImZ1bGxOYW1lIl09ImdjYy80LjcuMSIsWyJsb2FkT3JkZXIiXT03LHByb3BUPXt9LFsic2hvcnQiXT0iZ2NjIixbInN0YXR1cyJdPSJhY3RpdmUiLH0sbXZhcGljaDI9e1si" MPISPAWN_GENERIC_NAME_57=QTINC MPISPAWN_GENERIC_VALUE_57="/usr/lib64/qt-3.3/include" MPISPAWN_GENERIC_NAME_58=CVS_RSH MPISPAWN_GENERIC_VALUE_58="ssh" MPISPAWN_GENERIC_NAME_59=MV2_USE_OLD_BCAST MPISPAWN_GENERIC_VALUE_59="1" MPISPAWN_GENERIC_NAME_60=SLURM_JOB_NUM_NODES MPISPAWN_GENERIC_VALUE_60="128" MPISPAWN_GENERIC_NAME_61=SLURM_TASK_PID MPISPAWN_GENERIC_VALUE_61="31479" MPISPAWN_GENERIC_NAME_62=TACC_MPI_GETMODE MPISPAWN_GENERIC_VALUE_62="mvapich2_ssh" MPISPAWN_GENERIC_NAME_63=LANG MPISPAWN_GENERIC_VALUE_63="en_US.UTF-8" MPISPAWN_GENERIC_NAME_64=USER MPISPAWN_GENERIC_VALUE_64="tg825207" MPISPAWN_GENERIC_NAME_65=MIC_I_MPI_DAPL_PROVIDER MPISPAWN_GENERIC_VALUE_65="ofa-v2-mlx4_0-1u" MPISPAWN_GENERIC_NAME_66=G_BROKEN_FILENAMES MPISPAWN_GENERIC_VALUE_66="1" MPISPAWN_GENERIC_NAME_67=SLURM_LOCALID MPISPAWN_GENERIC_VALUE_67="0" MPISPAWN_GENERIC_NAME_68=SLURM_NPROCS MPISPAWN_GENERIC_VALUE_68="2048" MPISPAWN_GENERIC_NAME_69=SLURM_TACC_NNODES_SET MPISPAWN_GENERIC_VALUE_69="0" MPISPAWN_GENERIC_NAME_70=SLURM_TACC_CORES MPISPAWN_GENERIC_VALUE_70="2048" MPISPAWN_GENERIC_NAME_71=LMOD_DIR MPISPAWN_GENERIC_VALUE_71="/opt/apps/lmod/lmod/libexec" MPISPAWN_GENERIC_NAME_72=LMOD_SETTARG_CMD MPISPAWN_GENERIC_VALUE_72=":" MPISPAWN_GENERIC_NAME_73=OMP_NUM_THREADS MPISPAWN_GENERIC_VALUE_73="1" MPISPAWN_GENERIC_NAME_74=_ModuleTable004_ MPISPAWN_GENERIC_VALUE_74="Rk4iXT0iL29wdC9hcHBzL2djYzRfNy9tb2R1bGVmaWxlcy9tdmFwaWNoMi8xLjlhMiIsWyJkZWZhdWx0Il09MSxbImZ1bGxOYW1lIl09Im12YXBpY2gyLzEuOWEyIixbImxvYWRPcmRlciJdPTgscHJvcFQ9e30sWyJzaG9ydCJdPSJtdmFwaWNoMiIsWyJzdGF0dXMiXT0iYWN0aXZlIix9LHhhbHQ9e1siRk4iXT0iL29wdC9hcHBzL21vZHVsZWZpbGVzL3hhbHQvMC40LjQubHVhIixbImRlZmF1bHQiXT0xLFsiZnVsbE5hbWUiXT0ieGFsdC8wLjQuNCIsWyJsb2FkT3JkZXIiXT00LHByb3BUPXt9LFsic2hvcnQiXT0ieGFsdCIsWyJzdGF0dXMiXT0iYWN0aXZlIix9LH0sbXBhdGhBPXsiL29wdC9hcHBzL2djYzRfNy9tdmFwaWNoMl8xXzkvbW9kdWxlZmlsZXMiLCIvb3B0L2FwcHMv" MPISPAWN_GENERIC_NAME_75=__PERSONAL_PATH__ MPISPAWN_GENERIC_VALUE_75="1" MPISPAWN_GENERIC_NAME_76=LMOD_SYSTEM_DEFAULT_MODULES MPISPAWN_GENERIC_VALUE_76="TACC" MPISPAWN_GENERIC_NAME_77=SLURM_JOB_NAME MPISPAWN_GENERIC_VALUE_77="stampedeLargeScaleAllTS.sh" MPISPAWN_GENERIC_NAME_78=SLURM_SUBMIT_HOST MPISPAWN_GENERIC_VALUE_78="login4.stampede.tacc.utexas.edu" MPISPAWN_GENERIC_NAME_79=LD_LIBRARY_PATH MPISPAWN_GENERIC_VALUE_79="/opt/apps/gcc4_7/mvapich2/1.9/lib:/opt/apps/gcc4_7/mvapich2/1.9/lib/shared:/opt/apps/gcc/4.7.1/lib64:/opt/apps/gcc/4.7.1/lib:/opt/apps/xsede/gsi-openssh-5.7/lib64:/opt/apps/xsede/gsi-openssh-5.7/lib64" MPISPAWN_GENERIC_NAME_80=SLURM_QUEUE MPISPAWN_GENERIC_VALUE_80="normal" MPISPAWN_GENERIC_NAME_81=PKG_CONFIG_PATH MPISPAWN_GENERIC_VALUE_81="/opt/apps/gcc4_7/mvapich2/1.9/lib/pkgconfig" MPISPAWN_GENERIC_NAME_82=LMOD_PREPEND_BLOCK MPISPAWN_GENERIC_VALUE_82="normal" MPISPAWN_GENERIC_NAME_83=TACC_FAMILY_COMPILER MPISPAWN_GENERIC_VALUE_83="gcc" MPISPAWN_GENERIC_NAME_84=TACC_DOMAIN MPISPAWN_GENERIC_VALUE_84="stampede" MPISPAWN_GENERIC_NAME_85=MV2_HOMOGENEOUS_CLUSTER MPISPAWN_GENERIC_VALUE_85="1" MPISPAWN_GENERIC_NAME_86=LMOD_FAMILY_COMPILER MPISPAWN_GENERIC_VALUE_86="gcc" MPISPAWN_GENERIC_NAME_87=MV2_USE_UD_HYBRID MPISPAWN_GENERIC_VALUE_87="0" MPISPAWN_GENERIC_NAME_88=MPICH_HOME MPISPAWN_GENERIC_VALUE_88="/opt/apps/gcc4_7/mvapich2/1.9" MPISPAWN_GENERIC_NAME_89=SLURM_JOB_NODELIST MPISPAWN_GENERIC_VALUE_89="c468-601,c471-503,c472-[401-402],c473-[303-304,401,804,901],c474-[203-204,301,401,403-404],c478-303,c482-404,c483-[001-004,101,504,601,804,901],c484-[101,103-104,201,602,701],c485-502,c486-102,c490-[302,904],c491-004,c492-701,c493-[001,102,201],c494-[101,402-403,602,702-703,904],c495-[001-002,201],c497-[102,202,504,601-603],c498-[304,402,603],c501-[204,301,604,801],c502-104,c503-901,c504-[602,604,702,801],c505-[001,202-204,401,502-504,802,901-904],c506-[302,704],c507-[102,202,503,601,904],c508-[101,204,601,702-703,802,903-904],c509-[002,201,301,303,401,501-502],c510-[003,401,404,501,503],c511-[601-602,902-903],c536-[801,902],c537-902,c538-[001,003,104],c540-[104,201-202,804,901-904]" MPISPAWN_GENERIC_NAME_90=SLURM_PROCID MPISPAWN_GENERIC_VALUE_90="0" MPISPAWN_GENERIC_NAME_91=I_MPI_DAPL_PROVIDER MPISPAWN_GENERIC_VALUE_91="ofa-v2-mlx4_0-1u" MPISPAWN_GENERIC_NAME_92=GLOBUS_LOCATION MPISPAWN_GENERIC_VALUE_92="/opt/apps/xsede/gsi-openssh-5.7" MPISPAWN_GENERIC_NAME_93=LMOD_COLORIZE MPISPAWN_GENERIC_VALUE_93="yes" MPISPAWN_GENERIC_NAME_94=I_MPI_HYDRA_BRANCH_COUNT MPISPAWN_GENERIC_VALUE_94="6000" MPISPAWN_GENERIC_NAME_95=SLURM_NNODES MPISPAWN_GENERIC_VALUE_95="128" MPISPAWN_GENERIC_NAME_96=MV2_DEFAULT_TIME_OUT MPISPAWN_GENERIC_VALUE_96="23" MPISPAWN_GENERIC_NAME_97=SLURM_TACC_NCORES_SET MPISPAWN_GENERIC_VALUE_97="1" MPISPAWN_GENERIC_NAME_98=__Init_Default_Modules MPISPAWN_GENERIC_VALUE_98="1" MPISPAWN_GENERIC_NAME_99=BASH_ENV MPISPAWN_GENERIC_VALUE_99="/etc/tacc/tacc_functions" MPISPAWN_GENERIC_NAME_100=SLURM_TOPOLOGY_ADDR_PATTERN MPISPAWN_GENERIC_VALUE_100="node" MPISPAWN_GENERIC_NAME_101=MV2_USE_RING_STARTUP MPISPAWN_GENERIC_VALUE_101="0" MPISPAWN_GENERIC_NAME_102=TACC_XALT_BIN MPISPAWN_GENERIC_VALUE_102="/opt/apps/xalt/0.4.4/bin" MPISPAWN_GENERIC_NAME_103=X509_USER_PROXY MPISPAWN_GENERIC_VALUE_103="/tmp/x509up_p3084.file9zM3fu.1" MPISPAWN_GENERIC_NAME_104=TMPDIR MPISPAWN_GENERIC_VALUE_104="/tmp" MPISPAWN_GENERIC_NAME_105=LMOD_FULL_SETTARG_SUPPORT MPISPAWN_GENERIC_VALUE_105="no" MPISPAWN_ID=60 MPISPAWN_WORKING_DIR=/home1/03222/tg825207/parallelnbody MPISPAWN_MPIRUN_RANK_0=960 MPISPAWN_MPIRUN_RANK_1=961 MPISPAWN_MPIRUN_RANK_2=962 MPISPAWN_MPIRUN_RANK_3=963 MPISPAWN_MPIRUN_RANK_4=964 MPISPAWN_MPIRUN_RANK_5=965 MPISPAWN_MPIRUN_RANK_6=966 MPISPAWN_MPIRUN_RANK_7=967 MPISPAWN_MPIRUN_RANK_8=968 MPISPAWN_MPIRUN_RANK_9=969 MPISPAWN_MPIRUN_RANK_10=970 MPISPAWN_MPIRUN_RANK_11=971 MPISPAWN_MPIRUN_RANK_12=972 MPISPAWN_MPIRUN_RANK_13=973 MPISPAWN_MPIRUN_RANK_14=974 MPISPAWN_MPIRUN_RANK_15=975 /opt/apps/gcc4_7/mvapich2/1.9/bin/mpispawn 0
[c490-302.stampede.tacc.utexas.edu:mpispawn_34][error_sighandler] Caught error: Segmentation fault (signal 11)
[c494-402.stampede.tacc.utexas.edu:mpispawn_42][error_sighandler] Caught error: Segmentation fault (signal 11)
bash: line 1: 57438 Segmentation fault      /usr/bin/env LD_LIBRARY_PATH=/opt/apps/gcc4_7/mvapich2/1.9/lib:/opt/apps/gcc4_7/mvapich2/1.9/lib/shared:/opt/apps/gcc/4.7.1/lib64:/opt/apps/gcc/4.7.1/lib:/opt/apps/xsede/gsi-openssh-5.7/lib64:/opt/apps/xsede/gsi-openssh-5.7/lib64 MPISPAWN_MPIRUN_MPD=0 USE_LINEAR_SSH=1 MPISPAWN_MPIRUN_HOST=c468-601.stampede.tacc.utexas.edu MPISPAWN_MPIRUN_HOSTIP=129.114.76.165 MPIRUN_RSH_LAUNCH=1 MPISPAWN_CHECKIN_PORT=56630 MPISPAWN_MPIRUN_PORT=56630 MPISPAWN_NNODES=128 MPISPAWN_GLOBAL_NPROCS=2048 MPISPAWN_MPIRUN_ID=47451 MPISPAWN_ARGC=4 MPDMAN_KVS_TEMPLATE=kvs_437_c468-601.stampede.tacc.utexas.edu_47451 MPISPAWN_LOCAL_NPROCS=16 MPISPAWN_ARGV_0='/home1/03222/tg825207/parallelnbody/teamscatter' MPISPAWN_ARGV_1='256000' MPISPAWN_ARGV_2='-c' MPISPAWN_ARGV_3='2' MPISPAWN_ARGC=4 MPISPAWN_GENERIC_ENV_COUNT=106 MPISPAWN_GENERIC_NAME_0=TACC_ENVLEN MPISPAWN_GENERIC_VALUE_0=6578 MPISPAWN_GENERIC_NAME_1=SLURM_JOB_ID MPISPAWN_GENERIC_VALUE_1="4306264" MPISPAWN_GENERIC_NAME_2=SLURMD_NODENAME MPISPAWN_GENERIC_VALUE_2="c468-601" MPISPAWN_GENERIC_NAME_3=STOCKYARD MPISPAWN_GENERIC_VALUE_3="/work/03222/tg825207" MPISPAWN_GENERIC_NAME_4=LC_ALL MPISPAWN_GENERIC_VALUE_4="en_US.UTF-8" MPISPAWN_GENERIC_NAME_5=SLURM_CHECKPOINT_IMAGE_DIR MPISPAWN_GENERIC_VALUE_5="/home1/03222/tg825207/parallelnbody" MPISPAWN_GENERIC_NAME_6=LMOD_FAMILY_MPI MPISPAWN_GENERIC_VALUE_6="mvapich2" MPISPAWN_GENERIC_NAME_7=SLURM_JOBID MPISPAWN_GENERIC_VALUE_7="4306264" MPISPAWN_GENERIC_NAME_8=_ModuleTable001_ MPISPAWN_GENERIC_VALUE_8="X01vZHVsZVRhYmxlXz17WyJhY3RpdmVTaXplIl09OCxiYXNlTXBhdGhBPXsiL29wdC9hcHBzL3hzZWRlL21vZHVsZWZpbGVzIiwiL29wdC9hcHBzL21vZHVsZWZpbGVzIiwiL29wdC9tb2R1bGVmaWxlcyIsfSxbImNfcmVidWlsZFRpbWUiXT1mYWxzZSxbImNfc2hvcnRUaW1lIl09ZmFsc2UsZmFtaWx5PXtbIk1QSSJdPSJtdmFwaWNoMiIsWyJjb21waWxlciJdPSJnY2MiLH0saW5hY3RpdmU9e30sbVQ9e0xpbnV4PXtbIkZOIl09Ii9vcHQvbW9kdWxlZmlsZXMvTGludXgiLFsiZGVmYXVsdCJdPTAsWyJmdWxsTmFtZSJdPSJMaW51eCIsWyJsb2FkT3JkZXIiXT0yLHByb3BUPXt9LFsic2hvcnQiXT0iTGludXgiLFsic3RhdHVzIl09ImFjdGl2ZSIsfSxUQUNDPXtbIkZOIl09Ii9v" MPISPAWN_GENERIC_NAME_9=TACC_SYSTEM MPISPAWN_GENERIC_VALUE_9="stampede" MPISPAWN_GENERIC_NAME_10=WORK MPISPAWN_GENERIC_VALUE_10="/work/03222/tg825207" MPISPAWN_GENERIC_NAME_11=SLURM_GTIDS MPISPAWN_GENERIC_VALUE_11="0" MPISPAWN_GENERIC_NAME_12=GCC_LIB MPISPAWN_GENERIC_VALUE_12="/opt/apps/gcc/4.7.1/lib64" MPISPAWN_GENERIC_NAME_13=SLURM_PRIO_PROCESS MPISPAWN_GENERIC_VALUE_13="0" MPISPAWN_GENERIC_NAME_14=SCRATCH MPISPAWN_GENERIC_VALUE_14="/scratch/03222/tg825207" MPISPAWN_GENERIC_NAME_15=LOGNAME MPISPAWN_GENERIC_VALUE_15="tg825207" MPISPAWN_GENERIC_NAME_16=_ModuleTable002_ MPISPAWN_GENERIC_VALUE_16="cHQvbW9kdWxlZmlsZXMvVEFDQyIsWyJkZWZhdWx0Il09MCxbImZ1bGxOYW1lIl09IlRBQ0MiLFsibG9hZE9yZGVyIl09Nixwcm9wVD17fSxbInNob3J0Il09IlRBQ0MiLFsic3RhdHVzIl09ImFjdGl2ZSIsfSxbIlRBQ0MtcGF0aHMiXT17WyJGTiJdPSIvb3B0L21vZHVsZWZpbGVzL1RBQ0MtcGF0aHMiLFsiZGVmYXVsdCJdPTAsWyJmdWxsTmFtZSJdPSJUQUNDLXBhdGhzIixbImxvYWRPcmRlciJdPTEscHJvcFQ9e30sWyJzaG9ydCJdPSJUQUNDLXBhdGhzIixbInN0YXR1cyJdPSJhY3RpdmUiLH0sY2x1c3Rlcj17WyJGTiJdPSIvb3B0L21vZHVsZWZpbGVzL2NsdXN0ZXIiLFsiZGVmYXVsdCJdPTAsWyJmdWxsTmFtZSJdPSJjbHVzdGVyIixbImxvYWRPcmRlciJdPTUscHJvcFQ9" MPISPAWN_GENERIC_NAME_17=INPUTRC MPISPAWN_GENERIC_VALUE_17="/etc/inputrc" MPISPAWN_GENERIC_NAME_18=LMOD_CMD MPISPAWN_GENERIC_VALUE_18="/opt/apps/lmod/lmod/libexec/lmod" MPISPAWN_GENERIC_NAME_19=I_MPI_OFA_ADAPTER_NAME MPISPAWN_GENERIC_VALUE_19="mlx4_0" MPISPAWN_GENERIC_NAME_20=QTLIB MPISPAWN_GENERIC_VALUE_20="/usr/lib64/qt-3.3/lib" MPISPAWN_GENERIC_NAME_21=NODE_TASKS_PPN_INFO MPISPAWN_GENERIC_VALUE_21=""16,0_"" MPISPAWN_GENERIC_NAME_22=SLURM_CPUS_ON_NODE MPISPAWN_GENERIC_VALUE_22="16" MPISPAWN_GENERIC_NAME_23=PATH MPISPAWN_GENERIC_VALUE_23="/opt/apps/gcc4_7/mvapich2/1.9/bin:/opt/apps/gcc/4.7.1/bin:/opt/apps/xalt/0.4.4/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/opt/apps/xsede/gsi-openssh-5.7/bin:/usr/X11R6/bin:/opt/ofed/bin:/opt/ofed/sbin:." MPISPAWN_GENERIC_NAME_24=SLURM_TACC_JOBNAME MPISPAWN_GENERIC_VALUE_24="stampedeLargeScaleAllTS.sh" MPISPAWN_GENERIC_NAME_25=SLURM_NODEID MPISPAWN_GENERIC_VALUE_25="0" MPISPAWN_GENERIC_NAME_26=LMOD_PKG MPISPAWN_GENERIC_VALUE_26="/opt/apps/lmod/lmod" MPISPAWN_GENERIC_NAME_27=MIC_I_MPI_OFA_ADAPTER_NAME MPISPAWN_GENERIC_VALUE_27="mlx4_0" MPISPAWN_GENERIC_NAME_28=HISTSIZE MPISPAWN_GENERIC_VALUE_28="1000" MPISPAWN_GENERIC_NAME_29=SLURM_NTASKS MPISPAWN_GENERIC_VALUE_29="2048" MPISPAWN_GENERIC_NAME_30=SHLIB_PATH MPISPAWN_GENERIC_VALUE_30="/opt/apps/xsede/gsi-openssh-5.7/lib64" MPISPAWN_GENERIC_NAME_31=TACC_OVERRIDE_PROJECT MPISPAWN_GENERIC_VALUE_31="TG-ASC140020" MPISPAWN_GENERIC_NAME_32=_ModuleTable005_ MPISPAWN_GENERIC_VALUE_32="Z2NjNF83L21vZHVsZWZpbGVzIiwiL29wdC9hcHBzL3hzZWRlL21vZHVsZWZpbGVzIiwiL29wdC9hcHBzL21vZHVsZWZpbGVzIiwiL29wdC9tb2R1bGVmaWxlcyIsfSxbInN5c3RlbUJhc2VNUEFUSCJdPSIvb3B0L2FwcHMveHNlZGUvbW9kdWxlZmlsZXM6L29wdC9hcHBzL21vZHVsZWZpbGVzOi9vcHQvbW9kdWxlZmlsZXMiLFsidmVyc2lvbiJdPTIsfQ==" MPISPAWN_GENERIC_NAME_33=_ModuleTable_Sz_ MPISPAWN_GENERIC_VALUE_33="5" MPISPAWN_GENERIC_NAME_34=MAIL MPISPAWN_GENERIC_VALUE_34="/var/spool/mail/tg825207" MPISPAWN_GENERIC_NAME_35=MV2_USE_HUGEPAGES MPISPAWN_GENERIC_VALUE_35="0" MPISPAWN_GENERIC_NAME_36=SLURM_TOPOLOGY_ADDR MPISPAWN_GENERIC_VALUE_36="c468-601" MPISPAWN_GENERIC_NAME_37=TACC_XALT_DIR MPISPAWN_GENERIC_VALUE_37="/opt/apps/xalt/0.4.4/" MPISPAWN_GENERIC_NAME_38=QTDIR MPISPAWN_GENERIC_VALUE_38="/usr/lib64/qt-3.3" MPISPAWN_GENERIC_NAME_39=SYSTEM MPISPAWN_GENERIC_VALUE_39="linux" MPISPAWN_GENERIC_NAME_40=SLURM_TACC_RUNLIMIT_MINS MPISPAWN_GENERIC_VALUE_40="180" MPISPAWN_GENERIC_NAME_41=SLURM_SUBMIT_DIR MPISPAWN_GENERIC_VALUE_41="/home1/03222/tg825207/parallelnbody" MPISPAWN_GENERIC_NAME_42=LIBPATH MPISPAWN_GENERIC_VALUE_42="/opt/apps/xsede/gsi-openssh-5.7/lib64:/usr/lib:/lib" MPISPAWN_GENERIC_NAME_43=HOSTNAME MPISPAWN_GENERIC_VALUE_43="c468-601" MPISPAWN_GENERIC_NAME_44=LMOD_DEFAULT_MODULEPATH MPISPAWN_GENERIC_VALUE_44="/opt/apps/xsede/modulefiles:/opt/apps/modulefiles:/opt/modulefiles" MPISPAWN_GENERIC_NAME_45=ARCHIVE MPISPAWN_GENERIC_VALUE_45="/home/03222/tg825207" MPISPAWN_GENERIC_NAME_46=_ MPISPAWN_GENERIC_VALUE_46="/usr/local/bin/build_env.pl" MPISPAWN_GENERIC_NAME_47=__BASHRC_SOURCED__ MPISPAWN_GENERIC_VALUE_47="1" MPISPAWN_GENERIC_NAME_48=APPS MPISPAWN_GENERIC_VALUE_48="/opt/apps" MPISPAWN_GENERIC_NAME_49=SHELL MPISPAWN_GENERIC_VALUE_49="/bin/bash" MPISPAWN_GENERIC_NAME_50=ARMCI_OPENIB_DEVICE MPISPAWN_GENERIC_VALUE_50="mlx4_0" MPISPAWN_GENERIC_NAME_51=ENVIRONMENT MPISPAWN_GENERIC_VALUE_51="BATCH" MPISPAWN_GENERIC_NAME_52=TACC_FAMILY_MPI MPISPAWN_GENERIC_VALUE_52="mvapich2" MPISPAWN_GENERIC_NAME_53=MV2_IBA_HCA MPISPAWN_GENERIC_VALUE_53="mlx4_0" MPISPAWN_GENERIC_NAME_54=TACC_PUBLIC_MACHINE MPISPAWN_GENERIC_VALUE_54="1" MPISPAWN_GENERIC_NAME_55=SLURM_TACC_NODES MPISPAWN_GENERIC_VALUE_55="1" MPISPAWN_GENERIC_NAME_56=_ModuleTable003_ MPISPAWN_GENERIC_VALUE_56="e30sWyJzaG9ydCJdPSJjbHVzdGVyIixbInN0YXR1cyJdPSJhY3RpdmUiLH0sWyJjbHVzdGVyLXBhdGhzIl09e1siRk4iXT0iL29wdC9tb2R1bGVmaWxlcy9jbHVzdGVyLXBhdGhzIixbImRlZmF1bHQiXT0wLFsiZnVsbE5hbWUiXT0iY2x1c3Rlci1wYXRocyIsWyJsb2FkT3JkZXIiXT0zLHByb3BUPXt9LFsic2hvcnQiXT0iY2x1c3Rlci1wYXRocyIsWyJzdGF0dXMiXT0iYWN0aXZlIix9LGdjYz17WyJGTiJdPSIvb3B0L2FwcHMvbW9kdWxlZmlsZXMvZ2NjLzQuNy4xLmx1YSIsWyJkZWZhdWx0Il09MCxbImZ1bGxOYW1lIl09ImdjYy80LjcuMSIsWyJsb2FkT3JkZXIiXT03LHByb3BUPXt9LFsic2hvcnQiXT0iZ2NjIixbInN0YXR1cyJdPSJhY3RpdmUiLH0sbXZhcGljaDI9e1si" MPISPAWN_GENERIC_NAME_57=QTINC MPISPAWN_GENERIC_VALUE_57="/usr/lib64/qt-3.3/include" MPISPAWN_GENERIC_NAME_58=CVS_RSH MPISPAWN_GENERIC_VALUE_58="ssh" MPISPAWN_GENERIC_NAME_59=MV2_USE_OLD_BCAST MPISPAWN_GENERIC_VALUE_59="1" MPISPAWN_GENERIC_NAME_60=SLURM_JOB_NUM_NODES MPISPAWN_GENERIC_VALUE_60="128" MPISPAWN_GENERIC_NAME_61=SLURM_TASK_PID MPISPAWN_GENERIC_VALUE_61="31479" MPISPAWN_GENERIC_NAME_62=TACC_MPI_GETMODE MPISPAWN_GENERIC_VALUE_62="mvapich2_ssh" MPISPAWN_GENERIC_NAME_63=LANG MPISPAWN_GENERIC_VALUE_63="en_US.UTF-8" MPISPAWN_GENERIC_NAME_64=USER MPISPAWN_GENERIC_VALUE_64="tg825207" MPISPAWN_GENERIC_NAME_65=MIC_I_MPI_DAPL_PROVIDER MPISPAWN_GENERIC_VALUE_65="ofa-v2-mlx4_0-1u" MPISPAWN_GENERIC_NAME_66=G_BROKEN_FILENAMES MPISPAWN_GENERIC_VALUE_66="1" MPISPAWN_GENERIC_NAME_67=SLURM_LOCALID MPISPAWN_GENERIC_VALUE_67="0" MPISPAWN_GENERIC_NAME_68=SLURM_NPROCS MPISPAWN_GENERIC_VALUE_68="2048" MPISPAWN_GENERIC_NAME_69=SLURM_TACC_NNODES_SET MPISPAWN_GENERIC_VALUE_69="0" MPISPAWN_GENERIC_NAME_70=SLURM_TACC_CORES MPISPAWN_GENERIC_VALUE_70="2048" MPISPAWN_GENERIC_NAME_71=LMOD_DIR MPISPAWN_GENERIC_VALUE_71="/opt/apps/lmod/lmod/libexec" MPISPAWN_GENERIC_NAME_72=LMOD_SETTARG_CMD MPISPAWN_GENERIC_VALUE_72=":" MPISPAWN_GENERIC_NAME_73=OMP_NUM_THREADS MPISPAWN_GENERIC_VALUE_73="1" MPISPAWN_GENERIC_NAME_74=_ModuleTable004_ MPISPAWN_GENERIC_VALUE_74="Rk4iXT0iL29wdC9hcHBzL2djYzRfNy9tb2R1bGVmaWxlcy9tdmFwaWNoMi8xLjlhMiIsWyJkZWZhdWx0Il09MSxbImZ1bGxOYW1lIl09Im12YXBpY2gyLzEuOWEyIixbImxvYWRPcmRlciJdPTgscHJvcFQ9e30sWyJzaG9ydCJdPSJtdmFwaWNoMiIsWyJzdGF0dXMiXT0iYWN0aXZlIix9LHhhbHQ9e1siRk4iXT0iL29wdC9hcHBzL21vZHVsZWZpbGVzL3hhbHQvMC40LjQubHVhIixbImRlZmF1bHQiXT0xLFsiZnVsbE5hbWUiXT0ieGFsdC8wLjQuNCIsWyJsb2FkT3JkZXIiXT00LHByb3BUPXt9LFsic2hvcnQiXT0ieGFsdCIsWyJzdGF0dXMiXT0iYWN0aXZlIix9LH0sbXBhdGhBPXsiL29wdC9hcHBzL2djYzRfNy9tdmFwaWNoMl8xXzkvbW9kdWxlZmlsZXMiLCIvb3B0L2FwcHMv" MPISPAWN_GENERIC_NAME_75=__PERSONAL_PATH__ MPISPAWN_GENERIC_VALUE_75="1" MPISPAWN_GENERIC_NAME_76=LMOD_SYSTEM_DEFAULT_MODULES MPISPAWN_GENERIC_VALUE_76="TACC" MPISPAWN_GENERIC_NAME_77=SLURM_JOB_NAME MPISPAWN_GENERIC_VALUE_77="stampedeLargeScaleAllTS.sh" MPISPAWN_GENERIC_NAME_78=SLURM_SUBMIT_HOST MPISPAWN_GENERIC_VALUE_78="login4.stampede.tacc.utexas.edu" MPISPAWN_GENERIC_NAME_79=LD_LIBRARY_PATH MPISPAWN_GENERIC_VALUE_79="/opt/apps/gcc4_7/mvapich2/1.9/lib:/opt/apps/gcc4_7/mvapich2/1.9/lib/shared:/opt/apps/gcc/4.7.1/lib64:/opt/apps/gcc/4.7.1/lib:/opt/apps/xsede/gsi-openssh-5.7/lib64:/opt/apps/xsede/gsi-openssh-5.7/lib64" MPISPAWN_GENERIC_NAME_80=SLURM_QUEUE MPISPAWN_GENERIC_VALUE_80="normal" MPISPAWN_GENERIC_NAME_81=PKG_CONFIG_PATH MPISPAWN_GENERIC_VALUE_81="/opt/apps/gcc4_7/mvapich2/1.9/lib/pkgconfig" MPISPAWN_GENERIC_NAME_82=LMOD_PREPEND_BLOCK MPISPAWN_GENERIC_VALUE_82="normal" MPISPAWN_GENERIC_NAME_83=TACC_FAMILY_COMPILER MPISPAWN_GENERIC_VALUE_83="gcc" MPISPAWN_GENERIC_NAME_84=TACC_DOMAIN MPISPAWN_GENERIC_VALUE_84="stampede" MPISPAWN_GENERIC_NAME_85=MV2_HOMOGENEOUS_CLUSTER MPISPAWN_GENERIC_VALUE_85="1" MPISPAWN_GENERIC_NAME_86=LMOD_FAMILY_COMPILER MPISPAWN_GENERIC_VALUE_86="gcc" MPISPAWN_GENERIC_NAME_87=MV2_USE_UD_HYBRID MPISPAWN_GENERIC_VALUE_87="0" MPISPAWN_GENERIC_NAME_88=MPICH_HOME MPISPAWN_GENERIC_VALUE_88="/opt/apps/gcc4_7/mvapich2/1.9" MPISPAWN_GENERIC_NAME_89=SLURM_JOB_NODELIST MPISPAWN_GENERIC_VALUE_89="c468-601,c471-503,c472-[401-402],c473-[303-304,401,804,901],c474-[203-204,301,401,403-404],c478-303,c482-404,c483-[001-004,101,504,601,804,901],c484-[101,103-104,201,602,701],c485-502,c486-102,c490-[302,904],c491-004,c492-701,c493-[001,102,201],c494-[101,402-403,602,702-703,904],c495-[001-002,201],c497-[102,202,504,601-603],c498-[304,402,603],c501-[204,301,604,801],c502-104,c503-901,c504-[602,604,702,801],c505-[001,202-204,401,502-504,802,901-904],c506-[302,704],c507-[102,202,503,601,904],c508-[101,204,601,702-703,802,903-904],c509-[002,201,301,303,401,501-502],c510-[003,401,404,501,503],c511-[601-602,902-903],c536-[801,902],c537-902,c538-[001,003,104],c540-[104,201-202,804,901-904]" MPISPAWN_GENERIC_NAME_90=SLURM_PROCID MPISPAWN_GENERIC_VALUE_90="0" MPISPAWN_GENERIC_NAME_91=I_MPI_DAPL_PROVIDER MPISPAWN_GENERIC_VALUE_91="ofa-v2-mlx4_0-1u" MPISPAWN_GENERIC_NAME_92=GLOBUS_LOCATION MPISPAWN_GENERIC_VALUE_92="/opt/apps/xsede/gsi-openssh-5.7" MPISPAWN_GENERIC_NAME_93=LMOD_COLORIZE MPISPAWN_GENERIC_VALUE_93="yes" MPISPAWN_GENERIC_NAME_94=I_MPI_HYDRA_BRANCH_COUNT MPISPAWN_GENERIC_VALUE_94="6000" MPISPAWN_GENERIC_NAME_95=SLURM_NNODES MPISPAWN_GENERIC_VALUE_95="128" MPISPAWN_GENERIC_NAME_96=MV2_DEFAULT_TIME_OUT MPISPAWN_GENERIC_VALUE_96="23" MPISPAWN_GENERIC_NAME_97=SLURM_TACC_NCORES_SET MPISPAWN_GENERIC_VALUE_97="1" MPISPAWN_GENERIC_NAME_98=__Init_Default_Modules MPISPAWN_GENERIC_VALUE_98="1" MPISPAWN_GENERIC_NAME_99=BASH_ENV MPISPAWN_GENERIC_VALUE_99="/etc/tacc/tacc_functions" MPISPAWN_GENERIC_NAME_100=SLURM_TOPOLOGY_ADDR_PATTERN MPISPAWN_GENERIC_VALUE_100="node" MPISPAWN_GENERIC_NAME_101=MV2_USE_RING_STARTUP MPISPAWN_GENERIC_VALUE_101="0" MPISPAWN_GENERIC_NAME_102=TACC_XALT_BIN MPISPAWN_GENERIC_VALUE_102="/opt/apps/xalt/0.4.4/bin" MPISPAWN_GENERIC_NAME_103=X509_USER_PROXY MPISPAWN_GENERIC_VALUE_103="/tmp/x509up_p3084.file9zM3fu.1" MPISPAWN_GENERIC_NAME_104=TMPDIR MPISPAWN_GENERIC_VALUE_104="/tmp" MPISPAWN_GENERIC_NAME_105=LMOD_FULL_SETTARG_SUPPORT MPISPAWN_GENERIC_VALUE_105="no" MPISPAWN_ID=34 MPISPAWN_WORKING_DIR=/home1/03222/tg825207/parallelnbody MPISPAWN_MPIRUN_RANK_0=544 MPISPAWN_MPIRUN_RANK_1=545 MPISPAWN_MPIRUN_RANK_2=546 MPISPAWN_MPIRUN_RANK_3=547 MPISPAWN_MPIRUN_RANK_4=548 MPISPAWN_MPIRUN_RANK_5=549 MPISPAWN_MPIRUN_RANK_6=550 MPISPAWN_MPIRUN_RANK_7=551 MPISPAWN_MPIRUN_RANK_8=552 MPISPAWN_MPIRUN_RANK_9=553 MPISPAWN_MPIRUN_RANK_10=554 MPISPAWN_MPIRUN_RANK_11=555 MPISPAWN_MPIRUN_RANK_12=556 MPISPAWN_MPIRUN_RANK_13=557 MPISPAWN_MPIRUN_RANK_14=558 MPISPAWN_MPIRUN_RANK_15=559 /opt/apps/gcc4_7/mvapich2/1.9/bin/mpispawn 0
bash: line 1: 78011 Segmentation fault      /usr/bin/env LD_LIBRARY_PATH=/opt/apps/gcc4_7/mvapich2/1.9/lib:/opt/apps/gcc4_7/mvapich2/1.9/lib/shared:/opt/apps/gcc/4.7.1/lib64:/opt/apps/gcc/4.7.1/lib:/opt/apps/xsede/gsi-openssh-5.7/lib64:/opt/apps/xsede/gsi-openssh-5.7/lib64 MPISPAWN_MPIRUN_MPD=0 USE_LINEAR_SSH=1 MPISPAWN_MPIRUN_HOST=c468-601.stampede.tacc.utexas.edu MPISPAWN_MPIRUN_HOSTIP=129.114.76.165 MPIRUN_RSH_LAUNCH=1 MPISPAWN_CHECKIN_PORT=56630 MPISPAWN_MPIRUN_PORT=56630 MPISPAWN_NNODES=128 MPISPAWN_GLOBAL_NPROCS=2048 MPISPAWN_MPIRUN_ID=47451 MPISPAWN_ARGC=4 MPDMAN_KVS_TEMPLATE=kvs_437_c468-601.stampede.tacc.utexas.edu_47451 MPISPAWN_LOCAL_NPROCS=16 MPISPAWN_ARGV_0='/home1/03222/tg825207/parallelnbody/teamscatter' MPISPAWN_ARGV_1='256000' MPISPAWN_ARGV_2='-c' MPISPAWN_ARGV_3='2' MPISPAWN_ARGC=4 MPISPAWN_GENERIC_ENV_COUNT=106 MPISPAWN_GENERIC_NAME_0=TACC_ENVLEN MPISPAWN_GENERIC_VALUE_0=6578 MPISPAWN_GENERIC_NAME_1=SLURM_JOB_ID MPISPAWN_GENERIC_VALUE_1="4306264" MPISPAWN_GENERIC_NAME_2=SLURMD_NODENAME MPISPAWN_GENERIC_VALUE_2="c468-601" MPISPAWN_GENERIC_NAME_3=STOCKYARD MPISPAWN_GENERIC_VALUE_3="/work/03222/tg825207" MPISPAWN_GENERIC_NAME_4=LC_ALL MPISPAWN_GENERIC_VALUE_4="en_US.UTF-8" MPISPAWN_GENERIC_NAME_5=SLURM_CHECKPOINT_IMAGE_DIR MPISPAWN_GENERIC_VALUE_5="/home1/03222/tg825207/parallelnbody" MPISPAWN_GENERIC_NAME_6=LMOD_FAMILY_MPI MPISPAWN_GENERIC_VALUE_6="mvapich2" MPISPAWN_GENERIC_NAME_7=SLURM_JOBID MPISPAWN_GENERIC_VALUE_7="4306264" MPISPAWN_GENERIC_NAME_8=_ModuleTable001_ MPISPAWN_GENERIC_VALUE_8="X01vZHVsZVRhYmxlXz17WyJhY3RpdmVTaXplIl09OCxiYXNlTXBhdGhBPXsiL29wdC9hcHBzL3hzZWRlL21vZHVsZWZpbGVzIiwiL29wdC9hcHBzL21vZHVsZWZpbGVzIiwiL29wdC9tb2R1bGVmaWxlcyIsfSxbImNfcmVidWlsZFRpbWUiXT1mYWxzZSxbImNfc2hvcnRUaW1lIl09ZmFsc2UsZmFtaWx5PXtbIk1QSSJdPSJtdmFwaWNoMiIsWyJjb21waWxlciJdPSJnY2MiLH0saW5hY3RpdmU9e30sbVQ9e0xpbnV4PXtbIkZOIl09Ii9vcHQvbW9kdWxlZmlsZXMvTGludXgiLFsiZGVmYXVsdCJdPTAsWyJmdWxsTmFtZSJdPSJMaW51eCIsWyJsb2FkT3JkZXIiXT0yLHByb3BUPXt9LFsic2hvcnQiXT0iTGludXgiLFsic3RhdHVzIl09ImFjdGl2ZSIsfSxUQUNDPXtbIkZOIl09Ii9v" MPISPAWN_GENERIC_NAME_9=TACC_SYSTEM MPISPAWN_GENERIC_VALUE_9="stampede" MPISPAWN_GENERIC_NAME_10=WORK MPISPAWN_GENERIC_VALUE_10="/work/03222/tg825207" MPISPAWN_GENERIC_NAME_11=SLURM_GTIDS MPISPAWN_GENERIC_VALUE_11="0" MPISPAWN_GENERIC_NAME_12=GCC_LIB MPISPAWN_GENERIC_VALUE_12="/opt/apps/gcc/4.7.1/lib64" MPISPAWN_GENERIC_NAME_13=SLURM_PRIO_PROCESS MPISPAWN_GENERIC_VALUE_13="0" MPISPAWN_GENERIC_NAME_14=SCRATCH MPISPAWN_GENERIC_VALUE_14="/scratch/03222/tg825207" MPISPAWN_GENERIC_NAME_15=LOGNAME MPISPAWN_GENERIC_VALUE_15="tg825207" MPISPAWN_GENERIC_NAME_16=_ModuleTable002_ MPISPAWN_GENERIC_VALUE_16="cHQvbW9kdWxlZmlsZXMvVEFDQyIsWyJkZWZhdWx0Il09MCxbImZ1bGxOYW1lIl09IlRBQ0MiLFsibG9hZE9yZGVyIl09Nixwcm9wVD17fSxbInNob3J0Il09IlRBQ0MiLFsic3RhdHVzIl09ImFjdGl2ZSIsfSxbIlRBQ0MtcGF0aHMiXT17WyJGTiJdPSIvb3B0L21vZHVsZWZpbGVzL1RBQ0MtcGF0aHMiLFsiZGVmYXVsdCJdPTAsWyJmdWxsTmFtZSJdPSJUQUNDLXBhdGhzIixbImxvYWRPcmRlciJdPTEscHJvcFQ9e30sWyJzaG9ydCJdPSJUQUNDLXBhdGhzIixbInN0YXR1cyJdPSJhY3RpdmUiLH0sY2x1c3Rlcj17WyJGTiJdPSIvb3B0L21vZHVsZWZpbGVzL2NsdXN0ZXIiLFsiZGVmYXVsdCJdPTAsWyJmdWxsTmFtZSJdPSJjbHVzdGVyIixbImxvYWRPcmRlciJdPTUscHJvcFQ9" MPISPAWN_GENERIC_NAME_17=INPUTRC MPISPAWN_GENERIC_VALUE_17="/etc/inputrc" MPISPAWN_GENERIC_NAME_18=LMOD_CMD MPISPAWN_GENERIC_VALUE_18="/opt/apps/lmod/lmod/libexec/lmod" MPISPAWN_GENERIC_NAME_19=I_MPI_OFA_ADAPTER_NAME MPISPAWN_GENERIC_VALUE_19="mlx4_0" MPISPAWN_GENERIC_NAME_20=QTLIB MPISPAWN_GENERIC_VALUE_20="/usr/lib64/qt-3.3/lib" MPISPAWN_GENERIC_NAME_21=NODE_TASKS_PPN_INFO MPISPAWN_GENERIC_VALUE_21=""16,0_"" MPISPAWN_GENERIC_NAME_22=SLURM_CPUS_ON_NODE MPISPAWN_GENERIC_VALUE_22="16" MPISPAWN_GENERIC_NAME_23=PATH MPISPAWN_GENERIC_VALUE_23="/opt/apps/gcc4_7/mvapich2/1.9/bin:/opt/apps/gcc/4.7.1/bin:/opt/apps/xalt/0.4.4/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/opt/apps/xsede/gsi-openssh-5.7/bin:/usr/X11R6/bin:/opt/ofed/bin:/opt/ofed/sbin:." MPISPAWN_GENERIC_NAME_24=SLURM_TACC_JOBNAME MPISPAWN_GENERIC_VALUE_24="stampedeLargeScaleAllTS.sh" MPISPAWN_GENERIC_NAME_25=SLURM_NODEID MPISPAWN_GENERIC_VALUE_25="0" MPISPAWN_GENERIC_NAME_26=LMOD_PKG MPISPAWN_GENERIC_VALUE_26="/opt/apps/lmod/lmod" MPISPAWN_GENERIC_NAME_27=MIC_I_MPI_OFA_ADAPTER_NAME MPISPAWN_GENERIC_VALUE_27="mlx4_0" MPISPAWN_GENERIC_NAME_28=HISTSIZE MPISPAWN_GENERIC_VALUE_28="1000" MPISPAWN_GENERIC_NAME_29=SLURM_NTASKS MPISPAWN_GENERIC_VALUE_29="2048" MPISPAWN_GENERIC_NAME_30=SHLIB_PATH MPISPAWN_GENERIC_VALUE_30="/opt/apps/xsede/gsi-openssh-5.7/lib64" MPISPAWN_GENERIC_NAME_31=TACC_OVERRIDE_PROJECT MPISPAWN_GENERIC_VALUE_31="TG-ASC140020" MPISPAWN_GENERIC_NAME_32=_ModuleTable005_ MPISPAWN_GENERIC_VALUE_32="Z2NjNF83L21vZHVsZWZpbGVzIiwiL29wdC9hcHBzL3hzZWRlL21vZHVsZWZpbGVzIiwiL29wdC9hcHBzL21vZHVsZWZpbGVzIiwiL29wdC9tb2R1bGVmaWxlcyIsfSxbInN5c3RlbUJhc2VNUEFUSCJdPSIvb3B0L2FwcHMveHNlZGUvbW9kdWxlZmlsZXM6L29wdC9hcHBzL21vZHVsZWZpbGVzOi9vcHQvbW9kdWxlZmlsZXMiLFsidmVyc2lvbiJdPTIsfQ==" MPISPAWN_GENERIC_NAME_33=_ModuleTable_Sz_ MPISPAWN_GENERIC_VALUE_33="5" MPISPAWN_GENERIC_NAME_34=MAIL MPISPAWN_GENERIC_VALUE_34="/var/spool/mail/tg825207" MPISPAWN_GENERIC_NAME_35=MV2_USE_HUGEPAGES MPISPAWN_GENERIC_VALUE_35="0" MPISPAWN_GENERIC_NAME_36=SLURM_TOPOLOGY_ADDR MPISPAWN_GENERIC_VALUE_36="c468-601" MPISPAWN_GENERIC_NAME_37=TACC_XALT_DIR MPISPAWN_GENERIC_VALUE_37="/opt/apps/xalt/0.4.4/" MPISPAWN_GENERIC_NAME_38=QTDIR MPISPAWN_GENERIC_VALUE_38="/usr/lib64/qt-3.3" MPISPAWN_GENERIC_NAME_39=SYSTEM MPISPAWN_GENERIC_VALUE_39="linux" MPISPAWN_GENERIC_NAME_40=SLURM_TACC_RUNLIMIT_MINS MPISPAWN_GENERIC_VALUE_40="180" MPISPAWN_GENERIC_NAME_41=SLURM_SUBMIT_DIR MPISPAWN_GENERIC_VALUE_41="/home1/03222/tg825207/parallelnbody" MPISPAWN_GENERIC_NAME_42=LIBPATH MPISPAWN_GENERIC_VALUE_42="/opt/apps/xsede/gsi-openssh-5.7/lib64:/usr/lib:/lib" MPISPAWN_GENERIC_NAME_43=HOSTNAME MPISPAWN_GENERIC_VALUE_43="c468-601" MPISPAWN_GENERIC_NAME_44=LMOD_DEFAULT_MODULEPATH MPISPAWN_GENERIC_VALUE_44="/opt/apps/xsede/modulefiles:/opt/apps/modulefiles:/opt/modulefiles" MPISPAWN_GENERIC_NAME_45=ARCHIVE MPISPAWN_GENERIC_VALUE_45="/home/03222/tg825207" MPISPAWN_GENERIC_NAME_46=_ MPISPAWN_GENERIC_VALUE_46="/usr/local/bin/build_env.pl" MPISPAWN_GENERIC_NAME_47=__BASHRC_SOURCED__ MPISPAWN_GENERIC_VALUE_47="1" MPISPAWN_GENERIC_NAME_48=APPS MPISPAWN_GENERIC_VALUE_48="/opt/apps" MPISPAWN_GENERIC_NAME_49=SHELL MPISPAWN_GENERIC_VALUE_49="/bin/bash" MPISPAWN_GENERIC_NAME_50=ARMCI_OPENIB_DEVICE MPISPAWN_GENERIC_VALUE_50="mlx4_0" MPISPAWN_GENERIC_NAME_51=ENVIRONMENT MPISPAWN_GENERIC_VALUE_51="BATCH" MPISPAWN_GENERIC_NAME_52=TACC_FAMILY_MPI MPISPAWN_GENERIC_VALUE_52="mvapich2" MPISPAWN_GENERIC_NAME_53=MV2_IBA_HCA MPISPAWN_GENERIC_VALUE_53="mlx4_0" MPISPAWN_GENERIC_NAME_54=TACC_PUBLIC_MACHINE MPISPAWN_GENERIC_VALUE_54="1" MPISPAWN_GENERIC_NAME_55=SLURM_TACC_NODES MPISPAWN_GENERIC_VALUE_55="1" MPISPAWN_GENERIC_NAME_56=_ModuleTable003_ MPISPAWN_GENERIC_VALUE_56="e30sWyJzaG9ydCJdPSJjbHVzdGVyIixbInN0YXR1cyJdPSJhY3RpdmUiLH0sWyJjbHVzdGVyLXBhdGhzIl09e1siRk4iXT0iL29wdC9tb2R1bGVmaWxlcy9jbHVzdGVyLXBhdGhzIixbImRlZmF1bHQiXT0wLFsiZnVsbE5hbWUiXT0iY2x1c3Rlci1wYXRocyIsWyJsb2FkT3JkZXIiXT0zLHByb3BUPXt9LFsic2hvcnQiXT0iY2x1c3Rlci1wYXRocyIsWyJzdGF0dXMiXT0iYWN0aXZlIix9LGdjYz17WyJGTiJdPSIvb3B0L2FwcHMvbW9kdWxlZmlsZXMvZ2NjLzQuNy4xLmx1YSIsWyJkZWZhdWx0Il09MCxbImZ1bGxOYW1lIl09ImdjYy80LjcuMSIsWyJsb2FkT3JkZXIiXT03LHByb3BUPXt9LFsic2hvcnQiXT0iZ2NjIixbInN0YXR1cyJdPSJhY3RpdmUiLH0sbXZhcGljaDI9e1si" MPISPAWN_GENERIC_NAME_57=QTINC MPISPAWN_GENERIC_VALUE_57="/usr/lib64/qt-3.3/include" MPISPAWN_GENERIC_NAME_58=CVS_RSH MPISPAWN_GENERIC_VALUE_58="ssh" MPISPAWN_GENERIC_NAME_59=MV2_USE_OLD_BCAST MPISPAWN_GENERIC_VALUE_59="1" MPISPAWN_GENERIC_NAME_60=SLURM_JOB_NUM_NODES MPISPAWN_GENERIC_VALUE_60="128" MPISPAWN_GENERIC_NAME_61=SLURM_TASK_PID MPISPAWN_GENERIC_VALUE_61="31479" MPISPAWN_GENERIC_NAME_62=TACC_MPI_GETMODE MPISPAWN_GENERIC_VALUE_62="mvapich2_ssh" MPISPAWN_GENERIC_NAME_63=LANG MPISPAWN_GENERIC_VALUE_63="en_US.UTF-8" MPISPAWN_GENERIC_NAME_64=USER MPISPAWN_GENERIC_VALUE_64="tg825207" MPISPAWN_GENERIC_NAME_65=MIC_I_MPI_DAPL_PROVIDER MPISPAWN_GENERIC_VALUE_65="ofa-v2-mlx4_0-1u" MPISPAWN_GENERIC_NAME_66=G_BROKEN_FILENAMES MPISPAWN_GENERIC_VALUE_66="1" MPISPAWN_GENERIC_NAME_67=SLURM_LOCALID MPISPAWN_GENERIC_VALUE_67="0" MPISPAWN_GENERIC_NAME_68=SLURM_NPROCS MPISPAWN_GENERIC_VALUE_68="2048" MPISPAWN_GENERIC_NAME_69=SLURM_TACC_NNODES_SET MPISPAWN_GENERIC_VALUE_69="0" MPISPAWN_GENERIC_NAME_70=SLURM_TACC_CORES MPISPAWN_GENERIC_VALUE_70="2048" MPISPAWN_GENERIC_NAME_71=LMOD_DIR MPISPAWN_GENERIC_VALUE_71="/opt/apps/lmod/lmod/libexec" MPISPAWN_GENERIC_NAME_72=LMOD_SETTARG_CMD MPISPAWN_GENERIC_VALUE_72=":" MPISPAWN_GENERIC_NAME_73=OMP_NUM_THREADS MPISPAWN_GENERIC_VALUE_73="1" MPISPAWN_GENERIC_NAME_74=_ModuleTable004_ MPISPAWN_GENERIC_VALUE_74="Rk4iXT0iL29wdC9hcHBzL2djYzRfNy9tb2R1bGVmaWxlcy9tdmFwaWNoMi8xLjlhMiIsWyJkZWZhdWx0Il09MSxbImZ1bGxOYW1lIl09Im12YXBpY2gyLzEuOWEyIixbImxvYWRPcmRlciJdPTgscHJvcFQ9e30sWyJzaG9ydCJdPSJtdmFwaWNoMiIsWyJzdGF0dXMiXT0iYWN0aXZlIix9LHhhbHQ9e1siRk4iXT0iL29wdC9hcHBzL21vZHVsZWZpbGVzL3hhbHQvMC40LjQubHVhIixbImRlZmF1bHQiXT0xLFsiZnVsbE5hbWUiXT0ieGFsdC8wLjQuNCIsWyJsb2FkT3JkZXIiXT00LHByb3BUPXt9LFsic2hvcnQiXT0ieGFsdCIsWyJzdGF0dXMiXT0iYWN0aXZlIix9LH0sbXBhdGhBPXsiL29wdC9hcHBzL2djYzRfNy9tdmFwaWNoMl8xXzkvbW9kdWxlZmlsZXMiLCIvb3B0L2FwcHMv" MPISPAWN_GENERIC_NAME_75=__PERSONAL_PATH__ MPISPAWN_GENERIC_VALUE_75="1" MPISPAWN_GENERIC_NAME_76=LMOD_SYSTEM_DEFAULT_MODULES MPISPAWN_GENERIC_VALUE_76="TACC" MPISPAWN_GENERIC_NAME_77=SLURM_JOB_NAME MPISPAWN_GENERIC_VALUE_77="stampedeLargeScaleAllTS.sh" MPISPAWN_GENERIC_NAME_78=SLURM_SUBMIT_HOST MPISPAWN_GENERIC_VALUE_78="login4.stampede.tacc.utexas.edu" MPISPAWN_GENERIC_NAME_79=LD_LIBRARY_PATH MPISPAWN_GENERIC_VALUE_79="/opt/apps/gcc4_7/mvapich2/1.9/lib:/opt/apps/gcc4_7/mvapich2/1.9/lib/shared:/opt/apps/gcc/4.7.1/lib64:/opt/apps/gcc/4.7.1/lib:/opt/apps/xsede/gsi-openssh-5.7/lib64:/opt/apps/xsede/gsi-openssh-5.7/lib64" MPISPAWN_GENERIC_NAME_80=SLURM_QUEUE MPISPAWN_GENERIC_VALUE_80="normal" MPISPAWN_GENERIC_NAME_81=PKG_CONFIG_PATH MPISPAWN_GENERIC_VALUE_81="/opt/apps/gcc4_7/mvapich2/1.9/lib/pkgconfig" MPISPAWN_GENERIC_NAME_82=LMOD_PREPEND_BLOCK MPISPAWN_GENERIC_VALUE_82="normal" MPISPAWN_GENERIC_NAME_83=TACC_FAMILY_COMPILER MPISPAWN_GENERIC_VALUE_83="gcc" MPISPAWN_GENERIC_NAME_84=TACC_DOMAIN MPISPAWN_GENERIC_VALUE_84="stampede" MPISPAWN_GENERIC_NAME_85=MV2_HOMOGENEOUS_CLUSTER MPISPAWN_GENERIC_VALUE_85="1" MPISPAWN_GENERIC_NAME_86=LMOD_FAMILY_COMPILER MPISPAWN_GENERIC_VALUE_86="gcc" MPISPAWN_GENERIC_NAME_87=MV2_USE_UD_HYBRID MPISPAWN_GENERIC_VALUE_87="0" MPISPAWN_GENERIC_NAME_88=MPICH_HOME MPISPAWN_GENERIC_VALUE_88="/opt/apps/gcc4_7/mvapich2/1.9" MPISPAWN_GENERIC_NAME_89=SLURM_JOB_NODELIST MPISPAWN_GENERIC_VALUE_89="c468-601,c471-503,c472-[401-402],c473-[303-304,401,804,901],c474-[203-204,301,401,403-404],c478-303,c482-404,c483-[001-004,101,504,601,804,901],c484-[101,103-104,201,602,701],c485-502,c486-102,c490-[302,904],c491-004,c492-701,c493-[001,102,201],c494-[101,402-403,602,702-703,904],c495-[001-002,201],c497-[102,202,504,601-603],c498-[304,402,603],c501-[204,301,604,801],c502-104,c503-901,c504-[602,604,702,801],c505-[001,202-204,401,502-504,802,901-904],c506-[302,704],c507-[102,202,503,601,904],c508-[101,204,601,702-703,802,903-904],c509-[002,201,301,303,401,501-502],c510-[003,401,404,501,503],c511-[601-602,902-903],c536-[801,902],c537-902,c538-[001,003,104],c540-[104,201-202,804,901-904]" MPISPAWN_GENERIC_NAME_90=SLURM_PROCID MPISPAWN_GENERIC_VALUE_90="0" MPISPAWN_GENERIC_NAME_91=I_MPI_DAPL_PROVIDER MPISPAWN_GENERIC_VALUE_91="ofa-v2-mlx4_0-1u" MPISPAWN_GENERIC_NAME_92=GLOBUS_LOCATION MPISPAWN_GENERIC_VALUE_92="/opt/apps/xsede/gsi-openssh-5.7" MPISPAWN_GENERIC_NAME_93=LMOD_COLORIZE MPISPAWN_GENERIC_VALUE_93="yes" MPISPAWN_GENERIC_NAME_94=I_MPI_HYDRA_BRANCH_COUNT MPISPAWN_GENERIC_VALUE_94="6000" MPISPAWN_GENERIC_NAME_95=SLURM_NNODES MPISPAWN_GENERIC_VALUE_95="128" MPISPAWN_GENERIC_NAME_96=MV2_DEFAULT_TIME_OUT MPISPAWN_GENERIC_VALUE_96="23" MPISPAWN_GENERIC_NAME_97=SLURM_TACC_NCORES_SET MPISPAWN_GENERIC_VALUE_97="1" MPISPAWN_GENERIC_NAME_98=__Init_Default_Modules MPISPAWN_GENERIC_VALUE_98="1" MPISPAWN_GENERIC_NAME_99=BASH_ENV MPISPAWN_GENERIC_VALUE_99="/etc/tacc/tacc_functions" MPISPAWN_GENERIC_NAME_100=SLURM_TOPOLOGY_ADDR_PATTERN MPISPAWN_GENERIC_VALUE_100="node" MPISPAWN_GENERIC_NAME_101=MV2_USE_RING_STARTUP MPISPAWN_GENERIC_VALUE_101="0" MPISPAWN_GENERIC_NAME_102=TACC_XALT_BIN MPISPAWN_GENERIC_VALUE_102="/opt/apps/xalt/0.4.4/bin" MPISPAWN_GENERIC_NAME_103=X509_USER_PROXY MPISPAWN_GENERIC_VALUE_103="/tmp/x509up_p3084.file9zM3fu.1" MPISPAWN_GENERIC_NAME_104=TMPDIR MPISPAWN_GENERIC_VALUE_104="/tmp" MPISPAWN_GENERIC_NAME_105=LMOD_FULL_SETTARG_SUPPORT MPISPAWN_GENERIC_VALUE_105="no" MPISPAWN_ID=42 MPISPAWN_WORKING_DIR=/home1/03222/tg825207/parallelnbody MPISPAWN_MPIRUN_RANK_0=672 MPISPAWN_MPIRUN_RANK_1=673 MPISPAWN_MPIRUN_RANK_2=674 MPISPAWN_MPIRUN_RANK_3=675 MPISPAWN_MPIRUN_RANK_4=676 MPISPAWN_MPIRUN_RANK_5=677 MPISPAWN_MPIRUN_RANK_6=678 MPISPAWN_MPIRUN_RANK_7=679 MPISPAWN_MPIRUN_RANK_8=680 MPISPAWN_MPIRUN_RANK_9=681 MPISPAWN_MPIRUN_RANK_10=682 MPISPAWN_MPIRUN_RANK_11=683 MPISPAWN_MPIRUN_RANK_12=684 MPISPAWN_MPIRUN_RANK_13=685 MPISPAWN_MPIRUN_RANK_14=686 MPISPAWN_MPIRUN_RANK_15=687 /opt/apps/gcc4_7/mvapich2/1.9/bin/mpispawn 0
[c483-001.stampede.tacc.utexas.edu:mpispawn_17][error_sighandler] Caught error: Segmentation fault (signal 11)
[c483-004.stampede.tacc.utexas.edu:mpispawn_20][error_sighandler] Caught error: Segmentation fault (signal 11)
bash: line 1: 10484 Segmentation fault      /usr/bin/env LD_LIBRARY_PATH=/opt/apps/gcc4_7/mvapich2/1.9/lib:/opt/apps/gcc4_7/mvapich2/1.9/lib/shared:/opt/apps/gcc/4.7.1/lib64:/opt/apps/gcc/4.7.1/lib:/opt/apps/xsede/gsi-openssh-5.7/lib64:/opt/apps/xsede/gsi-openssh-5.7/lib64 MPISPAWN_MPIRUN_MPD=0 USE_LINEAR_SSH=1 MPISPAWN_MPIRUN_HOST=c468-601.stampede.tacc.utexas.edu MPISPAWN_MPIRUN_HOSTIP=129.114.76.165 MPIRUN_RSH_LAUNCH=1 MPISPAWN_CHECKIN_PORT=56630 MPISPAWN_MPIRUN_PORT=56630 MPISPAWN_NNODES=128 MPISPAWN_GLOBAL_NPROCS=2048 MPISPAWN_MPIRUN_ID=47451 MPISPAWN_ARGC=4 MPDMAN_KVS_TEMPLATE=kvs_437_c468-601.stampede.tacc.utexas.edu_47451 MPISPAWN_LOCAL_NPROCS=16 MPISPAWN_ARGV_0='/home1/03222/tg825207/parallelnbody/teamscatter' MPISPAWN_ARGV_1='256000' MPISPAWN_ARGV_2='-c' MPISPAWN_ARGV_3='2' MPISPAWN_ARGC=4 MPISPAWN_GENERIC_ENV_COUNT=106 MPISPAWN_GENERIC_NAME_0=TACC_ENVLEN MPISPAWN_GENERIC_VALUE_0=6578 MPISPAWN_GENERIC_NAME_1=SLURM_JOB_ID MPISPAWN_GENERIC_VALUE_1="4306264" MPISPAWN_GENERIC_NAME_2=SLURMD_NODENAME MPISPAWN_GENERIC_VALUE_2="c468-601" MPISPAWN_GENERIC_NAME_3=STOCKYARD MPISPAWN_GENERIC_VALUE_3="/work/03222/tg825207" MPISPAWN_GENERIC_NAME_4=LC_ALL MPISPAWN_GENERIC_VALUE_4="en_US.UTF-8" MPISPAWN_GENERIC_NAME_5=SLURM_CHECKPOINT_IMAGE_DIR MPISPAWN_GENERIC_VALUE_5="/home1/03222/tg825207/parallelnbody" MPISPAWN_GENERIC_NAME_6=LMOD_FAMILY_MPI MPISPAWN_GENERIC_VALUE_6="mvapich2" MPISPAWN_GENERIC_NAME_7=SLURM_JOBID MPISPAWN_GENERIC_VALUE_7="4306264" MPISPAWN_GENERIC_NAME_8=_ModuleTable001_ MPISPAWN_GENERIC_VALUE_8="X01vZHVsZVRhYmxlXz17WyJhY3RpdmVTaXplIl09OCxiYXNlTXBhdGhBPXsiL29wdC9hcHBzL3hzZWRlL21vZHVsZWZpbGVzIiwiL29wdC9hcHBzL21vZHVsZWZpbGVzIiwiL29wdC9tb2R1bGVmaWxlcyIsfSxbImNfcmVidWlsZFRpbWUiXT1mYWxzZSxbImNfc2hvcnRUaW1lIl09ZmFsc2UsZmFtaWx5PXtbIk1QSSJdPSJtdmFwaWNoMiIsWyJjb21waWxlciJdPSJnY2MiLH0saW5hY3RpdmU9e30sbVQ9e0xpbnV4PXtbIkZOIl09Ii9vcHQvbW9kdWxlZmlsZXMvTGludXgiLFsiZGVmYXVsdCJdPTAsWyJmdWxsTmFtZSJdPSJMaW51eCIsWyJsb2FkT3JkZXIiXT0yLHByb3BUPXt9LFsic2hvcnQiXT0iTGludXgiLFsic3RhdHVzIl09ImFjdGl2ZSIsfSxUQUNDPXtbIkZOIl09Ii9v" MPISPAWN_GENERIC_NAME_9=TACC_SYSTEM MPISPAWN_GENERIC_VALUE_9="stampede" MPISPAWN_GENERIC_NAME_10=WORK MPISPAWN_GENERIC_VALUE_10="/work/03222/tg825207" MPISPAWN_GENERIC_NAME_11=SLURM_GTIDS MPISPAWN_GENERIC_VALUE_11="0" MPISPAWN_GENERIC_NAME_12=GCC_LIB MPISPAWN_GENERIC_VALUE_12="/opt/apps/gcc/4.7.1/lib64" MPISPAWN_GENERIC_NAME_13=SLURM_PRIO_PROCESS MPISPAWN_GENERIC_VALUE_13="0" MPISPAWN_GENERIC_NAME_14=SCRATCH MPISPAWN_GENERIC_VALUE_14="/scratch/03222/tg825207" MPISPAWN_GENERIC_NAME_15=LOGNAME MPISPAWN_GENERIC_VALUE_15="tg825207" MPISPAWN_GENERIC_NAME_16=_ModuleTable002_ MPISPAWN_GENERIC_VALUE_16="cHQvbW9kdWxlZmlsZXMvVEFDQyIsWyJkZWZhdWx0Il09MCxbImZ1bGxOYW1lIl09IlRBQ0MiLFsibG9hZE9yZGVyIl09Nixwcm9wVD17fSxbInNob3J0Il09IlRBQ0MiLFsic3RhdHVzIl09ImFjdGl2ZSIsfSxbIlRBQ0MtcGF0aHMiXT17WyJGTiJdPSIvb3B0L21vZHVsZWZpbGVzL1RBQ0MtcGF0aHMiLFsiZGVmYXVsdCJdPTAsWyJmdWxsTmFtZSJdPSJUQUNDLXBhdGhzIixbImxvYWRPcmRlciJdPTEscHJvcFQ9e30sWyJzaG9ydCJdPSJUQUNDLXBhdGhzIixbInN0YXR1cyJdPSJhY3RpdmUiLH0sY2x1c3Rlcj17WyJGTiJdPSIvb3B0L21vZHVsZWZpbGVzL2NsdXN0ZXIiLFsiZGVmYXVsdCJdPTAsWyJmdWxsTmFtZSJdPSJjbHVzdGVyIixbImxvYWRPcmRlciJdPTUscHJvcFQ9" MPISPAWN_GENERIC_NAME_17=INPUTRC MPISPAWN_GENERIC_VALUE_17="/etc/inputrc" MPISPAWN_GENERIC_NAME_18=LMOD_CMD MPISPAWN_GENERIC_VALUE_18="/opt/apps/lmod/lmod/libexec/lmod" MPISPAWN_GENERIC_NAME_19=I_MPI_OFA_ADAPTER_NAME MPISPAWN_GENERIC_VALUE_19="mlx4_0" MPISPAWN_GENERIC_NAME_20=QTLIB MPISPAWN_GENERIC_VALUE_20="/usr/lib64/qt-3.3/lib" MPISPAWN_GENERIC_NAME_21=NODE_TASKS_PPN_INFO MPISPAWN_GENERIC_VALUE_21=""16,0_"" MPISPAWN_GENERIC_NAME_22=SLURM_CPUS_ON_NODE MPISPAWN_GENERIC_VALUE_22="16" MPISPAWN_GENERIC_NAME_23=PATH MPISPAWN_GENERIC_VALUE_23="/opt/apps/gcc4_7/mvapich2/1.9/bin:/opt/apps/gcc/4.7.1/bin:/opt/apps/xalt/0.4.4/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/opt/apps/xsede/gsi-openssh-5.7/bin:/usr/X11R6/bin:/opt/ofed/bin:/opt/ofed/sbin:." MPISPAWN_GENERIC_NAME_24=SLURM_TACC_JOBNAME MPISPAWN_GENERIC_VALUE_24="stampedeLargeScaleAllTS.sh" MPISPAWN_GENERIC_NAME_25=SLURM_NODEID MPISPAWN_GENERIC_VALUE_25="0" MPISPAWN_GENERIC_NAME_26=LMOD_PKG MPISPAWN_GENERIC_VALUE_26="/opt/apps/lmod/lmod" MPISPAWN_GENERIC_NAME_27=MIC_I_MPI_OFA_ADAPTER_NAME MPISPAWN_GENERIC_VALUE_27="mlx4_0" MPISPAWN_GENERIC_NAME_28=HISTSIZE MPISPAWN_GENERIC_VALUE_28="1000" MPISPAWN_GENERIC_NAME_29=SLURM_NTASKS MPISPAWN_GENERIC_VALUE_29="2048" MPISPAWN_GENERIC_NAME_30=SHLIB_PATH MPISPAWN_GENERIC_VALUE_30="/opt/apps/xsede/gsi-openssh-5.7/lib64" MPISPAWN_GENERIC_NAME_31=TACC_OVERRIDE_PROJECT MPISPAWN_GENERIC_VALUE_31="TG-ASC140020" MPISPAWN_GENERIC_NAME_32=_ModuleTable005_ MPISPAWN_GENERIC_VALUE_32="Z2NjNF83L21vZHVsZWZpbGVzIiwiL29wdC9hcHBzL3hzZWRlL21vZHVsZWZpbGVzIiwiL29wdC9hcHBzL21vZHVsZWZpbGVzIiwiL29wdC9tb2R1bGVmaWxlcyIsfSxbInN5c3RlbUJhc2VNUEFUSCJdPSIvb3B0L2FwcHMveHNlZGUvbW9kdWxlZmlsZXM6L29wdC9hcHBzL21vZHVsZWZpbGVzOi9vcHQvbW9kdWxlZmlsZXMiLFsidmVyc2lvbiJdPTIsfQ==" MPISPAWN_GENERIC_NAME_33=_ModuleTable_Sz_ MPISPAWN_GENERIC_VALUE_33="5" MPISPAWN_GENERIC_NAME_34=MAIL MPISPAWN_GENERIC_VALUE_34="/var/spool/mail/tg825207" MPISPAWN_GENERIC_NAME_35=MV2_USE_HUGEPAGES MPISPAWN_GENERIC_VALUE_35="0" MPISPAWN_GENERIC_NAME_36=SLURM_TOPOLOGY_ADDR MPISPAWN_GENERIC_VALUE_36="c468-601" MPISPAWN_GENERIC_NAME_37=TACC_XALT_DIR MPISPAWN_GENERIC_VALUE_37="/opt/apps/xalt/0.4.4/" MPISPAWN_GENERIC_NAME_38=QTDIR MPISPAWN_GENERIC_VALUE_38="/usr/lib64/qt-3.3" MPISPAWN_GENERIC_NAME_39=SYSTEM MPISPAWN_GENERIC_VALUE_39="linux" MPISPAWN_GENERIC_NAME_40=SLURM_TACC_RUNLIMIT_MINS MPISPAWN_GENERIC_VALUE_40="180" MPISPAWN_GENERIC_NAME_41=SLURM_SUBMIT_DIR MPISPAWN_GENERIC_VALUE_41="/home1/03222/tg825207/parallelnbody" MPISPAWN_GENERIC_NAME_42=LIBPATH MPISPAWN_GENERIC_VALUE_42="/opt/apps/xsede/gsi-openssh-5.7/lib64:/usr/lib:/lib" MPISPAWN_GENERIC_NAME_43=HOSTNAME MPISPAWN_GENERIC_VALUE_43="c468-601" MPISPAWN_GENERIC_NAME_44=LMOD_DEFAULT_MODULEPATH MPISPAWN_GENERIC_VALUE_44="/opt/apps/xsede/modulefiles:/opt/apps/modulefiles:/opt/modulefiles" MPISPAWN_GENERIC_NAME_45=ARCHIVE MPISPAWN_GENERIC_VALUE_45="/home/03222/tg825207" MPISPAWN_GENERIC_NAME_46=_ MPISPAWN_GENERIC_VALUE_46="/usr/local/bin/build_env.pl" MPISPAWN_GENERIC_NAME_47=__BASHRC_SOURCED__ MPISPAWN_GENERIC_VALUE_47="1" MPISPAWN_GENERIC_NAME_48=APPS MPISPAWN_GENERIC_VALUE_48="/opt/apps" MPISPAWN_GENERIC_NAME_49=SHELL MPISPAWN_GENERIC_VALUE_49="/bin/bash" MPISPAWN_GENERIC_NAME_50=ARMCI_OPENIB_DEVICE MPISPAWN_GENERIC_VALUE_50="mlx4_0" MPISPAWN_GENERIC_NAME_51=ENVIRONMENT MPISPAWN_GENERIC_VALUE_51="BATCH" MPISPAWN_GENERIC_NAME_52=TACC_FAMILY_MPI MPISPAWN_GENERIC_VALUE_52="mvapich2" MPISPAWN_GENERIC_NAME_53=MV2_IBA_HCA MPISPAWN_GENERIC_VALUE_53="mlx4_0" MPISPAWN_GENERIC_NAME_54=TACC_PUBLIC_MACHINE MPISPAWN_GENERIC_VALUE_54="1" MPISPAWN_GENERIC_NAME_55=SLURM_TACC_NODES MPISPAWN_GENERIC_VALUE_55="1" MPISPAWN_GENERIC_NAME_56=_ModuleTable003_ MPISPAWN_GENERIC_VALUE_56="e30sWyJzaG9ydCJdPSJjbHVzdGVyIixbInN0YXR1cyJdPSJhY3RpdmUiLH0sWyJjbHVzdGVyLXBhdGhzIl09e1siRk4iXT0iL29wdC9tb2R1bGVmaWxlcy9jbHVzdGVyLXBhdGhzIixbImRlZmF1bHQiXT0wLFsiZnVsbE5hbWUiXT0iY2x1c3Rlci1wYXRocyIsWyJsb2FkT3JkZXIiXT0zLHByb3BUPXt9LFsic2hvcnQiXT0iY2x1c3Rlci1wYXRocyIsWyJzdGF0dXMiXT0iYWN0aXZlIix9LGdjYz17WyJGTiJdPSIvb3B0L2FwcHMvbW9kdWxlZmlsZXMvZ2NjLzQuNy4xLmx1YSIsWyJkZWZhdWx0Il09MCxbImZ1bGxOYW1lIl09ImdjYy80LjcuMSIsWyJsb2FkT3JkZXIiXT03LHByb3BUPXt9LFsic2hvcnQiXT0iZ2NjIixbInN0YXR1cyJdPSJhY3RpdmUiLH0sbXZhcGljaDI9e1si" MPISPAWN_GENERIC_NAME_57=QTINC MPISPAWN_GENERIC_VALUE_57="/usr/lib64/qt-3.3/include" MPISPAWN_GENERIC_NAME_58=CVS_RSH MPISPAWN_GENERIC_VALUE_58="ssh" MPISPAWN_GENERIC_NAME_59=MV2_USE_OLD_BCAST MPISPAWN_GENERIC_VALUE_59="1" MPISPAWN_GENERIC_NAME_60=SLURM_JOB_NUM_NODES MPISPAWN_GENERIC_VALUE_60="128" MPISPAWN_GENERIC_NAME_61=SLURM_TASK_PID MPISPAWN_GENERIC_VALUE_61="31479" MPISPAWN_GENERIC_NAME_62=TACC_MPI_GETMODE MPISPAWN_GENERIC_VALUE_62="mvapich2_ssh" MPISPAWN_GENERIC_NAME_63=LANG MPISPAWN_GENERIC_VALUE_63="en_US.UTF-8" MPISPAWN_GENERIC_NAME_64=USER MPISPAWN_GENERIC_VALUE_64="tg825207" MPISPAWN_GENERIC_NAME_65=MIC_I_MPI_DAPL_PROVIDER MPISPAWN_GENERIC_VALUE_65="ofa-v2-mlx4_0-1u" MPISPAWN_GENERIC_NAME_66=G_BROKEN_FILENAMES MPISPAWN_GENERIC_VALUE_66="1" MPISPAWN_GENERIC_NAME_67=SLURM_LOCALID MPISPAWN_GENERIC_VALUE_67="0" MPISPAWN_GENERIC_NAME_68=SLURM_NPROCS MPISPAWN_GENERIC_VALUE_68="2048" MPISPAWN_GENERIC_NAME_69=SLURM_TACC_NNODES_SET MPISPAWN_GENERIC_VALUE_69="0" MPISPAWN_GENERIC_NAME_70=SLURM_TACC_CORES MPISPAWN_GENERIC_VALUE_70="2048" MPISPAWN_GENERIC_NAME_71=LMOD_DIR MPISPAWN_GENERIC_VALUE_71="/opt/apps/lmod/lmod/libexec" MPISPAWN_GENERIC_NAME_72=LMOD_SETTARG_CMD MPISPAWN_GENERIC_VALUE_72=":" MPISPAWN_GENERIC_NAME_73=OMP_NUM_THREADS MPISPAWN_GENERIC_VALUE_73="1" MPISPAWN_GENERIC_NAME_74=_ModuleTable004_ MPISPAWN_GENERIC_VALUE_74="Rk4iXT0iL29wdC9hcHBzL2djYzRfNy9tb2R1bGVmaWxlcy9tdmFwaWNoMi8xLjlhMiIsWyJkZWZhdWx0Il09MSxbImZ1bGxOYW1lIl09Im12YXBpY2gyLzEuOWEyIixbImxvYWRPcmRlciJdPTgscHJvcFQ9e30sWyJzaG9ydCJdPSJtdmFwaWNoMiIsWyJzdGF0dXMiXT0iYWN0aXZlIix9LHhhbHQ9e1siRk4iXT0iL29wdC9hcHBzL21vZHVsZWZpbGVzL3hhbHQvMC40LjQubHVhIixbImRlZmF1bHQiXT0xLFsiZnVsbE5hbWUiXT0ieGFsdC8wLjQuNCIsWyJsb2FkT3JkZXIiXT00LHByb3BUPXt9LFsic2hvcnQiXT0ieGFsdCIsWyJzdGF0dXMiXT0iYWN0aXZlIix9LH0sbXBhdGhBPXsiL29wdC9hcHBzL2djYzRfNy9tdmFwaWNoMl8xXzkvbW9kdWxlZmlsZXMiLCIvb3B0L2FwcHMv" MPISPAWN_GENERIC_NAME_75=__PERSONAL_PATH__ MPISPAWN_GENERIC_VALUE_75="1" MPISPAWN_GENERIC_NAME_76=LMOD_SYSTEM_DEFAULT_MODULES MPISPAWN_GENERIC_VALUE_76="TACC" MPISPAWN_GENERIC_NAME_77=SLURM_JOB_NAME MPISPAWN_GENERIC_VALUE_77="stampedeLargeScaleAllTS.sh" MPISPAWN_GENERIC_NAME_78=SLURM_SUBMIT_HOST MPISPAWN_GENERIC_VALUE_78="login4.stampede.tacc.utexas.edu" MPISPAWN_GENERIC_NAME_79=LD_LIBRARY_PATH MPISPAWN_GENERIC_VALUE_79="/opt/apps/gcc4_7/mvapich2/1.9/lib:/opt/apps/gcc4_7/mvapich2/1.9/lib/shared:/opt/apps/gcc/4.7.1/lib64:/opt/apps/gcc/4.7.1/lib:/opt/apps/xsede/gsi-openssh-5.7/lib64:/opt/apps/xsede/gsi-openssh-5.7/lib64" MPISPAWN_GENERIC_NAME_80=SLURM_QUEUE MPISPAWN_GENERIC_VALUE_80="normal" MPISPAWN_GENERIC_NAME_81=PKG_CONFIG_PATH MPISPAWN_GENERIC_VALUE_81="/opt/apps/gcc4_7/mvapich2/1.9/lib/pkgconfig" MPISPAWN_GENERIC_NAME_82=LMOD_PREPEND_BLOCK MPISPAWN_GENERIC_VALUE_82="normal" MPISPAWN_GENERIC_NAME_83=TACC_FAMILY_COMPILER MPISPAWN_GENERIC_VALUE_83="gcc" MPISPAWN_GENERIC_NAME_84=TACC_DOMAIN MPISPAWN_GENERIC_VALUE_84="stampede" MPISPAWN_GENERIC_NAME_85=MV2_HOMOGENEOUS_CLUSTER MPISPAWN_GENERIC_VALUE_85="1" MPISPAWN_GENERIC_NAME_86=LMOD_FAMILY_COMPILER MPISPAWN_GENERIC_VALUE_86="gcc" MPISPAWN_GENERIC_NAME_87=MV2_USE_UD_HYBRID MPISPAWN_GENERIC_VALUE_87="0" MPISPAWN_GENERIC_NAME_88=MPICH_HOME MPISPAWN_GENERIC_VALUE_88="/opt/apps/gcc4_7/mvapich2/1.9" MPISPAWN_GENERIC_NAME_89=SLURM_JOB_NODELIST MPISPAWN_GENERIC_VALUE_89="c468-601,c471-503,c472-[401-402],c473-[303-304,401,804,901],c474-[203-204,301,401,403-404],c478-303,c482-404,c483-[001-004,101,504,601,804,901],c484-[101,103-104,201,602,701],c485-502,c486-102,c490-[302,904],c491-004,c492-701,c493-[001,102,201],c494-[101,402-403,602,702-703,904],c495-[001-002,201],c497-[102,202,504,601-603],c498-[304,402,603],c501-[204,301,604,801],c502-104,c503-901,c504-[602,604,702,801],c505-[001,202-204,401,502-504,802,901-904],c506-[302,704],c507-[102,202,503,601,904],c508-[101,204,601,702-703,802,903-904],c509-[002,201,301,303,401,501-502],c510-[003,401,404,501,503],c511-[601-602,902-903],c536-[801,902],c537-902,c538-[001,003,104],c540-[104,201-202,804,901-904]" MPISPAWN_GENERIC_NAME_90=SLURM_PROCID MPISPAWN_GENERIC_VALUE_90="0" MPISPAWN_GENERIC_NAME_91=I_MPI_DAPL_PROVIDER MPISPAWN_GENERIC_VALUE_91="ofa-v2-mlx4_0-1u" MPISPAWN_GENERIC_NAME_92=GLOBUS_LOCATION MPISPAWN_GENERIC_VALUE_92="/opt/apps/xsede/gsi-openssh-5.7" MPISPAWN_GENERIC_NAME_93=LMOD_COLORIZE MPISPAWN_GENERIC_VALUE_93="yes" MPISPAWN_GENERIC_NAME_94=I_MPI_HYDRA_BRANCH_COUNT MPISPAWN_GENERIC_VALUE_94="6000" MPISPAWN_GENERIC_NAME_95=SLURM_NNODES MPISPAWN_GENERIC_VALUE_95="128" MPISPAWN_GENERIC_NAME_96=MV2_DEFAULT_TIME_OUT MPISPAWN_GENERIC_VALUE_96="23" MPISPAWN_GENERIC_NAME_97=SLURM_TACC_NCORES_SET MPISPAWN_GENERIC_VALUE_97="1" MPISPAWN_GENERIC_NAME_98=__Init_Default_Modules MPISPAWN_GENERIC_VALUE_98="1" MPISPAWN_GENERIC_NAME_99=BASH_ENV MPISPAWN_GENERIC_VALUE_99="/etc/tacc/tacc_functions" MPISPAWN_GENERIC_NAME_100=SLURM_TOPOLOGY_ADDR_PATTERN MPISPAWN_GENERIC_VALUE_100="node" MPISPAWN_GENERIC_NAME_101=MV2_USE_RING_STARTUP MPISPAWN_GENERIC_VALUE_101="0" MPISPAWN_GENERIC_NAME_102=TACC_XALT_BIN MPISPAWN_GENERIC_VALUE_102="/opt/apps/xalt/0.4.4/bin" MPISPAWN_GENERIC_NAME_103=X509_USER_PROXY MPISPAWN_GENERIC_VALUE_103="/tmp/x509up_p3084.file9zM3fu.1" MPISPAWN_GENERIC_NAME_104=TMPDIR MPISPAWN_GENERIC_VALUE_104="/tmp" MPISPAWN_GENERIC_NAME_105=LMOD_FULL_SETTARG_SUPPORT MPISPAWN_GENERIC_VALUE_105="no" MPISPAWN_ID=20 MPISPAWN_WORKING_DIR=/home1/03222/tg825207/parallelnbody MPISPAWN_MPIRUN_RANK_0=320 MPISPAWN_MPIRUN_RANK_1=321 MPISPAWN_MPIRUN_RANK_2=322 MPISPAWN_MPIRUN_RANK_3=323 MPISPAWN_MPIRUN_RANK_4=324 MPISPAWN_MPIRUN_RANK_5=325 MPISPAWN_MPIRUN_RANK_6=326 MPISPAWN_MPIRUN_RANK_7=327 MPISPAWN_MPIRUN_RANK_8=328 MPISPAWN_MPIRUN_RANK_9=329 MPISPAWN_MPIRUN_RANK_10=330 MPISPAWN_MPIRUN_RANK_11=331 MPISPAWN_MPIRUN_RANK_12=332 MPISPAWN_MPIRUN_RANK_13=333 MPISPAWN_MPIRUN_RANK_14=334 MPISPAWN_MPIRUN_RANK_15=335 /opt/apps/gcc4_7/mvapich2/1.9/bin/mpispawn 0
bash: line 1: 19645 Segmentation fault      /usr/bin/env LD_LIBRARY_PATH=/opt/apps/gcc4_7/mvapich2/1.9/lib:/opt/apps/gcc4_7/mvapich2/1.9/lib/shared:/opt/apps/gcc/4.7.1/lib64:/opt/apps/gcc/4.7.1/lib:/opt/apps/xsede/gsi-openssh-5.7/lib64:/opt/apps/xsede/gsi-openssh-5.7/lib64 MPISPAWN_MPIRUN_MPD=0 USE_LINEAR_SSH=1 MPISPAWN_MPIRUN_HOST=c468-601.stampede.tacc.utexas.edu MPISPAWN_MPIRUN_HOSTIP=129.114.76.165 MPIRUN_RSH_LAUNCH=1 MPISPAWN_CHECKIN_PORT=56630 MPISPAWN_MPIRUN_PORT=56630 MPISPAWN_NNODES=128 MPISPAWN_GLOBAL_NPROCS=2048 MPISPAWN_MPIRUN_ID=47451 MPISPAWN_ARGC=4 MPDMAN_KVS_TEMPLATE=kvs_437_c468-601.stampede.tacc.utexas.edu_47451 MPISPAWN_LOCAL_NPROCS=16 MPISPAWN_ARGV_0='/home1/03222/tg825207/parallelnbody/teamscatter' MPISPAWN_ARGV_1='256000' MPISPAWN_ARGV_2='-c' MPISPAWN_ARGV_3='2' MPISPAWN_ARGC=4 MPISPAWN_GENERIC_ENV_COUNT=106 MPISPAWN_GENERIC_NAME_0=TACC_ENVLEN MPISPAWN_GENERIC_VALUE_0=6578 MPISPAWN_GENERIC_NAME_1=SLURM_JOB_ID MPISPAWN_GENERIC_VALUE_1="4306264" MPISPAWN_GENERIC_NAME_2=SLURMD_NODENAME MPISPAWN_GENERIC_VALUE_2="c468-601" MPISPAWN_GENERIC_NAME_3=STOCKYARD MPISPAWN_GENERIC_VALUE_3="/work/03222/tg825207" MPISPAWN_GENERIC_NAME_4=LC_ALL MPISPAWN_GENERIC_VALUE_4="en_US.UTF-8" MPISPAWN_GENERIC_NAME_5=SLURM_CHECKPOINT_IMAGE_DIR MPISPAWN_GENERIC_VALUE_5="/home1/03222/tg825207/parallelnbody" MPISPAWN_GENERIC_NAME_6=LMOD_FAMILY_MPI MPISPAWN_GENERIC_VALUE_6="mvapich2" MPISPAWN_GENERIC_NAME_7=SLURM_JOBID MPISPAWN_GENERIC_VALUE_7="4306264" MPISPAWN_GENERIC_NAME_8=_ModuleTable001_ MPISPAWN_GENERIC_VALUE_8="X01vZHVsZVRhYmxlXz17WyJhY3RpdmVTaXplIl09OCxiYXNlTXBhdGhBPXsiL29wdC9hcHBzL3hzZWRlL21vZHVsZWZpbGVzIiwiL29wdC9hcHBzL21vZHVsZWZpbGVzIiwiL29wdC9tb2R1bGVmaWxlcyIsfSxbImNfcmVidWlsZFRpbWUiXT1mYWxzZSxbImNfc2hvcnRUaW1lIl09ZmFsc2UsZmFtaWx5PXtbIk1QSSJdPSJtdmFwaWNoMiIsWyJjb21waWxlciJdPSJnY2MiLH0saW5hY3RpdmU9e30sbVQ9e0xpbnV4PXtbIkZOIl09Ii9vcHQvbW9kdWxlZmlsZXMvTGludXgiLFsiZGVmYXVsdCJdPTAsWyJmdWxsTmFtZSJdPSJMaW51eCIsWyJsb2FkT3JkZXIiXT0yLHByb3BUPXt9LFsic2hvcnQiXT0iTGludXgiLFsic3RhdHVzIl09ImFjdGl2ZSIsfSxUQUNDPXtbIkZOIl09Ii9v" MPISPAWN_GENERIC_NAME_9=TACC_SYSTEM MPISPAWN_GENERIC_VALUE_9="stampede" MPISPAWN_GENERIC_NAME_10=WORK MPISPAWN_GENERIC_VALUE_10="/work/03222/tg825207" MPISPAWN_GENERIC_NAME_11=SLURM_GTIDS MPISPAWN_GENERIC_VALUE_11="0" MPISPAWN_GENERIC_NAME_12=GCC_LIB MPISPAWN_GENERIC_VALUE_12="/opt/apps/gcc/4.7.1/lib64" MPISPAWN_GENERIC_NAME_13=SLURM_PRIO_PROCESS MPISPAWN_GENERIC_VALUE_13="0" MPISPAWN_GENERIC_NAME_14=SCRATCH MPISPAWN_GENERIC_VALUE_14="/scratch/03222/tg825207" MPISPAWN_GENERIC_NAME_15=LOGNAME MPISPAWN_GENERIC_VALUE_15="tg825207" MPISPAWN_GENERIC_NAME_16=_ModuleTable002_ MPISPAWN_GENERIC_VALUE_16="cHQvbW9kdWxlZmlsZXMvVEFDQyIsWyJkZWZhdWx0Il09MCxbImZ1bGxOYW1lIl09IlRBQ0MiLFsibG9hZE9yZGVyIl09Nixwcm9wVD17fSxbInNob3J0Il09IlRBQ0MiLFsic3RhdHVzIl09ImFjdGl2ZSIsfSxbIlRBQ0MtcGF0aHMiXT17WyJGTiJdPSIvb3B0L21vZHVsZWZpbGVzL1RBQ0MtcGF0aHMiLFsiZGVmYXVsdCJdPTAsWyJmdWxsTmFtZSJdPSJUQUNDLXBhdGhzIixbImxvYWRPcmRlciJdPTEscHJvcFQ9e30sWyJzaG9ydCJdPSJUQUNDLXBhdGhzIixbInN0YXR1cyJdPSJhY3RpdmUiLH0sY2x1c3Rlcj17WyJGTiJdPSIvb3B0L21vZHVsZWZpbGVzL2NsdXN0ZXIiLFsiZGVmYXVsdCJdPTAsWyJmdWxsTmFtZSJdPSJjbHVzdGVyIixbImxvYWRPcmRlciJdPTUscHJvcFQ9" MPISPAWN_GENERIC_NAME_17=INPUTRC MPISPAWN_GENERIC_VALUE_17="/etc/inputrc" MPISPAWN_GENERIC_NAME_18=LMOD_CMD MPISPAWN_GENERIC_VALUE_18="/opt/apps/lmod/lmod/libexec/lmod" MPISPAWN_GENERIC_NAME_19=I_MPI_OFA_ADAPTER_NAME MPISPAWN_GENERIC_VALUE_19="mlx4_0" MPISPAWN_GENERIC_NAME_20=QTLIB MPISPAWN_GENERIC_VALUE_20="/usr/lib64/qt-3.3/lib" MPISPAWN_GENERIC_NAME_21=NODE_TASKS_PPN_INFO MPISPAWN_GENERIC_VALUE_21=""16,0_"" MPISPAWN_GENERIC_NAME_22=SLURM_CPUS_ON_NODE MPISPAWN_GENERIC_VALUE_22="16" MPISPAWN_GENERIC_NAME_23=PATH MPISPAWN_GENERIC_VALUE_23="/opt/apps/gcc4_7/mvapich2/1.9/bin:/opt/apps/gcc/4.7.1/bin:/opt/apps/xalt/0.4.4/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/opt/apps/xsede/gsi-openssh-5.7/bin:/usr/X11R6/bin:/opt/ofed/bin:/opt/ofed/sbin:." MPISPAWN_GENERIC_NAME_24=SLURM_TACC_JOBNAME MPISPAWN_GENERIC_VALUE_24="stampedeLargeScaleAllTS.sh" MPISPAWN_GENERIC_NAME_25=SLURM_NODEID MPISPAWN_GENERIC_VALUE_25="0" MPISPAWN_GENERIC_NAME_26=LMOD_PKG MPISPAWN_GENERIC_VALUE_26="/opt/apps/lmod/lmod" MPISPAWN_GENERIC_NAME_27=MIC_I_MPI_OFA_ADAPTER_NAME MPISPAWN_GENERIC_VALUE_27="mlx4_0" MPISPAWN_GENERIC_NAME_28=HISTSIZE MPISPAWN_GENERIC_VALUE_28="1000" MPISPAWN_GENERIC_NAME_29=SLURM_NTASKS MPISPAWN_GENERIC_VALUE_29="2048" MPISPAWN_GENERIC_NAME_30=SHLIB_PATH MPISPAWN_GENERIC_VALUE_30="/opt/apps/xsede/gsi-openssh-5.7/lib64" MPISPAWN_GENERIC_NAME_31=TACC_OVERRIDE_PROJECT MPISPAWN_GENERIC_VALUE_31="TG-ASC140020" MPISPAWN_GENERIC_NAME_32=_ModuleTable005_ MPISPAWN_GENERIC_VALUE_32="Z2NjNF83L21vZHVsZWZpbGVzIiwiL29wdC9hcHBzL3hzZWRlL21vZHVsZWZpbGVzIiwiL29wdC9hcHBzL21vZHVsZWZpbGVzIiwiL29wdC9tb2R1bGVmaWxlcyIsfSxbInN5c3RlbUJhc2VNUEFUSCJdPSIvb3B0L2FwcHMveHNlZGUvbW9kdWxlZmlsZXM6L29wdC9hcHBzL21vZHVsZWZpbGVzOi9vcHQvbW9kdWxlZmlsZXMiLFsidmVyc2lvbiJdPTIsfQ==" MPISPAWN_GENERIC_NAME_33=_ModuleTable_Sz_ MPISPAWN_GENERIC_VALUE_33="5" MPISPAWN_GENERIC_NAME_34=MAIL MPISPAWN_GENERIC_VALUE_34="/var/spool/mail/tg825207" MPISPAWN_GENERIC_NAME_35=MV2_USE_HUGEPAGES MPISPAWN_GENERIC_VALUE_35="0" MPISPAWN_GENERIC_NAME_36=SLURM_TOPOLOGY_ADDR MPISPAWN_GENERIC_VALUE_36="c468-601" MPISPAWN_GENERIC_NAME_37=TACC_XALT_DIR MPISPAWN_GENERIC_VALUE_37="/opt/apps/xalt/0.4.4/" MPISPAWN_GENERIC_NAME_38=QTDIR MPISPAWN_GENERIC_VALUE_38="/usr/lib64/qt-3.3" MPISPAWN_GENERIC_NAME_39=SYSTEM MPISPAWN_GENERIC_VALUE_39="linux" MPISPAWN_GENERIC_NAME_40=SLURM_TACC_RUNLIMIT_MINS MPISPAWN_GENERIC_VALUE_40="180" MPISPAWN_GENERIC_NAME_41=SLURM_SUBMIT_DIR MPISPAWN_GENERIC_VALUE_41="/home1/03222/tg825207/parallelnbody" MPISPAWN_GENERIC_NAME_42=LIBPATH MPISPAWN_GENERIC_VALUE_42="/opt/apps/xsede/gsi-openssh-5.7/lib64:/usr/lib:/lib" MPISPAWN_GENERIC_NAME_43=HOSTNAME MPISPAWN_GENERIC_VALUE_43="c468-601" MPISPAWN_GENERIC_NAME_44=LMOD_DEFAULT_MODULEPATH MPISPAWN_GENERIC_VALUE_44="/opt/apps/xsede/modulefiles:/opt/apps/modulefiles:/opt/modulefiles" MPISPAWN_GENERIC_NAME_45=ARCHIVE MPISPAWN_GENERIC_VALUE_45="/home/03222/tg825207" MPISPAWN_GENERIC_NAME_46=_ MPISPAWN_GENERIC_VALUE_46="/usr/local/bin/build_env.pl" MPISPAWN_GENERIC_NAME_47=__BASHRC_SOURCED__ MPISPAWN_GENERIC_VALUE_47="1" MPISPAWN_GENERIC_NAME_48=APPS MPISPAWN_GENERIC_VALUE_48="/opt/apps" MPISPAWN_GENERIC_NAME_49=SHELL MPISPAWN_GENERIC_VALUE_49="/bin/bash" MPISPAWN_GENERIC_NAME_50=ARMCI_OPENIB_DEVICE MPISPAWN_GENERIC_VALUE_50="mlx4_0" MPISPAWN_GENERIC_NAME_51=ENVIRONMENT MPISPAWN_GENERIC_VALUE_51="BATCH" MPISPAWN_GENERIC_NAME_52=TACC_FAMILY_MPI MPISPAWN_GENERIC_VALUE_52="mvapich2" MPISPAWN_GENERIC_NAME_53=MV2_IBA_HCA MPISPAWN_GENERIC_VALUE_53="mlx4_0" MPISPAWN_GENERIC_NAME_54=TACC_PUBLIC_MACHINE MPISPAWN_GENERIC_VALUE_54="1" MPISPAWN_GENERIC_NAME_55=SLURM_TACC_NODES MPISPAWN_GENERIC_VALUE_55="1" MPISPAWN_GENERIC_NAME_56=_ModuleTable003_ MPISPAWN_GENERIC_VALUE_56="e30sWyJzaG9ydCJdPSJjbHVzdGVyIixbInN0YXR1cyJdPSJhY3RpdmUiLH0sWyJjbHVzdGVyLXBhdGhzIl09e1siRk4iXT0iL29wdC9tb2R1bGVmaWxlcy9jbHVzdGVyLXBhdGhzIixbImRlZmF1bHQiXT0wLFsiZnVsbE5hbWUiXT0iY2x1c3Rlci1wYXRocyIsWyJsb2FkT3JkZXIiXT0zLHByb3BUPXt9LFsic2hvcnQiXT0iY2x1c3Rlci1wYXRocyIsWyJzdGF0dXMiXT0iYWN0aXZlIix9LGdjYz17WyJGTiJdPSIvb3B0L2FwcHMvbW9kdWxlZmlsZXMvZ2NjLzQuNy4xLmx1YSIsWyJkZWZhdWx0Il09MCxbImZ1bGxOYW1lIl09ImdjYy80LjcuMSIsWyJsb2FkT3JkZXIiXT03LHByb3BUPXt9LFsic2hvcnQiXT0iZ2NjIixbInN0YXR1cyJdPSJhY3RpdmUiLH0sbXZhcGljaDI9e1si" MPISPAWN_GENERIC_NAME_57=QTINC MPISPAWN_GENERIC_VALUE_57="/usr/lib64/qt-3.3/include" MPISPAWN_GENERIC_NAME_58=CVS_RSH MPISPAWN_GENERIC_VALUE_58="ssh" MPISPAWN_GENERIC_NAME_59=MV2_USE_OLD_BCAST MPISPAWN_GENERIC_VALUE_59="1" MPISPAWN_GENERIC_NAME_60=SLURM_JOB_NUM_NODES MPISPAWN_GENERIC_VALUE_60="128" MPISPAWN_GENERIC_NAME_61=SLURM_TASK_PID MPISPAWN_GENERIC_VALUE_61="31479" MPISPAWN_GENERIC_NAME_62=TACC_MPI_GETMODE MPISPAWN_GENERIC_VALUE_62="mvapich2_ssh" MPISPAWN_GENERIC_NAME_63=LANG MPISPAWN_GENERIC_VALUE_63="en_US.UTF-8" MPISPAWN_GENERIC_NAME_64=USER MPISPAWN_GENERIC_VALUE_64="tg825207" MPISPAWN_GENERIC_NAME_65=MIC_I_MPI_DAPL_PROVIDER MPISPAWN_GENERIC_VALUE_65="ofa-v2-mlx4_0-1u" MPISPAWN_GENERIC_NAME_66=G_BROKEN_FILENAMES MPISPAWN_GENERIC_VALUE_66="1" MPISPAWN_GENERIC_NAME_67=SLURM_LOCALID MPISPAWN_GENERIC_VALUE_67="0" MPISPAWN_GENERIC_NAME_68=SLURM_NPROCS MPISPAWN_GENERIC_VALUE_68="2048" MPISPAWN_GENERIC_NAME_69=SLURM_TACC_NNODES_SET MPISPAWN_GENERIC_VALUE_69="0" MPISPAWN_GENERIC_NAME_70=SLURM_TACC_CORES MPISPAWN_GENERIC_VALUE_70="2048" MPISPAWN_GENERIC_NAME_71=LMOD_DIR MPISPAWN_GENERIC_VALUE_71="/opt/apps/lmod/lmod/libexec" MPISPAWN_GENERIC_NAME_72=LMOD_SETTARG_CMD MPISPAWN_GENERIC_VALUE_72=":" MPISPAWN_GENERIC_NAME_73=OMP_NUM_THREADS MPISPAWN_GENERIC_VALUE_73="1" MPISPAWN_GENERIC_NAME_74=_ModuleTable004_ MPISPAWN_GENERIC_VALUE_74="Rk4iXT0iL29wdC9hcHBzL2djYzRfNy9tb2R1bGVmaWxlcy9tdmFwaWNoMi8xLjlhMiIsWyJkZWZhdWx0Il09MSxbImZ1bGxOYW1lIl09Im12YXBpY2gyLzEuOWEyIixbImxvYWRPcmRlciJdPTgscHJvcFQ9e30sWyJzaG9ydCJdPSJtdmFwaWNoMiIsWyJzdGF0dXMiXT0iYWN0aXZlIix9LHhhbHQ9e1siRk4iXT0iL29wdC9hcHBzL21vZHVsZWZpbGVzL3hhbHQvMC40LjQubHVhIixbImRlZmF1bHQiXT0xLFsiZnVsbE5hbWUiXT0ieGFsdC8wLjQuNCIsWyJsb2FkT3JkZXIiXT00LHByb3BUPXt9LFsic2hvcnQiXT0ieGFsdCIsWyJzdGF0dXMiXT0iYWN0aXZlIix9LH0sbXBhdGhBPXsiL29wdC9hcHBzL2djYzRfNy9tdmFwaWNoMl8xXzkvbW9kdWxlZmlsZXMiLCIvb3B0L2FwcHMv" MPISPAWN_GENERIC_NAME_75=__PERSONAL_PATH__ MPISPAWN_GENERIC_VALUE_75="1" MPISPAWN_GENERIC_NAME_76=LMOD_SYSTEM_DEFAULT_MODULES MPISPAWN_GENERIC_VALUE_76="TACC" MPISPAWN_GENERIC_NAME_77=SLURM_JOB_NAME MPISPAWN_GENERIC_VALUE_77="stampedeLargeScaleAllTS.sh" MPISPAWN_GENERIC_NAME_78=SLURM_SUBMIT_HOST MPISPAWN_GENERIC_VALUE_78="login4.stampede.tacc.utexas.edu" MPISPAWN_GENERIC_NAME_79=LD_LIBRARY_PATH MPISPAWN_GENERIC_VALUE_79="/opt/apps/gcc4_7/mvapich2/1.9/lib:/opt/apps/gcc4_7/mvapich2/1.9/lib/shared:/opt/apps/gcc/4.7.1/lib64:/opt/apps/gcc/4.7.1/lib:/opt/apps/xsede/gsi-openssh-5.7/lib64:/opt/apps/xsede/gsi-openssh-5.7/lib64" MPISPAWN_GENERIC_NAME_80=SLURM_QUEUE MPISPAWN_GENERIC_VALUE_80="normal" MPISPAWN_GENERIC_NAME_81=PKG_CONFIG_PATH MPISPAWN_GENERIC_VALUE_81="/opt/apps/gcc4_7/mvapich2/1.9/lib/pkgconfig" MPISPAWN_GENERIC_NAME_82=LMOD_PREPEND_BLOCK MPISPAWN_GENERIC_VALUE_82="normal" MPISPAWN_GENERIC_NAME_83=TACC_FAMILY_COMPILER MPISPAWN_GENERIC_VALUE_83="gcc" MPISPAWN_GENERIC_NAME_84=TACC_DOMAIN MPISPAWN_GENERIC_VALUE_84="stampede" MPISPAWN_GENERIC_NAME_85=MV2_HOMOGENEOUS_CLUSTER MPISPAWN_GENERIC_VALUE_85="1" MPISPAWN_GENERIC_NAME_86=LMOD_FAMILY_COMPILER MPISPAWN_GENERIC_VALUE_86="gcc" MPISPAWN_GENERIC_NAME_87=MV2_USE_UD_HYBRID MPISPAWN_GENERIC_VALUE_87="0" MPISPAWN_GENERIC_NAME_88=MPICH_HOME MPISPAWN_GENERIC_VALUE_88="/opt/apps/gcc4_7/mvapich2/1.9" MPISPAWN_GENERIC_NAME_89=SLURM_JOB_NODELIST MPISPAWN_GENERIC_VALUE_89="c468-601,c471-503,c472-[401-402],c473-[303-304,401,804,901],c474-[203-204,301,401,403-404],c478-303,c482-404,c483-[001-004,101,504,601,804,901],c484-[101,103-104,201,602,701],c485-502,c486-102,c490-[302,904],c491-004,c492-701,c493-[001,102,201],c494-[101,402-403,602,702-703,904],c495-[001-002,201],c497-[102,202,504,601-603],c498-[304,402,603],c501-[204,301,604,801],c502-104,c503-901,c504-[602,604,702,801],c505-[001,202-204,401,502-504,802,901-904],c506-[302,704],c507-[102,202,503,601,904],c508-[101,204,601,702-703,802,903-904],c509-[002,201,301,303,401,501-502],c510-[003,401,404,501,503],c511-[601-602,902-903],c536-[801,902],c537-902,c538-[001,003,104],c540-[104,201-202,804,901-904]" MPISPAWN_GENERIC_NAME_90=SLURM_PROCID MPISPAWN_GENERIC_VALUE_90="0" MPISPAWN_GENERIC_NAME_91=I_MPI_DAPL_PROVIDER MPISPAWN_GENERIC_VALUE_91="ofa-v2-mlx4_0-1u" MPISPAWN_GENERIC_NAME_92=GLOBUS_LOCATION MPISPAWN_GENERIC_VALUE_92="/opt/apps/xsede/gsi-openssh-5.7" MPISPAWN_GENERIC_NAME_93=LMOD_COLORIZE MPISPAWN_GENERIC_VALUE_93="yes" MPISPAWN_GENERIC_NAME_94=I_MPI_HYDRA_BRANCH_COUNT MPISPAWN_GENERIC_VALUE_94="6000" MPISPAWN_GENERIC_NAME_95=SLURM_NNODES MPISPAWN_GENERIC_VALUE_95="128" MPISPAWN_GENERIC_NAME_96=MV2_DEFAULT_TIME_OUT MPISPAWN_GENERIC_VALUE_96="23" MPISPAWN_GENERIC_NAME_97=SLURM_TACC_NCORES_SET MPISPAWN_GENERIC_VALUE_97="1" MPISPAWN_GENERIC_NAME_98=__Init_Default_Modules MPISPAWN_GENERIC_VALUE_98="1" MPISPAWN_GENERIC_NAME_99=BASH_ENV MPISPAWN_GENERIC_VALUE_99="/etc/tacc/tacc_functions" MPISPAWN_GENERIC_NAME_100=SLURM_TOPOLOGY_ADDR_PATTERN MPISPAWN_GENERIC_VALUE_100="node" MPISPAWN_GENERIC_NAME_101=MV2_USE_RING_STARTUP MPISPAWN_GENERIC_VALUE_101="0" MPISPAWN_GENERIC_NAME_102=TACC_XALT_BIN MPISPAWN_GENERIC_VALUE_102="/opt/apps/xalt/0.4.4/bin" MPISPAWN_GENERIC_NAME_103=X509_USER_PROXY MPISPAWN_GENERIC_VALUE_103="/tmp/x509up_p3084.file9zM3fu.1" MPISPAWN_GENERIC_NAME_104=TMPDIR MPISPAWN_GENERIC_VALUE_104="/tmp" MPISPAWN_GENERIC_NAME_105=LMOD_FULL_SETTARG_SUPPORT MPISPAWN_GENERIC_VALUE_105="no" MPISPAWN_ID=17 MPISPAWN_WORKING_DIR=/home1/03222/tg825207/parallelnbody MPISPAWN_MPIRUN_RANK_0=272 MPISPAWN_MPIRUN_RANK_1=273 MPISPAWN_MPIRUN_RANK_2=274 MPISPAWN_MPIRUN_RANK_3=275 MPISPAWN_MPIRUN_RANK_4=276 MPISPAWN_MPIRUN_RANK_5=277 MPISPAWN_MPIRUN_RANK_6=278 MPISPAWN_MPIRUN_RANK_7=279 MPISPAWN_MPIRUN_RANK_8=280 MPISPAWN_MPIRUN_RANK_9=281 MPISPAWN_MPIRUN_RANK_10=282 MPISPAWN_MPIRUN_RANK_11=283 MPISPAWN_MPIRUN_RANK_12=284 MPISPAWN_MPIRUN_RANK_13=285 MPISPAWN_MPIRUN_RANK_14=286 MPISPAWN_MPIRUN_RANK_15=287 /opt/apps/gcc4_7/mvapich2/1.9/bin/mpispawn 0
[c493-102.stampede.tacc.utexas.edu:mpispawn_39][error_sighandler] Caught error: Segmentation fault (signal 11)
[c484-701.stampede.tacc.utexas.edu:mpispawn_31][error_sighandler] Caught error: Segmentation fault (signal 11)
bash: line 1: 73240 Segmentation fault      /usr/bin/env LD_LIBRARY_PATH=/opt/apps/gcc4_7/mvapich2/1.9/lib:/opt/apps/gcc4_7/mvapich2/1.9/lib/shared:/opt/apps/gcc/4.7.1/lib64:/opt/apps/gcc/4.7.1/lib:/opt/apps/xsede/gsi-openssh-5.7/lib64:/opt/apps/xsede/gsi-openssh-5.7/lib64 MPISPAWN_MPIRUN_MPD=0 USE_LINEAR_SSH=1 MPISPAWN_MPIRUN_HOST=c468-601.stampede.tacc.utexas.edu MPISPAWN_MPIRUN_HOSTIP=129.114.76.165 MPIRUN_RSH_LAUNCH=1 MPISPAWN_CHECKIN_PORT=36101 MPISPAWN_MPIRUN_PORT=36101 MPISPAWN_NNODES=64 MPISPAWN_GLOBAL_NPROCS=1024 MPISPAWN_MPIRUN_ID=44295 MPISPAWN_ARGC=4 MPDMAN_KVS_TEMPLATE=kvs_513_c468-601.stampede.tacc.utexas.edu_44295 MPISPAWN_LOCAL_NPROCS=16 MPISPAWN_ARGV_0='/home1/03222/tg825207/parallelnbody/teamscatter' MPISPAWN_ARGV_1='256000' MPISPAWN_ARGV_2='-c' MPISPAWN_ARGV_3='2' MPISPAWN_ARGC=4 MPISPAWN_GENERIC_ENV_COUNT=106 MPISPAWN_GENERIC_NAME_0=TACC_ENVLEN MPISPAWN_GENERIC_VALUE_0=6578 MPISPAWN_GENERIC_NAME_1=SLURM_JOB_ID MPISPAWN_GENERIC_VALUE_1="4306264" MPISPAWN_GENERIC_NAME_2=SLURMD_NODENAME MPISPAWN_GENERIC_VALUE_2="c468-601" MPISPAWN_GENERIC_NAME_3=STOCKYARD MPISPAWN_GENERIC_VALUE_3="/work/03222/tg825207" MPISPAWN_GENERIC_NAME_4=LC_ALL MPISPAWN_GENERIC_VALUE_4="en_US.UTF-8" MPISPAWN_GENERIC_NAME_5=SLURM_CHECKPOINT_IMAGE_DIR MPISPAWN_GENERIC_VALUE_5="/home1/03222/tg825207/parallelnbody" MPISPAWN_GENERIC_NAME_6=LMOD_FAMILY_MPI MPISPAWN_GENERIC_VALUE_6="mvapich2" MPISPAWN_GENERIC_NAME_7=SLURM_JOBID MPISPAWN_GENERIC_VALUE_7="4306264" MPISPAWN_GENERIC_NAME_8=_ModuleTable001_ MPISPAWN_GENERIC_VALUE_8="X01vZHVsZVRhYmxlXz17WyJhY3RpdmVTaXplIl09OCxiYXNlTXBhdGhBPXsiL29wdC9hcHBzL3hzZWRlL21vZHVsZWZpbGVzIiwiL29wdC9hcHBzL21vZHVsZWZpbGVzIiwiL29wdC9tb2R1bGVmaWxlcyIsfSxbImNfcmVidWlsZFRpbWUiXT1mYWxzZSxbImNfc2hvcnRUaW1lIl09ZmFsc2UsZmFtaWx5PXtbIk1QSSJdPSJtdmFwaWNoMiIsWyJjb21waWxlciJdPSJnY2MiLH0saW5hY3RpdmU9e30sbVQ9e0xpbnV4PXtbIkZOIl09Ii9vcHQvbW9kdWxlZmlsZXMvTGludXgiLFsiZGVmYXVsdCJdPTAsWyJmdWxsTmFtZSJdPSJMaW51eCIsWyJsb2FkT3JkZXIiXT0yLHByb3BUPXt9LFsic2hvcnQiXT0iTGludXgiLFsic3RhdHVzIl09ImFjdGl2ZSIsfSxUQUNDPXtbIkZOIl09Ii9v" MPISPAWN_GENERIC_NAME_9=TACC_SYSTEM MPISPAWN_GENERIC_VALUE_9="stampede" MPISPAWN_GENERIC_NAME_10=WORK MPISPAWN_GENERIC_VALUE_10="/work/03222/tg825207" MPISPAWN_GENERIC_NAME_11=SLURM_GTIDS MPISPAWN_GENERIC_VALUE_11="0" MPISPAWN_GENERIC_NAME_12=GCC_LIB MPISPAWN_GENERIC_VALUE_12="/opt/apps/gcc/4.7.1/lib64" MPISPAWN_GENERIC_NAME_13=SLURM_PRIO_PROCESS MPISPAWN_GENERIC_VALUE_13="0" MPISPAWN_GENERIC_NAME_14=SCRATCH MPISPAWN_GENERIC_VALUE_14="/scratch/03222/tg825207" MPISPAWN_GENERIC_NAME_15=LOGNAME MPISPAWN_GENERIC_VALUE_15="tg825207" MPISPAWN_GENERIC_NAME_16=_ModuleTable002_ MPISPAWN_GENERIC_VALUE_16="cHQvbW9kdWxlZmlsZXMvVEFDQyIsWyJkZWZhdWx0Il09MCxbImZ1bGxOYW1lIl09IlRBQ0MiLFsibG9hZE9yZGVyIl09Nixwcm9wVD17fSxbInNob3J0Il09IlRBQ0MiLFsic3RhdHVzIl09ImFjdGl2ZSIsfSxbIlRBQ0MtcGF0aHMiXT17WyJGTiJdPSIvb3B0L21vZHVsZWZpbGVzL1RBQ0MtcGF0aHMiLFsiZGVmYXVsdCJdPTAsWyJmdWxsTmFtZSJdPSJUQUNDLXBhdGhzIixbImxvYWRPcmRlciJdPTEscHJvcFQ9e30sWyJzaG9ydCJdPSJUQUNDLXBhdGhzIixbInN0YXR1cyJdPSJhY3RpdmUiLH0sY2x1c3Rlcj17WyJGTiJdPSIvb3B0L21vZHVsZWZpbGVzL2NsdXN0ZXIiLFsiZGVmYXVsdCJdPTAsWyJmdWxsTmFtZSJdPSJjbHVzdGVyIixbImxvYWRPcmRlciJdPTUscHJvcFQ9" MPISPAWN_GENERIC_NAME_17=INPUTRC MPISPAWN_GENERIC_VALUE_17="/etc/inputrc" MPISPAWN_GENERIC_NAME_18=LMOD_CMD MPISPAWN_GENERIC_VALUE_18="/opt/apps/lmod/lmod/libexec/lmod" MPISPAWN_GENERIC_NAME_19=I_MPI_OFA_ADAPTER_NAME MPISPAWN_GENERIC_VALUE_19="mlx4_0" MPISPAWN_GENERIC_NAME_20=QTLIB MPISPAWN_GENERIC_VALUE_20="/usr/lib64/qt-3.3/lib" MPISPAWN_GENERIC_NAME_21=NODE_TASKS_PPN_INFO MPISPAWN_GENERIC_VALUE_21=""16,0_"" MPISPAWN_GENERIC_NAME_22=SLURM_CPUS_ON_NODE MPISPAWN_GENERIC_VALUE_22="16" MPISPAWN_GENERIC_NAME_23=PATH MPISPAWN_GENERIC_VALUE_23="/opt/apps/gcc4_7/mvapich2/1.9/bin:/opt/apps/gcc/4.7.1/bin:/opt/apps/xalt/0.4.4/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/opt/apps/xsede/gsi-openssh-5.7/bin:/usr/X11R6/bin:/opt/ofed/bin:/opt/ofed/sbin:." MPISPAWN_GENERIC_NAME_24=SLURM_TACC_JOBNAME MPISPAWN_GENERIC_VALUE_24="stampedeLargeScaleAllTS.sh" MPISPAWN_GENERIC_NAME_25=SLURM_NODEID MPISPAWN_GENERIC_VALUE_25="0" MPISPAWN_GENERIC_NAME_26=LMOD_PKG MPISPAWN_GENERIC_VALUE_26="/opt/apps/lmod/lmod" MPISPAWN_GENERIC_NAME_27=MIC_I_MPI_OFA_ADAPTER_NAME MPISPAWN_GENERIC_VALUE_27="mlx4_0" MPISPAWN_GENERIC_NAME_28=HISTSIZE MPISPAWN_GENERIC_VALUE_28="1000" MPISPAWN_GENERIC_NAME_29=SLURM_NTASKS MPISPAWN_GENERIC_VALUE_29="2048" MPISPAWN_GENERIC_NAME_30=SHLIB_PATH MPISPAWN_GENERIC_VALUE_30="/opt/apps/xsede/gsi-openssh-5.7/lib64" MPISPAWN_GENERIC_NAME_31=TACC_OVERRIDE_PROJECT MPISPAWN_GENERIC_VALUE_31="TG-ASC140020" MPISPAWN_GENERIC_NAME_32=_ModuleTable005_ MPISPAWN_GENERIC_VALUE_32="Z2NjNF83L21vZHVsZWZpbGVzIiwiL29wdC9hcHBzL3hzZWRlL21vZHVsZWZpbGVzIiwiL29wdC9hcHBzL21vZHVsZWZpbGVzIiwiL29wdC9tb2R1bGVmaWxlcyIsfSxbInN5c3RlbUJhc2VNUEFUSCJdPSIvb3B0L2FwcHMveHNlZGUvbW9kdWxlZmlsZXM6L29wdC9hcHBzL21vZHVsZWZpbGVzOi9vcHQvbW9kdWxlZmlsZXMiLFsidmVyc2lvbiJdPTIsfQ==" MPISPAWN_GENERIC_NAME_33=_ModuleTable_Sz_ MPISPAWN_GENERIC_VALUE_33="5" MPISPAWN_GENERIC_NAME_34=MAIL MPISPAWN_GENERIC_VALUE_34="/var/spool/mail/tg825207" MPISPAWN_GENERIC_NAME_35=MV2_USE_HUGEPAGES MPISPAWN_GENERIC_VALUE_35="0" MPISPAWN_GENERIC_NAME_36=SLURM_TOPOLOGY_ADDR MPISPAWN_GENERIC_VALUE_36="c468-601" MPISPAWN_GENERIC_NAME_37=TACC_XALT_DIR MPISPAWN_GENERIC_VALUE_37="/opt/apps/xalt/0.4.4/" MPISPAWN_GENERIC_NAME_38=QTDIR MPISPAWN_GENERIC_VALUE_38="/usr/lib64/qt-3.3" MPISPAWN_GENERIC_NAME_39=SYSTEM MPISPAWN_GENERIC_VALUE_39="linux" MPISPAWN_GENERIC_NAME_40=SLURM_TACC_RUNLIMIT_MINS MPISPAWN_GENERIC_VALUE_40="180" MPISPAWN_GENERIC_NAME_41=SLURM_SUBMIT_DIR MPISPAWN_GENERIC_VALUE_41="/home1/03222/tg825207/parallelnbody" MPISPAWN_GENERIC_NAME_42=LIBPATH MPISPAWN_GENERIC_VALUE_42="/opt/apps/xsede/gsi-openssh-5.7/lib64:/usr/lib:/lib" MPISPAWN_GENERIC_NAME_43=HOSTNAME MPISPAWN_GENERIC_VALUE_43="c468-601" MPISPAWN_GENERIC_NAME_44=LMOD_DEFAULT_MODULEPATH MPISPAWN_GENERIC_VALUE_44="/opt/apps/xsede/modulefiles:/opt/apps/modulefiles:/opt/modulefiles" MPISPAWN_GENERIC_NAME_45=ARCHIVE MPISPAWN_GENERIC_VALUE_45="/home/03222/tg825207" MPISPAWN_GENERIC_NAME_46=_ MPISPAWN_GENERIC_VALUE_46="/usr/local/bin/build_env.pl" MPISPAWN_GENERIC_NAME_47=__BASHRC_SOURCED__ MPISPAWN_GENERIC_VALUE_47="1" MPISPAWN_GENERIC_NAME_48=APPS MPISPAWN_GENERIC_VALUE_48="/opt/apps" MPISPAWN_GENERIC_NAME_49=SHELL MPISPAWN_GENERIC_VALUE_49="/bin/bash" MPISPAWN_GENERIC_NAME_50=ARMCI_OPENIB_DEVICE MPISPAWN_GENERIC_VALUE_50="mlx4_0" MPISPAWN_GENERIC_NAME_51=ENVIRONMENT MPISPAWN_GENERIC_VALUE_51="BATCH" MPISPAWN_GENERIC_NAME_52=TACC_FAMILY_MPI MPISPAWN_GENERIC_VALUE_52="mvapich2" MPISPAWN_GENERIC_NAME_53=MV2_IBA_HCA MPISPAWN_GENERIC_VALUE_53="mlx4_0" MPISPAWN_GENERIC_NAME_54=TACC_PUBLIC_MACHINE MPISPAWN_GENERIC_VALUE_54="1" MPISPAWN_GENERIC_NAME_55=SLURM_TACC_NODES MPISPAWN_GENERIC_VALUE_55="1" MPISPAWN_GENERIC_NAME_56=_ModuleTable003_ MPISPAWN_GENERIC_VALUE_56="e30sWyJzaG9ydCJdPSJjbHVzdGVyIixbInN0YXR1cyJdPSJhY3RpdmUiLH0sWyJjbHVzdGVyLXBhdGhzIl09e1siRk4iXT0iL29wdC9tb2R1bGVmaWxlcy9jbHVzdGVyLXBhdGhzIixbImRlZmF1bHQiXT0wLFsiZnVsbE5hbWUiXT0iY2x1c3Rlci1wYXRocyIsWyJsb2FkT3JkZXIiXT0zLHByb3BUPXt9LFsic2hvcnQiXT0iY2x1c3Rlci1wYXRocyIsWyJzdGF0dXMiXT0iYWN0aXZlIix9LGdjYz17WyJGTiJdPSIvb3B0L2FwcHMvbW9kdWxlZmlsZXMvZ2NjLzQuNy4xLmx1YSIsWyJkZWZhdWx0Il09MCxbImZ1bGxOYW1lIl09ImdjYy80LjcuMSIsWyJsb2FkT3JkZXIiXT03LHByb3BUPXt9LFsic2hvcnQiXT0iZ2NjIixbInN0YXR1cyJdPSJhY3RpdmUiLH0sbXZhcGljaDI9e1si" MPISPAWN_GENERIC_NAME_57=QTINC MPISPAWN_GENERIC_VALUE_57="/usr/lib64/qt-3.3/include" MPISPAWN_GENERIC_NAME_58=CVS_RSH MPISPAWN_GENERIC_VALUE_58="ssh" MPISPAWN_GENERIC_NAME_59=MV2_USE_OLD_BCAST MPISPAWN_GENERIC_VALUE_59="1" MPISPAWN_GENERIC_NAME_60=SLURM_JOB_NUM_NODES MPISPAWN_GENERIC_VALUE_60="128" MPISPAWN_GENERIC_NAME_61=SLURM_TASK_PID MPISPAWN_GENERIC_VALUE_61="31479" MPISPAWN_GENERIC_NAME_62=TACC_MPI_GETMODE MPISPAWN_GENERIC_VALUE_62="mvapich2_ssh" MPISPAWN_GENERIC_NAME_63=LANG MPISPAWN_GENERIC_VALUE_63="en_US.UTF-8" MPISPAWN_GENERIC_NAME_64=USER MPISPAWN_GENERIC_VALUE_64="tg825207" MPISPAWN_GENERIC_NAME_65=MIC_I_MPI_DAPL_PROVIDER MPISPAWN_GENERIC_VALUE_65="ofa-v2-mlx4_0-1u" MPISPAWN_GENERIC_NAME_66=G_BROKEN_FILENAMES MPISPAWN_GENERIC_VALUE_66="1" MPISPAWN_GENERIC_NAME_67=SLURM_LOCALID MPISPAWN_GENERIC_VALUE_67="0" MPISPAWN_GENERIC_NAME_68=SLURM_NPROCS MPISPAWN_GENERIC_VALUE_68="2048" MPISPAWN_GENERIC_NAME_69=SLURM_TACC_NNODES_SET MPISPAWN_GENERIC_VALUE_69="0" MPISPAWN_GENERIC_NAME_70=SLURM_TACC_CORES MPISPAWN_GENERIC_VALUE_70="2048" MPISPAWN_GENERIC_NAME_71=LMOD_DIR MPISPAWN_GENERIC_VALUE_71="/opt/apps/lmod/lmod/libexec" MPISPAWN_GENERIC_NAME_72=LMOD_SETTARG_CMD MPISPAWN_GENERIC_VALUE_72=":" MPISPAWN_GENERIC_NAME_73=OMP_NUM_THREADS MPISPAWN_GENERIC_VALUE_73="1" MPISPAWN_GENERIC_NAME_74=_ModuleTable004_ MPISPAWN_GENERIC_VALUE_74="Rk4iXT0iL29wdC9hcHBzL2djYzRfNy9tb2R1bGVmaWxlcy9tdmFwaWNoMi8xLjlhMiIsWyJkZWZhdWx0Il09MSxbImZ1bGxOYW1lIl09Im12YXBpY2gyLzEuOWEyIixbImxvYWRPcmRlciJdPTgscHJvcFQ9e30sWyJzaG9ydCJdPSJtdmFwaWNoMiIsWyJzdGF0dXMiXT0iYWN0aXZlIix9LHhhbHQ9e1siRk4iXT0iL29wdC9hcHBzL21vZHVsZWZpbGVzL3hhbHQvMC40LjQubHVhIixbImRlZmF1bHQiXT0xLFsiZnVsbE5hbWUiXT0ieGFsdC8wLjQuNCIsWyJsb2FkT3JkZXIiXT00LHByb3BUPXt9LFsic2hvcnQiXT0ieGFsdCIsWyJzdGF0dXMiXT0iYWN0aXZlIix9LH0sbXBhdGhBPXsiL29wdC9hcHBzL2djYzRfNy9tdmFwaWNoMl8xXzkvbW9kdWxlZmlsZXMiLCIvb3B0L2FwcHMv" MPISPAWN_GENERIC_NAME_75=__PERSONAL_PATH__ MPISPAWN_GENERIC_VALUE_75="1" MPISPAWN_GENERIC_NAME_76=LMOD_SYSTEM_DEFAULT_MODULES MPISPAWN_GENERIC_VALUE_76="TACC" MPISPAWN_GENERIC_NAME_77=SLURM_JOB_NAME MPISPAWN_GENERIC_VALUE_77="stampedeLargeScaleAllTS.sh" MPISPAWN_GENERIC_NAME_78=SLURM_SUBMIT_HOST MPISPAWN_GENERIC_VALUE_78="login4.stampede.tacc.utexas.edu" MPISPAWN_GENERIC_NAME_79=LD_LIBRARY_PATH MPISPAWN_GENERIC_VALUE_79="/opt/apps/gcc4_7/mvapich2/1.9/lib:/opt/apps/gcc4_7/mvapich2/1.9/lib/shared:/opt/apps/gcc/4.7.1/lib64:/opt/apps/gcc/4.7.1/lib:/opt/apps/xsede/gsi-openssh-5.7/lib64:/opt/apps/xsede/gsi-openssh-5.7/lib64" MPISPAWN_GENERIC_NAME_80=SLURM_QUEUE MPISPAWN_GENERIC_VALUE_80="normal" MPISPAWN_GENERIC_NAME_81=PKG_CONFIG_PATH MPISPAWN_GENERIC_VALUE_81="/opt/apps/gcc4_7/mvapich2/1.9/lib/pkgconfig" MPISPAWN_GENERIC_NAME_82=LMOD_PREPEND_BLOCK MPISPAWN_GENERIC_VALUE_82="normal" MPISPAWN_GENERIC_NAME_83=TACC_FAMILY_COMPILER MPISPAWN_GENERIC_VALUE_83="gcc" MPISPAWN_GENERIC_NAME_84=TACC_DOMAIN MPISPAWN_GENERIC_VALUE_84="stampede" MPISPAWN_GENERIC_NAME_85=MV2_HOMOGENEOUS_CLUSTER MPISPAWN_GENERIC_VALUE_85="1" MPISPAWN_GENERIC_NAME_86=LMOD_FAMILY_COMPILER MPISPAWN_GENERIC_VALUE_86="gcc" MPISPAWN_GENERIC_NAME_87=MV2_USE_UD_HYBRID MPISPAWN_GENERIC_VALUE_87="0" MPISPAWN_GENERIC_NAME_88=MPICH_HOME MPISPAWN_GENERIC_VALUE_88="/opt/apps/gcc4_7/mvapich2/1.9" MPISPAWN_GENERIC_NAME_89=SLURM_JOB_NODELIST MPISPAWN_GENERIC_VALUE_89="c468-601,c471-503,c472-[401-402],c473-[303-304,401,804,901],c474-[203-204,301,401,403-404],c478-303,c482-404,c483-[001-004,101,504,601,804,901],c484-[101,103-104,201,602,701],c485-502,c486-102,c490-[302,904],c491-004,c492-701,c493-[001,102,201],c494-[101,402-403,602,702-703,904],c495-[001-002,201],c497-[102,202,504,601-603],c498-[304,402,603],c501-[204,301,604,801],c502-104,c503-901,c504-[602,604,702,801],c505-[001,202-204,401,502-504,802,901-904],c506-[302,704],c507-[102,202,503,601,904],c508-[101,204,601,702-703,802,903-904],c509-[002,201,301,303,401,501-502],c510-[003,401,404,501,503],c511-[601-602,902-903],c536-[801,902],c537-902,c538-[001,003,104],c540-[104,201-202,804,901-904]" MPISPAWN_GENERIC_NAME_90=SLURM_PROCID MPISPAWN_GENERIC_VALUE_90="0" MPISPAWN_GENERIC_NAME_91=I_MPI_DAPL_PROVIDER MPISPAWN_GENERIC_VALUE_91="ofa-v2-mlx4_0-1u" MPISPAWN_GENERIC_NAME_92=GLOBUS_LOCATION MPISPAWN_GENERIC_VALUE_92="/opt/apps/xsede/gsi-openssh-5.7" MPISPAWN_GENERIC_NAME_93=LMOD_COLORIZE MPISPAWN_GENERIC_VALUE_93="yes" MPISPAWN_GENERIC_NAME_94=I_MPI_HYDRA_BRANCH_COUNT MPISPAWN_GENERIC_VALUE_94="6000" MPISPAWN_GENERIC_NAME_95=SLURM_NNODES MPISPAWN_GENERIC_VALUE_95="128" MPISPAWN_GENERIC_NAME_96=MV2_DEFAULT_TIME_OUT MPISPAWN_GENERIC_VALUE_96="23" MPISPAWN_GENERIC_NAME_97=SLURM_TACC_NCORES_SET MPISPAWN_GENERIC_VALUE_97="1" MPISPAWN_GENERIC_NAME_98=__Init_Default_Modules MPISPAWN_GENERIC_VALUE_98="1" MPISPAWN_GENERIC_NAME_99=BASH_ENV MPISPAWN_GENERIC_VALUE_99="/etc/tacc/tacc_functions" MPISPAWN_GENERIC_NAME_100=SLURM_TOPOLOGY_ADDR_PATTERN MPISPAWN_GENERIC_VALUE_100="node" MPISPAWN_GENERIC_NAME_101=MV2_USE_RING_STARTUP MPISPAWN_GENERIC_VALUE_101="0" MPISPAWN_GENERIC_NAME_102=TACC_XALT_BIN MPISPAWN_GENERIC_VALUE_102="/opt/apps/xalt/0.4.4/bin" MPISPAWN_GENERIC_NAME_103=X509_USER_PROXY MPISPAWN_GENERIC_VALUE_103="/tmp/x509up_p3084.file9zM3fu.1" MPISPAWN_GENERIC_NAME_104=TMPDIR MPISPAWN_GENERIC_VALUE_104="/tmp" MPISPAWN_GENERIC_NAME_105=LMOD_FULL_SETTARG_SUPPORT MPISPAWN_GENERIC_VALUE_105="no" MPISPAWN_ID=39 MPISPAWN_WORKING_DIR=/home1/03222/tg825207/parallelnbody MPISPAWN_MPIRUN_RANK_0=624 MPISPAWN_MPIRUN_RANK_1=625 MPISPAWN_MPIRUN_RANK_2=626 MPISPAWN_MPIRUN_RANK_3=627 MPISPAWN_MPIRUN_RANK_4=628 MPISPAWN_MPIRUN_RANK_5=629 MPISPAWN_MPIRUN_RANK_6=630 MPISPAWN_MPIRUN_RANK_7=631 MPISPAWN_MPIRUN_RANK_8=632 MPISPAWN_MPIRUN_RANK_9=633 MPISPAWN_MPIRUN_RANK_10=634 MPISPAWN_MPIRUN_RANK_11=635 MPISPAWN_MPIRUN_RANK_12=636 MPISPAWN_MPIRUN_RANK_13=637 MPISPAWN_MPIRUN_RANK_14=638 MPISPAWN_MPIRUN_RANK_15=639 /opt/apps/gcc4_7/mvapich2/1.9/bin/mpispawn 0
bash: line 1: 42132 Segmentation fault      /usr/bin/env LD_LIBRARY_PATH=/opt/apps/gcc4_7/mvapich2/1.9/lib:/opt/apps/gcc4_7/mvapich2/1.9/lib/shared:/opt/apps/gcc/4.7.1/lib64:/opt/apps/gcc/4.7.1/lib:/opt/apps/xsede/gsi-openssh-5.7/lib64:/opt/apps/xsede/gsi-openssh-5.7/lib64 MPISPAWN_MPIRUN_MPD=0 USE_LINEAR_SSH=1 MPISPAWN_MPIRUN_HOST=c468-601.stampede.tacc.utexas.edu MPISPAWN_MPIRUN_HOSTIP=129.114.76.165 MPIRUN_RSH_LAUNCH=1 MPISPAWN_CHECKIN_PORT=36101 MPISPAWN_MPIRUN_PORT=36101 MPISPAWN_NNODES=64 MPISPAWN_GLOBAL_NPROCS=1024 MPISPAWN_MPIRUN_ID=44295 MPISPAWN_ARGC=4 MPDMAN_KVS_TEMPLATE=kvs_513_c468-601.stampede.tacc.utexas.edu_44295 MPISPAWN_LOCAL_NPROCS=16 MPISPAWN_ARGV_0='/home1/03222/tg825207/parallelnbody/teamscatter' MPISPAWN_ARGV_1='256000' MPISPAWN_ARGV_2='-c' MPISPAWN_ARGV_3='2' MPISPAWN_ARGC=4 MPISPAWN_GENERIC_ENV_COUNT=106 MPISPAWN_GENERIC_NAME_0=TACC_ENVLEN MPISPAWN_GENERIC_VALUE_0=6578 MPISPAWN_GENERIC_NAME_1=SLURM_JOB_ID MPISPAWN_GENERIC_VALUE_1="4306264" MPISPAWN_GENERIC_NAME_2=SLURMD_NODENAME MPISPAWN_GENERIC_VALUE_2="c468-601" MPISPAWN_GENERIC_NAME_3=STOCKYARD MPISPAWN_GENERIC_VALUE_3="/work/03222/tg825207" MPISPAWN_GENERIC_NAME_4=LC_ALL MPISPAWN_GENERIC_VALUE_4="en_US.UTF-8" MPISPAWN_GENERIC_NAME_5=SLURM_CHECKPOINT_IMAGE_DIR MPISPAWN_GENERIC_VALUE_5="/home1/03222/tg825207/parallelnbody" MPISPAWN_GENERIC_NAME_6=LMOD_FAMILY_MPI MPISPAWN_GENERIC_VALUE_6="mvapich2" MPISPAWN_GENERIC_NAME_7=SLURM_JOBID MPISPAWN_GENERIC_VALUE_7="4306264" MPISPAWN_GENERIC_NAME_8=_ModuleTable001_ MPISPAWN_GENERIC_VALUE_8="X01vZHVsZVRhYmxlXz17WyJhY3RpdmVTaXplIl09OCxiYXNlTXBhdGhBPXsiL29wdC9hcHBzL3hzZWRlL21vZHVsZWZpbGVzIiwiL29wdC9hcHBzL21vZHVsZWZpbGVzIiwiL29wdC9tb2R1bGVmaWxlcyIsfSxbImNfcmVidWlsZFRpbWUiXT1mYWxzZSxbImNfc2hvcnRUaW1lIl09ZmFsc2UsZmFtaWx5PXtbIk1QSSJdPSJtdmFwaWNoMiIsWyJjb21waWxlciJdPSJnY2MiLH0saW5hY3RpdmU9e30sbVQ9e0xpbnV4PXtbIkZOIl09Ii9vcHQvbW9kdWxlZmlsZXMvTGludXgiLFsiZGVmYXVsdCJdPTAsWyJmdWxsTmFtZSJdPSJMaW51eCIsWyJsb2FkT3JkZXIiXT0yLHByb3BUPXt9LFsic2hvcnQiXT0iTGludXgiLFsic3RhdHVzIl09ImFjdGl2ZSIsfSxUQUNDPXtbIkZOIl09Ii9v" MPISPAWN_GENERIC_NAME_9=TACC_SYSTEM MPISPAWN_GENERIC_VALUE_9="stampede" MPISPAWN_GENERIC_NAME_10=WORK MPISPAWN_GENERIC_VALUE_10="/work/03222/tg825207" MPISPAWN_GENERIC_NAME_11=SLURM_GTIDS MPISPAWN_GENERIC_VALUE_11="0" MPISPAWN_GENERIC_NAME_12=GCC_LIB MPISPAWN_GENERIC_VALUE_12="/opt/apps/gcc/4.7.1/lib64" MPISPAWN_GENERIC_NAME_13=SLURM_PRIO_PROCESS MPISPAWN_GENERIC_VALUE_13="0" MPISPAWN_GENERIC_NAME_14=SCRATCH MPISPAWN_GENERIC_VALUE_14="/scratch/03222/tg825207" MPISPAWN_GENERIC_NAME_15=LOGNAME MPISPAWN_GENERIC_VALUE_15="tg825207" MPISPAWN_GENERIC_NAME_16=_ModuleTable002_ MPISPAWN_GENERIC_VALUE_16="cHQvbW9kdWxlZmlsZXMvVEFDQyIsWyJkZWZhdWx0Il09MCxbImZ1bGxOYW1lIl09IlRBQ0MiLFsibG9hZE9yZGVyIl09Nixwcm9wVD17fSxbInNob3J0Il09IlRBQ0MiLFsic3RhdHVzIl09ImFjdGl2ZSIsfSxbIlRBQ0MtcGF0aHMiXT17WyJGTiJdPSIvb3B0L21vZHVsZWZpbGVzL1RBQ0MtcGF0aHMiLFsiZGVmYXVsdCJdPTAsWyJmdWxsTmFtZSJdPSJUQUNDLXBhdGhzIixbImxvYWRPcmRlciJdPTEscHJvcFQ9e30sWyJzaG9ydCJdPSJUQUNDLXBhdGhzIixbInN0YXR1cyJdPSJhY3RpdmUiLH0sY2x1c3Rlcj17WyJGTiJdPSIvb3B0L21vZHVsZWZpbGVzL2NsdXN0ZXIiLFsiZGVmYXVsdCJdPTAsWyJmdWxsTmFtZSJdPSJjbHVzdGVyIixbImxvYWRPcmRlciJdPTUscHJvcFQ9" MPISPAWN_GENERIC_NAME_17=INPUTRC MPISPAWN_GENERIC_VALUE_17="/etc/inputrc" MPISPAWN_GENERIC_NAME_18=LMOD_CMD MPISPAWN_GENERIC_VALUE_18="/opt/apps/lmod/lmod/libexec/lmod" MPISPAWN_GENERIC_NAME_19=I_MPI_OFA_ADAPTER_NAME MPISPAWN_GENERIC_VALUE_19="mlx4_0" MPISPAWN_GENERIC_NAME_20=QTLIB MPISPAWN_GENERIC_VALUE_20="/usr/lib64/qt-3.3/lib" MPISPAWN_GENERIC_NAME_21=NODE_TASKS_PPN_INFO MPISPAWN_GENERIC_VALUE_21=""16,0_"" MPISPAWN_GENERIC_NAME_22=SLURM_CPUS_ON_NODE MPISPAWN_GENERIC_VALUE_22="16" MPISPAWN_GENERIC_NAME_23=PATH MPISPAWN_GENERIC_VALUE_23="/opt/apps/gcc4_7/mvapich2/1.9/bin:/opt/apps/gcc/4.7.1/bin:/opt/apps/xalt/0.4.4/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/opt/apps/xsede/gsi-openssh-5.7/bin:/usr/X11R6/bin:/opt/ofed/bin:/opt/ofed/sbin:." MPISPAWN_GENERIC_NAME_24=SLURM_TACC_JOBNAME MPISPAWN_GENERIC_VALUE_24="stampedeLargeScaleAllTS.sh" MPISPAWN_GENERIC_NAME_25=SLURM_NODEID MPISPAWN_GENERIC_VALUE_25="0" MPISPAWN_GENERIC_NAME_26=LMOD_PKG MPISPAWN_GENERIC_VALUE_26="/opt/apps/lmod/lmod" MPISPAWN_GENERIC_NAME_27=MIC_I_MPI_OFA_ADAPTER_NAME MPISPAWN_GENERIC_VALUE_27="mlx4_0" MPISPAWN_GENERIC_NAME_28=HISTSIZE MPISPAWN_GENERIC_VALUE_28="1000" MPISPAWN_GENERIC_NAME_29=SLURM_NTASKS MPISPAWN_GENERIC_VALUE_29="2048" MPISPAWN_GENERIC_NAME_30=SHLIB_PATH MPISPAWN_GENERIC_VALUE_30="/opt/apps/xsede/gsi-openssh-5.7/lib64" MPISPAWN_GENERIC_NAME_31=TACC_OVERRIDE_PROJECT MPISPAWN_GENERIC_VALUE_31="TG-ASC140020" MPISPAWN_GENERIC_NAME_32=_ModuleTable005_ MPISPAWN_GENERIC_VALUE_32="Z2NjNF83L21vZHVsZWZpbGVzIiwiL29wdC9hcHBzL3hzZWRlL21vZHVsZWZpbGVzIiwiL29wdC9hcHBzL21vZHVsZWZpbGVzIiwiL29wdC9tb2R1bGVmaWxlcyIsfSxbInN5c3RlbUJhc2VNUEFUSCJdPSIvb3B0L2FwcHMveHNlZGUvbW9kdWxlZmlsZXM6L29wdC9hcHBzL21vZHVsZWZpbGVzOi9vcHQvbW9kdWxlZmlsZXMiLFsidmVyc2lvbiJdPTIsfQ==" MPISPAWN_GENERIC_NAME_33=_ModuleTable_Sz_ MPISPAWN_GENERIC_VALUE_33="5" MPISPAWN_GENERIC_NAME_34=MAIL MPISPAWN_GENERIC_VALUE_34="/var/spool/mail/tg825207" MPISPAWN_GENERIC_NAME_35=MV2_USE_HUGEPAGES MPISPAWN_GENERIC_VALUE_35="0" MPISPAWN_GENERIC_NAME_36=SLURM_TOPOLOGY_ADDR MPISPAWN_GENERIC_VALUE_36="c468-601" MPISPAWN_GENERIC_NAME_37=TACC_XALT_DIR MPISPAWN_GENERIC_VALUE_37="/opt/apps/xalt/0.4.4/" MPISPAWN_GENERIC_NAME_38=QTDIR MPISPAWN_GENERIC_VALUE_38="/usr/lib64/qt-3.3" MPISPAWN_GENERIC_NAME_39=SYSTEM MPISPAWN_GENERIC_VALUE_39="linux" MPISPAWN_GENERIC_NAME_40=SLURM_TACC_RUNLIMIT_MINS MPISPAWN_GENERIC_VALUE_40="180" MPISPAWN_GENERIC_NAME_41=SLURM_SUBMIT_DIR MPISPAWN_GENERIC_VALUE_41="/home1/03222/tg825207/parallelnbody" MPISPAWN_GENERIC_NAME_42=LIBPATH MPISPAWN_GENERIC_VALUE_42="/opt/apps/xsede/gsi-openssh-5.7/lib64:/usr/lib:/lib" MPISPAWN_GENERIC_NAME_43=HOSTNAME MPISPAWN_GENERIC_VALUE_43="c468-601" MPISPAWN_GENERIC_NAME_44=LMOD_DEFAULT_MODULEPATH MPISPAWN_GENERIC_VALUE_44="/opt/apps/xsede/modulefiles:/opt/apps/modulefiles:/opt/modulefiles" MPISPAWN_GENERIC_NAME_45=ARCHIVE MPISPAWN_GENERIC_VALUE_45="/home/03222/tg825207" MPISPAWN_GENERIC_NAME_46=_ MPISPAWN_GENERIC_VALUE_46="/usr/local/bin/build_env.pl" MPISPAWN_GENERIC_NAME_47=__BASHRC_SOURCED__ MPISPAWN_GENERIC_VALUE_47="1" MPISPAWN_GENERIC_NAME_48=APPS MPISPAWN_GENERIC_VALUE_48="/opt/apps" MPISPAWN_GENERIC_NAME_49=SHELL MPISPAWN_GENERIC_VALUE_49="/bin/bash" MPISPAWN_GENERIC_NAME_50=ARMCI_OPENIB_DEVICE MPISPAWN_GENERIC_VALUE_50="mlx4_0" MPISPAWN_GENERIC_NAME_51=ENVIRONMENT MPISPAWN_GENERIC_VALUE_51="BATCH" MPISPAWN_GENERIC_NAME_52=TACC_FAMILY_MPI MPISPAWN_GENERIC_VALUE_52="mvapich2" MPISPAWN_GENERIC_NAME_53=MV2_IBA_HCA MPISPAWN_GENERIC_VALUE_53="mlx4_0" MPISPAWN_GENERIC_NAME_54=TACC_PUBLIC_MACHINE MPISPAWN_GENERIC_VALUE_54="1" MPISPAWN_GENERIC_NAME_55=SLURM_TACC_NODES MPISPAWN_GENERIC_VALUE_55="1" MPISPAWN_GENERIC_NAME_56=_ModuleTable003_ MPISPAWN_GENERIC_VALUE_56="e30sWyJzaG9ydCJdPSJjbHVzdGVyIixbInN0YXR1cyJdPSJhY3RpdmUiLH0sWyJjbHVzdGVyLXBhdGhzIl09e1siRk4iXT0iL29wdC9tb2R1bGVmaWxlcy9jbHVzdGVyLXBhdGhzIixbImRlZmF1bHQiXT0wLFsiZnVsbE5hbWUiXT0iY2x1c3Rlci1wYXRocyIsWyJsb2FkT3JkZXIiXT0zLHByb3BUPXt9LFsic2hvcnQiXT0iY2x1c3Rlci1wYXRocyIsWyJzdGF0dXMiXT0iYWN0aXZlIix9LGdjYz17WyJGTiJdPSIvb3B0L2FwcHMvbW9kdWxlZmlsZXMvZ2NjLzQuNy4xLmx1YSIsWyJkZWZhdWx0Il09MCxbImZ1bGxOYW1lIl09ImdjYy80LjcuMSIsWyJsb2FkT3JkZXIiXT03LHByb3BUPXt9LFsic2hvcnQiXT0iZ2NjIixbInN0YXR1cyJdPSJhY3RpdmUiLH0sbXZhcGljaDI9e1si" MPISPAWN_GENERIC_NAME_57=QTINC MPISPAWN_GENERIC_VALUE_57="/usr/lib64/qt-3.3/include" MPISPAWN_GENERIC_NAME_58=CVS_RSH MPISPAWN_GENERIC_VALUE_58="ssh" MPISPAWN_GENERIC_NAME_59=MV2_USE_OLD_BCAST MPISPAWN_GENERIC_VALUE_59="1" MPISPAWN_GENERIC_NAME_60=SLURM_JOB_NUM_NODES MPISPAWN_GENERIC_VALUE_60="128" MPISPAWN_GENERIC_NAME_61=SLURM_TASK_PID MPISPAWN_GENERIC_VALUE_61="31479" MPISPAWN_GENERIC_NAME_62=TACC_MPI_GETMODE MPISPAWN_GENERIC_VALUE_62="mvapich2_ssh" MPISPAWN_GENERIC_NAME_63=LANG MPISPAWN_GENERIC_VALUE_63="en_US.UTF-8" MPISPAWN_GENERIC_NAME_64=USER MPISPAWN_GENERIC_VALUE_64="tg825207" MPISPAWN_GENERIC_NAME_65=MIC_I_MPI_DAPL_PROVIDER MPISPAWN_GENERIC_VALUE_65="ofa-v2-mlx4_0-1u" MPISPAWN_GENERIC_NAME_66=G_BROKEN_FILENAMES MPISPAWN_GENERIC_VALUE_66="1" MPISPAWN_GENERIC_NAME_67=SLURM_LOCALID MPISPAWN_GENERIC_VALUE_67="0" MPISPAWN_GENERIC_NAME_68=SLURM_NPROCS MPISPAWN_GENERIC_VALUE_68="2048" MPISPAWN_GENERIC_NAME_69=SLURM_TACC_NNODES_SET MPISPAWN_GENERIC_VALUE_69="0" MPISPAWN_GENERIC_NAME_70=SLURM_TACC_CORES MPISPAWN_GENERIC_VALUE_70="2048" MPISPAWN_GENERIC_NAME_71=LMOD_DIR MPISPAWN_GENERIC_VALUE_71="/opt/apps/lmod/lmod/libexec" MPISPAWN_GENERIC_NAME_72=LMOD_SETTARG_CMD MPISPAWN_GENERIC_VALUE_72=":" MPISPAWN_GENERIC_NAME_73=OMP_NUM_THREADS MPISPAWN_GENERIC_VALUE_73="1" MPISPAWN_GENERIC_NAME_74=_ModuleTable004_ MPISPAWN_GENERIC_VALUE_74="Rk4iXT0iL29wdC9hcHBzL2djYzRfNy9tb2R1bGVmaWxlcy9tdmFwaWNoMi8xLjlhMiIsWyJkZWZhdWx0Il09MSxbImZ1bGxOYW1lIl09Im12YXBpY2gyLzEuOWEyIixbImxvYWRPcmRlciJdPTgscHJvcFQ9e30sWyJzaG9ydCJdPSJtdmFwaWNoMiIsWyJzdGF0dXMiXT0iYWN0aXZlIix9LHhhbHQ9e1siRk4iXT0iL29wdC9hcHBzL21vZHVsZWZpbGVzL3hhbHQvMC40LjQubHVhIixbImRlZmF1bHQiXT0xLFsiZnVsbE5hbWUiXT0ieGFsdC8wLjQuNCIsWyJsb2FkT3JkZXIiXT00LHByb3BUPXt9LFsic2hvcnQiXT0ieGFsdCIsWyJzdGF0dXMiXT0iYWN0aXZlIix9LH0sbXBhdGhBPXsiL29wdC9hcHBzL2djYzRfNy9tdmFwaWNoMl8xXzkvbW9kdWxlZmlsZXMiLCIvb3B0L2FwcHMv" MPISPAWN_GENERIC_NAME_75=__PERSONAL_PATH__ MPISPAWN_GENERIC_VALUE_75="1" MPISPAWN_GENERIC_NAME_76=LMOD_SYSTEM_DEFAULT_MODULES MPISPAWN_GENERIC_VALUE_76="TACC" MPISPAWN_GENERIC_NAME_77=SLURM_JOB_NAME MPISPAWN_GENERIC_VALUE_77="stampedeLargeScaleAllTS.sh" MPISPAWN_GENERIC_NAME_78=SLURM_SUBMIT_HOST MPISPAWN_GENERIC_VALUE_78="login4.stampede.tacc.utexas.edu" MPISPAWN_GENERIC_NAME_79=LD_LIBRARY_PATH MPISPAWN_GENERIC_VALUE_79="/opt/apps/gcc4_7/mvapich2/1.9/lib:/opt/apps/gcc4_7/mvapich2/1.9/lib/shared:/opt/apps/gcc/4.7.1/lib64:/opt/apps/gcc/4.7.1/lib:/opt/apps/xsede/gsi-openssh-5.7/lib64:/opt/apps/xsede/gsi-openssh-5.7/lib64" MPISPAWN_GENERIC_NAME_80=SLURM_QUEUE MPISPAWN_GENERIC_VALUE_80="normal" MPISPAWN_GENERIC_NAME_81=PKG_CONFIG_PATH MPISPAWN_GENERIC_VALUE_81="/opt/apps/gcc4_7/mvapich2/1.9/lib/pkgconfig" MPISPAWN_GENERIC_NAME_82=LMOD_PREPEND_BLOCK MPISPAWN_GENERIC_VALUE_82="normal" MPISPAWN_GENERIC_NAME_83=TACC_FAMILY_COMPILER MPISPAWN_GENERIC_VALUE_83="gcc" MPISPAWN_GENERIC_NAME_84=TACC_DOMAIN MPISPAWN_GENERIC_VALUE_84="stampede" MPISPAWN_GENERIC_NAME_85=MV2_HOMOGENEOUS_CLUSTER MPISPAWN_GENERIC_VALUE_85="1" MPISPAWN_GENERIC_NAME_86=LMOD_FAMILY_COMPILER MPISPAWN_GENERIC_VALUE_86="gcc" MPISPAWN_GENERIC_NAME_87=MV2_USE_UD_HYBRID MPISPAWN_GENERIC_VALUE_87="0" MPISPAWN_GENERIC_NAME_88=MPICH_HOME MPISPAWN_GENERIC_VALUE_88="/opt/apps/gcc4_7/mvapich2/1.9" MPISPAWN_GENERIC_NAME_89=SLURM_JOB_NODELIST MPISPAWN_GENERIC_VALUE_89="c468-601,c471-503,c472-[401-402],c473-[303-304,401,804,901],c474-[203-204,301,401,403-404],c478-303,c482-404,c483-[001-004,101,504,601,804,901],c484-[101,103-104,201,602,701],c485-502,c486-102,c490-[302,904],c491-004,c492-701,c493-[001,102,201],c494-[101,402-403,602,702-703,904],c495-[001-002,201],c497-[102,202,504,601-603],c498-[304,402,603],c501-[204,301,604,801],c502-104,c503-901,c504-[602,604,702,801],c505-[001,202-204,401,502-504,802,901-904],c506-[302,704],c507-[102,202,503,601,904],c508-[101,204,601,702-703,802,903-904],c509-[002,201,301,303,401,501-502],c510-[003,401,404,501,503],c511-[601-602,902-903],c536-[801,902],c537-902,c538-[001,003,104],c540-[104,201-202,804,901-904]" MPISPAWN_GENERIC_NAME_90=SLURM_PROCID MPISPAWN_GENERIC_VALUE_90="0" MPISPAWN_GENERIC_NAME_91=I_MPI_DAPL_PROVIDER MPISPAWN_GENERIC_VALUE_91="ofa-v2-mlx4_0-1u" MPISPAWN_GENERIC_NAME_92=GLOBUS_LOCATION MPISPAWN_GENERIC_VALUE_92="/opt/apps/xsede/gsi-openssh-5.7" MPISPAWN_GENERIC_NAME_93=LMOD_COLORIZE MPISPAWN_GENERIC_VALUE_93="yes" MPISPAWN_GENERIC_NAME_94=I_MPI_HYDRA_BRANCH_COUNT MPISPAWN_GENERIC_VALUE_94="6000" MPISPAWN_GENERIC_NAME_95=SLURM_NNODES MPISPAWN_GENERIC_VALUE_95="128" MPISPAWN_GENERIC_NAME_96=MV2_DEFAULT_TIME_OUT MPISPAWN_GENERIC_VALUE_96="23" MPISPAWN_GENERIC_NAME_97=SLURM_TACC_NCORES_SET MPISPAWN_GENERIC_VALUE_97="1" MPISPAWN_GENERIC_NAME_98=__Init_Default_Modules MPISPAWN_GENERIC_VALUE_98="1" MPISPAWN_GENERIC_NAME_99=BASH_ENV MPISPAWN_GENERIC_VALUE_99="/etc/tacc/tacc_functions" MPISPAWN_GENERIC_NAME_100=SLURM_TOPOLOGY_ADDR_PATTERN MPISPAWN_GENERIC_VALUE_100="node" MPISPAWN_GENERIC_NAME_101=MV2_USE_RING_STARTUP MPISPAWN_GENERIC_VALUE_101="0" MPISPAWN_GENERIC_NAME_102=TACC_XALT_BIN MPISPAWN_GENERIC_VALUE_102="/opt/apps/xalt/0.4.4/bin" MPISPAWN_GENERIC_NAME_103=X509_USER_PROXY MPISPAWN_GENERIC_VALUE_103="/tmp/x509up_p3084.file9zM3fu.1" MPISPAWN_GENERIC_NAME_104=TMPDIR MPISPAWN_GENERIC_VALUE_104="/tmp" MPISPAWN_GENERIC_NAME_105=LMOD_FULL_SETTARG_SUPPORT MPISPAWN_GENERIC_VALUE_105="no" MPISPAWN_ID=31 MPISPAWN_WORKING_DIR=/home1/03222/tg825207/parallelnbody MPISPAWN_MPIRUN_RANK_0=496 MPISPAWN_MPIRUN_RANK_1=497 MPISPAWN_MPIRUN_RANK_2=498 MPISPAWN_MPIRUN_RANK_3=499 MPISPAWN_MPIRUN_RANK_4=500 MPISPAWN_MPIRUN_RANK_5=501 MPISPAWN_MPIRUN_RANK_6=502 MPISPAWN_MPIRUN_RANK_7=503 MPISPAWN_MPIRUN_RANK_8=504 MPISPAWN_MPIRUN_RANK_9=505 MPISPAWN_MPIRUN_RANK_10=506 MPISPAWN_MPIRUN_RANK_11=507 MPISPAWN_MPIRUN_RANK_12=508 MPISPAWN_MPIRUN_RANK_13=509 MPISPAWN_MPIRUN_RANK_14=510 MPISPAWN_MPIRUN_RANK_15=511 /opt/apps/gcc4_7/mvapich2/1.9/bin/mpispawn 0
[c493-102.stampede.tacc.utexas.edu:mpispawn_39][error_sighandler] Caught error: Segmentation fault (signal 11)
[c483-504.stampede.tacc.utexas.edu:mpispawn_22][error_sighandler] Caught error: Segmentation fault (signal 11)
bash: line 1: 12839 Segmentation fault      /usr/bin/env LD_LIBRARY_PATH=/opt/apps/gcc4_7/mvapich2/1.9/lib:/opt/apps/gcc4_7/mvapich2/1.9/lib/shared:/opt/apps/gcc/4.7.1/lib64:/opt/apps/gcc/4.7.1/lib:/opt/apps/xsede/gsi-openssh-5.7/lib64:/opt/apps/xsede/gsi-openssh-5.7/lib64 MPISPAWN_MPIRUN_MPD=0 USE_LINEAR_SSH=1 MPISPAWN_MPIRUN_HOST=c468-601.stampede.tacc.utexas.edu MPISPAWN_MPIRUN_HOSTIP=129.114.76.165 MPIRUN_RSH_LAUNCH=1 MPISPAWN_CHECKIN_PORT=56630 MPISPAWN_MPIRUN_PORT=56630 MPISPAWN_NNODES=128 MPISPAWN_GLOBAL_NPROCS=2048 MPISPAWN_MPIRUN_ID=47451 MPISPAWN_ARGC=4 MPDMAN_KVS_TEMPLATE=kvs_437_c468-601.stampede.tacc.utexas.edu_47451 MPISPAWN_LOCAL_NPROCS=16 MPISPAWN_ARGV_0='/home1/03222/tg825207/parallelnbody/teamscatter' MPISPAWN_ARGV_1='256000' MPISPAWN_ARGV_2='-c' MPISPAWN_ARGV_3='2' MPISPAWN_ARGC=4 MPISPAWN_GENERIC_ENV_COUNT=106 MPISPAWN_GENERIC_NAME_0=TACC_ENVLEN MPISPAWN_GENERIC_VALUE_0=6578 MPISPAWN_GENERIC_NAME_1=SLURM_JOB_ID MPISPAWN_GENERIC_VALUE_1="4306264" MPISPAWN_GENERIC_NAME_2=SLURMD_NODENAME MPISPAWN_GENERIC_VALUE_2="c468-601" MPISPAWN_GENERIC_NAME_3=STOCKYARD MPISPAWN_GENERIC_VALUE_3="/work/03222/tg825207" MPISPAWN_GENERIC_NAME_4=LC_ALL MPISPAWN_GENERIC_VALUE_4="en_US.UTF-8" MPISPAWN_GENERIC_NAME_5=SLURM_CHECKPOINT_IMAGE_DIR MPISPAWN_GENERIC_VALUE_5="/home1/03222/tg825207/parallelnbody" MPISPAWN_GENERIC_NAME_6=LMOD_FAMILY_MPI MPISPAWN_GENERIC_VALUE_6="mvapich2" MPISPAWN_GENERIC_NAME_7=SLURM_JOBID MPISPAWN_GENERIC_VALUE_7="4306264" MPISPAWN_GENERIC_NAME_8=_ModuleTable001_ MPISPAWN_GENERIC_VALUE_8="X01vZHVsZVRhYmxlXz17WyJhY3RpdmVTaXplIl09OCxiYXNlTXBhdGhBPXsiL29wdC9hcHBzL3hzZWRlL21vZHVsZWZpbGVzIiwiL29wdC9hcHBzL21vZHVsZWZpbGVzIiwiL29wdC9tb2R1bGVmaWxlcyIsfSxbImNfcmVidWlsZFRpbWUiXT1mYWxzZSxbImNfc2hvcnRUaW1lIl09ZmFsc2UsZmFtaWx5PXtbIk1QSSJdPSJtdmFwaWNoMiIsWyJjb21waWxlciJdPSJnY2MiLH0saW5hY3RpdmU9e30sbVQ9e0xpbnV4PXtbIkZOIl09Ii9vcHQvbW9kdWxlZmlsZXMvTGludXgiLFsiZGVmYXVsdCJdPTAsWyJmdWxsTmFtZSJdPSJMaW51eCIsWyJsb2FkT3JkZXIiXT0yLHByb3BUPXt9LFsic2hvcnQiXT0iTGludXgiLFsic3RhdHVzIl09ImFjdGl2ZSIsfSxUQUNDPXtbIkZOIl09Ii9v" MPISPAWN_GENERIC_NAME_9=TACC_SYSTEM MPISPAWN_GENERIC_VALUE_9="stampede" MPISPAWN_GENERIC_NAME_10=WORK MPISPAWN_GENERIC_VALUE_10="/work/03222/tg825207" MPISPAWN_GENERIC_NAME_11=SLURM_GTIDS MPISPAWN_GENERIC_VALUE_11="0" MPISPAWN_GENERIC_NAME_12=GCC_LIB MPISPAWN_GENERIC_VALUE_12="/opt/apps/gcc/4.7.1/lib64" MPISPAWN_GENERIC_NAME_13=SLURM_PRIO_PROCESS MPISPAWN_GENERIC_VALUE_13="0" MPISPAWN_GENERIC_NAME_14=SCRATCH MPISPAWN_GENERIC_VALUE_14="/scratch/03222/tg825207" MPISPAWN_GENERIC_NAME_15=LOGNAME MPISPAWN_GENERIC_VALUE_15="tg825207" MPISPAWN_GENERIC_NAME_16=_ModuleTable002_ MPISPAWN_GENERIC_VALUE_16="cHQvbW9kdWxlZmlsZXMvVEFDQyIsWyJkZWZhdWx0Il09MCxbImZ1bGxOYW1lIl09IlRBQ0MiLFsibG9hZE9yZGVyIl09Nixwcm9wVD17fSxbInNob3J0Il09IlRBQ0MiLFsic3RhdHVzIl09ImFjdGl2ZSIsfSxbIlRBQ0MtcGF0aHMiXT17WyJGTiJdPSIvb3B0L21vZHVsZWZpbGVzL1RBQ0MtcGF0aHMiLFsiZGVmYXVsdCJdPTAsWyJmdWxsTmFtZSJdPSJUQUNDLXBhdGhzIixbImxvYWRPcmRlciJdPTEscHJvcFQ9e30sWyJzaG9ydCJdPSJUQUNDLXBhdGhzIixbInN0YXR1cyJdPSJhY3RpdmUiLH0sY2x1c3Rlcj17WyJGTiJdPSIvb3B0L21vZHVsZWZpbGVzL2NsdXN0ZXIiLFsiZGVmYXVsdCJdPTAsWyJmdWxsTmFtZSJdPSJjbHVzdGVyIixbImxvYWRPcmRlciJdPTUscHJvcFQ9" MPISPAWN_GENERIC_NAME_17=INPUTRC MPISPAWN_GENERIC_VALUE_17="/etc/inputrc" MPISPAWN_GENERIC_NAME_18=LMOD_CMD MPISPAWN_GENERIC_VALUE_18="/opt/apps/lmod/lmod/libexec/lmod" MPISPAWN_GENERIC_NAME_19=I_MPI_OFA_ADAPTER_NAME MPISPAWN_GENERIC_VALUE_19="mlx4_0" MPISPAWN_GENERIC_NAME_20=QTLIB MPISPAWN_GENERIC_VALUE_20="/usr/lib64/qt-3.3/lib" MPISPAWN_GENERIC_NAME_21=NODE_TASKS_PPN_INFO MPISPAWN_GENERIC_VALUE_21=""16,0_"" MPISPAWN_GENERIC_NAME_22=SLURM_CPUS_ON_NODE MPISPAWN_GENERIC_VALUE_22="16" MPISPAWN_GENERIC_NAME_23=PATH MPISPAWN_GENERIC_VALUE_23="/opt/apps/gcc4_7/mvapich2/1.9/bin:/opt/apps/gcc/4.7.1/bin:/opt/apps/xalt/0.4.4/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/opt/apps/xsede/gsi-openssh-5.7/bin:/usr/X11R6/bin:/opt/ofed/bin:/opt/ofed/sbin:." MPISPAWN_GENERIC_NAME_24=SLURM_TACC_JOBNAME MPISPAWN_GENERIC_VALUE_24="stampedeLargeScaleAllTS.sh" MPISPAWN_GENERIC_NAME_25=SLURM_NODEID MPISPAWN_GENERIC_VALUE_25="0" MPISPAWN_GENERIC_NAME_26=LMOD_PKG MPISPAWN_GENERIC_VALUE_26="/opt/apps/lmod/lmod" MPISPAWN_GENERIC_NAME_27=MIC_I_MPI_OFA_ADAPTER_NAME MPISPAWN_GENERIC_VALUE_27="mlx4_0" MPISPAWN_GENERIC_NAME_28=HISTSIZE MPISPAWN_GENERIC_VALUE_28="1000" MPISPAWN_GENERIC_NAME_29=SLURM_NTASKS MPISPAWN_GENERIC_VALUE_29="2048" MPISPAWN_GENERIC_NAME_30=SHLIB_PATH MPISPAWN_GENERIC_VALUE_30="/opt/apps/xsede/gsi-openssh-5.7/lib64" MPISPAWN_GENERIC_NAME_31=TACC_OVERRIDE_PROJECT MPISPAWN_GENERIC_VALUE_31="TG-ASC140020" MPISPAWN_GENERIC_NAME_32=_ModuleTable005_ MPISPAWN_GENERIC_VALUE_32="Z2NjNF83L21vZHVsZWZpbGVzIiwiL29wdC9hcHBzL3hzZWRlL21vZHVsZWZpbGVzIiwiL29wdC9hcHBzL21vZHVsZWZpbGVzIiwiL29wdC9tb2R1bGVmaWxlcyIsfSxbInN5c3RlbUJhc2VNUEFUSCJdPSIvb3B0L2FwcHMveHNlZGUvbW9kdWxlZmlsZXM6L29wdC9hcHBzL21vZHVsZWZpbGVzOi9vcHQvbW9kdWxlZmlsZXMiLFsidmVyc2lvbiJdPTIsfQ==" MPISPAWN_GENERIC_NAME_33=_ModuleTable_Sz_ MPISPAWN_GENERIC_VALUE_33="5" MPISPAWN_GENERIC_NAME_34=MAIL MPISPAWN_GENERIC_VALUE_34="/var/spool/mail/tg825207" MPISPAWN_GENERIC_NAME_35=MV2_USE_HUGEPAGES MPISPAWN_GENERIC_VALUE_35="0" MPISPAWN_GENERIC_NAME_36=SLURM_TOPOLOGY_ADDR MPISPAWN_GENERIC_VALUE_36="c468-601" MPISPAWN_GENERIC_NAME_37=TACC_XALT_DIR MPISPAWN_GENERIC_VALUE_37="/opt/apps/xalt/0.4.4/" MPISPAWN_GENERIC_NAME_38=QTDIR MPISPAWN_GENERIC_VALUE_38="/usr/lib64/qt-3.3" MPISPAWN_GENERIC_NAME_39=SYSTEM MPISPAWN_GENERIC_VALUE_39="linux" MPISPAWN_GENERIC_NAME_40=SLURM_TACC_RUNLIMIT_MINS MPISPAWN_GENERIC_VALUE_40="180" MPISPAWN_GENERIC_NAME_41=SLURM_SUBMIT_DIR MPISPAWN_GENERIC_VALUE_41="/home1/03222/tg825207/parallelnbody" MPISPAWN_GENERIC_NAME_42=LIBPATH MPISPAWN_GENERIC_VALUE_42="/opt/apps/xsede/gsi-openssh-5.7/lib64:/usr/lib:/lib" MPISPAWN_GENERIC_NAME_43=HOSTNAME MPISPAWN_GENERIC_VALUE_43="c468-601" MPISPAWN_GENERIC_NAME_44=LMOD_DEFAULT_MODULEPATH MPISPAWN_GENERIC_VALUE_44="/opt/apps/xsede/modulefiles:/opt/apps/modulefiles:/opt/modulefiles" MPISPAWN_GENERIC_NAME_45=ARCHIVE MPISPAWN_GENERIC_VALUE_45="/home/03222/tg825207" MPISPAWN_GENERIC_NAME_46=_ MPISPAWN_GENERIC_VALUE_46="/usr/local/bin/build_env.pl" MPISPAWN_GENERIC_NAME_47=__BASHRC_SOURCED__ MPISPAWN_GENERIC_VALUE_47="1" MPISPAWN_GENERIC_NAME_48=APPS MPISPAWN_GENERIC_VALUE_48="/opt/apps" MPISPAWN_GENERIC_NAME_49=SHELL MPISPAWN_GENERIC_VALUE_49="/bin/bash" MPISPAWN_GENERIC_NAME_50=ARMCI_OPENIB_DEVICE MPISPAWN_GENERIC_VALUE_50="mlx4_0" MPISPAWN_GENERIC_NAME_51=ENVIRONMENT MPISPAWN_GENERIC_VALUE_51="BATCH" MPISPAWN_GENERIC_NAME_52=TACC_FAMILY_MPI MPISPAWN_GENERIC_VALUE_52="mvapich2" MPISPAWN_GENERIC_NAME_53=MV2_IBA_HCA MPISPAWN_GENERIC_VALUE_53="mlx4_0" MPISPAWN_GENERIC_NAME_54=TACC_PUBLIC_MACHINE MPISPAWN_GENERIC_VALUE_54="1" MPISPAWN_GENERIC_NAME_55=SLURM_TACC_NODES MPISPAWN_GENERIC_VALUE_55="1" MPISPAWN_GENERIC_NAME_56=_ModuleTable003_ MPISPAWN_GENERIC_VALUE_56="e30sWyJzaG9ydCJdPSJjbHVzdGVyIixbInN0YXR1cyJdPSJhY3RpdmUiLH0sWyJjbHVzdGVyLXBhdGhzIl09e1siRk4iXT0iL29wdC9tb2R1bGVmaWxlcy9jbHVzdGVyLXBhdGhzIixbImRlZmF1bHQiXT0wLFsiZnVsbE5hbWUiXT0iY2x1c3Rlci1wYXRocyIsWyJsb2FkT3JkZXIiXT0zLHByb3BUPXt9LFsic2hvcnQiXT0iY2x1c3Rlci1wYXRocyIsWyJzdGF0dXMiXT0iYWN0aXZlIix9LGdjYz17WyJGTiJdPSIvb3B0L2FwcHMvbW9kdWxlZmlsZXMvZ2NjLzQuNy4xLmx1YSIsWyJkZWZhdWx0Il09MCxbImZ1bGxOYW1lIl09ImdjYy80LjcuMSIsWyJsb2FkT3JkZXIiXT03LHByb3BUPXt9LFsic2hvcnQiXT0iZ2NjIixbInN0YXR1cyJdPSJhY3RpdmUiLH0sbXZhcGljaDI9e1si" MPISPAWN_GENERIC_NAME_57=QTINC MPISPAWN_GENERIC_VALUE_57="/usr/lib64/qt-3.3/include" MPISPAWN_GENERIC_NAME_58=CVS_RSH MPISPAWN_GENERIC_VALUE_58="ssh" MPISPAWN_GENERIC_NAME_59=MV2_USE_OLD_BCAST MPISPAWN_GENERIC_VALUE_59="1" MPISPAWN_GENERIC_NAME_60=SLURM_JOB_NUM_NODES MPISPAWN_GENERIC_VALUE_60="128" MPISPAWN_GENERIC_NAME_61=SLURM_TASK_PID MPISPAWN_GENERIC_VALUE_61="31479" MPISPAWN_GENERIC_NAME_62=TACC_MPI_GETMODE MPISPAWN_GENERIC_VALUE_62="mvapich2_ssh" MPISPAWN_GENERIC_NAME_63=LANG MPISPAWN_GENERIC_VALUE_63="en_US.UTF-8" MPISPAWN_GENERIC_NAME_64=USER MPISPAWN_GENERIC_VALUE_64="tg825207" MPISPAWN_GENERIC_NAME_65=MIC_I_MPI_DAPL_PROVIDER MPISPAWN_GENERIC_VALUE_65="ofa-v2-mlx4_0-1u" MPISPAWN_GENERIC_NAME_66=G_BROKEN_FILENAMES MPISPAWN_GENERIC_VALUE_66="1" MPISPAWN_GENERIC_NAME_67=SLURM_LOCALID MPISPAWN_GENERIC_VALUE_67="0" MPISPAWN_GENERIC_NAME_68=SLURM_NPROCS MPISPAWN_GENERIC_VALUE_68="2048" MPISPAWN_GENERIC_NAME_69=SLURM_TACC_NNODES_SET MPISPAWN_GENERIC_VALUE_69="0" MPISPAWN_GENERIC_NAME_70=SLURM_TACC_CORES MPISPAWN_GENERIC_VALUE_70="2048" MPISPAWN_GENERIC_NAME_71=LMOD_DIR MPISPAWN_GENERIC_VALUE_71="/opt/apps/lmod/lmod/libexec" MPISPAWN_GENERIC_NAME_72=LMOD_SETTARG_CMD MPISPAWN_GENERIC_VALUE_72=":" MPISPAWN_GENERIC_NAME_73=OMP_NUM_THREADS MPISPAWN_GENERIC_VALUE_73="1" MPISPAWN_GENERIC_NAME_74=_ModuleTable004_ MPISPAWN_GENERIC_VALUE_74="Rk4iXT0iL29wdC9hcHBzL2djYzRfNy9tb2R1bGVmaWxlcy9tdmFwaWNoMi8xLjlhMiIsWyJkZWZhdWx0Il09MSxbImZ1bGxOYW1lIl09Im12YXBpY2gyLzEuOWEyIixbImxvYWRPcmRlciJdPTgscHJvcFQ9e30sWyJzaG9ydCJdPSJtdmFwaWNoMiIsWyJzdGF0dXMiXT0iYWN0aXZlIix9LHhhbHQ9e1siRk4iXT0iL29wdC9hcHBzL21vZHVsZWZpbGVzL3hhbHQvMC40LjQubHVhIixbImRlZmF1bHQiXT0xLFsiZnVsbE5hbWUiXT0ieGFsdC8wLjQuNCIsWyJsb2FkT3JkZXIiXT00LHByb3BUPXt9LFsic2hvcnQiXT0ieGFsdCIsWyJzdGF0dXMiXT0iYWN0aXZlIix9LH0sbXBhdGhBPXsiL29wdC9hcHBzL2djYzRfNy9tdmFwaWNoMl8xXzkvbW9kdWxlZmlsZXMiLCIvb3B0L2FwcHMv" MPISPAWN_GENERIC_NAME_75=__PERSONAL_PATH__ MPISPAWN_GENERIC_VALUE_75="1" MPISPAWN_GENERIC_NAME_76=LMOD_SYSTEM_DEFAULT_MODULES MPISPAWN_GENERIC_VALUE_76="TACC" MPISPAWN_GENERIC_NAME_77=SLURM_JOB_NAME MPISPAWN_GENERIC_VALUE_77="stampedeLargeScaleAllTS.sh" MPISPAWN_GENERIC_NAME_78=SLURM_SUBMIT_HOST MPISPAWN_GENERIC_VALUE_78="login4.stampede.tacc.utexas.edu" MPISPAWN_GENERIC_NAME_79=LD_LIBRARY_PATH MPISPAWN_GENERIC_VALUE_79="/opt/apps/gcc4_7/mvapich2/1.9/lib:/opt/apps/gcc4_7/mvapich2/1.9/lib/shared:/opt/apps/gcc/4.7.1/lib64:/opt/apps/gcc/4.7.1/lib:/opt/apps/xsede/gsi-openssh-5.7/lib64:/opt/apps/xsede/gsi-openssh-5.7/lib64" MPISPAWN_GENERIC_NAME_80=SLURM_QUEUE MPISPAWN_GENERIC_VALUE_80="normal" MPISPAWN_GENERIC_NAME_81=PKG_CONFIG_PATH MPISPAWN_GENERIC_VALUE_81="/opt/apps/gcc4_7/mvapich2/1.9/lib/pkgconfig" MPISPAWN_GENERIC_NAME_82=LMOD_PREPEND_BLOCK MPISPAWN_GENERIC_VALUE_82="normal" MPISPAWN_GENERIC_NAME_83=TACC_FAMILY_COMPILER MPISPAWN_GENERIC_VALUE_83="gcc" MPISPAWN_GENERIC_NAME_84=TACC_DOMAIN MPISPAWN_GENERIC_VALUE_84="stampede" MPISPAWN_GENERIC_NAME_85=MV2_HOMOGENEOUS_CLUSTER MPISPAWN_GENERIC_VALUE_85="1" MPISPAWN_GENERIC_NAME_86=LMOD_FAMILY_COMPILER MPISPAWN_GENERIC_VALUE_86="gcc" MPISPAWN_GENERIC_NAME_87=MV2_USE_UD_HYBRID MPISPAWN_GENERIC_VALUE_87="0" MPISPAWN_GENERIC_NAME_88=MPICH_HOME MPISPAWN_GENERIC_VALUE_88="/opt/apps/gcc4_7/mvapich2/1.9" MPISPAWN_GENERIC_NAME_89=SLURM_JOB_NODELIST MPISPAWN_GENERIC_VALUE_89="c468-601,c471-503,c472-[401-402],c473-[303-304,401,804,901],c474-[203-204,301,401,403-404],c478-303,c482-404,c483-[001-004,101,504,601,804,901],c484-[101,103-104,201,602,701],c485-502,c486-102,c490-[302,904],c491-004,c492-701,c493-[001,102,201],c494-[101,402-403,602,702-703,904],c495-[001-002,201],c497-[102,202,504,601-603],c498-[304,402,603],c501-[204,301,604,801],c502-104,c503-901,c504-[602,604,702,801],c505-[001,202-204,401,502-504,802,901-904],c506-[302,704],c507-[102,202,503,601,904],c508-[101,204,601,702-703,802,903-904],c509-[002,201,301,303,401,501-502],c510-[003,401,404,501,503],c511-[601-602,902-903],c536-[801,902],c537-902,c538-[001,003,104],c540-[104,201-202,804,901-904]" MPISPAWN_GENERIC_NAME_90=SLURM_PROCID MPISPAWN_GENERIC_VALUE_90="0" MPISPAWN_GENERIC_NAME_91=I_MPI_DAPL_PROVIDER MPISPAWN_GENERIC_VALUE_91="ofa-v2-mlx4_0-1u" MPISPAWN_GENERIC_NAME_92=GLOBUS_LOCATION MPISPAWN_GENERIC_VALUE_92="/opt/apps/xsede/gsi-openssh-5.7" MPISPAWN_GENERIC_NAME_93=LMOD_COLORIZE MPISPAWN_GENERIC_VALUE_93="yes" MPISPAWN_GENERIC_NAME_94=I_MPI_HYDRA_BRANCH_COUNT MPISPAWN_GENERIC_VALUE_94="6000" MPISPAWN_GENERIC_NAME_95=SLURM_NNODES MPISPAWN_GENERIC_VALUE_95="128" MPISPAWN_GENERIC_NAME_96=MV2_DEFAULT_TIME_OUT MPISPAWN_GENERIC_VALUE_96="23" MPISPAWN_GENERIC_NAME_97=SLURM_TACC_NCORES_SET MPISPAWN_GENERIC_VALUE_97="1" MPISPAWN_GENERIC_NAME_98=__Init_Default_Modules MPISPAWN_GENERIC_VALUE_98="1" MPISPAWN_GENERIC_NAME_99=BASH_ENV MPISPAWN_GENERIC_VALUE_99="/etc/tacc/tacc_functions" MPISPAWN_GENERIC_NAME_100=SLURM_TOPOLOGY_ADDR_PATTERN MPISPAWN_GENERIC_VALUE_100="node" MPISPAWN_GENERIC_NAME_101=MV2_USE_RING_STARTUP MPISPAWN_GENERIC_VALUE_101="0" MPISPAWN_GENERIC_NAME_102=TACC_XALT_BIN MPISPAWN_GENERIC_VALUE_102="/opt/apps/xalt/0.4.4/bin" MPISPAWN_GENERIC_NAME_103=X509_USER_PROXY MPISPAWN_GENERIC_VALUE_103="/tmp/x509up_p3084.file9zM3fu.1" MPISPAWN_GENERIC_NAME_104=TMPDIR MPISPAWN_GENERIC_VALUE_104="/tmp" MPISPAWN_GENERIC_NAME_105=LMOD_FULL_SETTARG_SUPPORT MPISPAWN_GENERIC_VALUE_105="no" MPISPAWN_ID=22 MPISPAWN_WORKING_DIR=/home1/03222/tg825207/parallelnbody MPISPAWN_MPIRUN_RANK_0=352 MPISPAWN_MPIRUN_RANK_1=353 MPISPAWN_MPIRUN_RANK_2=354 MPISPAWN_MPIRUN_RANK_3=355 MPISPAWN_MPIRUN_RANK_4=356 MPISPAWN_MPIRUN_RANK_5=357 MPISPAWN_MPIRUN_RANK_6=358 MPISPAWN_MPIRUN_RANK_7=359 MPISPAWN_MPIRUN_RANK_8=360 MPISPAWN_MPIRUN_RANK_9=361 MPISPAWN_MPIRUN_RANK_10=362 MPISPAWN_MPIRUN_RANK_11=363 MPISPAWN_MPIRUN_RANK_12=364 MPISPAWN_MPIRUN_RANK_13=365 MPISPAWN_MPIRUN_RANK_14=366 MPISPAWN_MPIRUN_RANK_15=367 /opt/apps/gcc4_7/mvapich2/1.9/bin/mpispawn 0
bash: line 1: 73986 Segmentation fault      /usr/bin/env LD_LIBRARY_PATH=/opt/apps/gcc4_7/mvapich2/1.9/lib:/opt/apps/gcc4_7/mvapich2/1.9/lib/shared:/opt/apps/gcc/4.7.1/lib64:/opt/apps/gcc/4.7.1/lib:/opt/apps/xsede/gsi-openssh-5.7/lib64:/opt/apps/xsede/gsi-openssh-5.7/lib64 MPISPAWN_MPIRUN_MPD=0 USE_LINEAR_SSH=1 MPISPAWN_MPIRUN_HOST=c468-601.stampede.tacc.utexas.edu MPISPAWN_MPIRUN_HOSTIP=129.114.76.165 MPIRUN_RSH_LAUNCH=1 MPISPAWN_CHECKIN_PORT=56630 MPISPAWN_MPIRUN_PORT=56630 MPISPAWN_NNODES=128 MPISPAWN_GLOBAL_NPROCS=2048 MPISPAWN_MPIRUN_ID=47451 MPISPAWN_ARGC=4 MPDMAN_KVS_TEMPLATE=kvs_437_c468-601.stampede.tacc.utexas.edu_47451 MPISPAWN_LOCAL_NPROCS=16 MPISPAWN_ARGV_0='/home1/03222/tg825207/parallelnbody/teamscatter' MPISPAWN_ARGV_1='256000' MPISPAWN_ARGV_2='-c' MPISPAWN_ARGV_3='2' MPISPAWN_ARGC=4 MPISPAWN_GENERIC_ENV_COUNT=106 MPISPAWN_GENERIC_NAME_0=TACC_ENVLEN MPISPAWN_GENERIC_VALUE_0=6578 MPISPAWN_GENERIC_NAME_1=SLURM_JOB_ID MPISPAWN_GENERIC_VALUE_1="4306264" MPISPAWN_GENERIC_NAME_2=SLURMD_NODENAME MPISPAWN_GENERIC_VALUE_2="c468-601" MPISPAWN_GENERIC_NAME_3=STOCKYARD MPISPAWN_GENERIC_VALUE_3="/work/03222/tg825207" MPISPAWN_GENERIC_NAME_4=LC_ALL MPISPAWN_GENERIC_VALUE_4="en_US.UTF-8" MPISPAWN_GENERIC_NAME_5=SLURM_CHECKPOINT_IMAGE_DIR MPISPAWN_GENERIC_VALUE_5="/home1/03222/tg825207/parallelnbody" MPISPAWN_GENERIC_NAME_6=LMOD_FAMILY_MPI MPISPAWN_GENERIC_VALUE_6="mvapich2" MPISPAWN_GENERIC_NAME_7=SLURM_JOBID MPISPAWN_GENERIC_VALUE_7="4306264" MPISPAWN_GENERIC_NAME_8=_ModuleTable001_ MPISPAWN_GENERIC_VALUE_8="X01vZHVsZVRhYmxlXz17WyJhY3RpdmVTaXplIl09OCxiYXNlTXBhdGhBPXsiL29wdC9hcHBzL3hzZWRlL21vZHVsZWZpbGVzIiwiL29wdC9hcHBzL21vZHVsZWZpbGVzIiwiL29wdC9tb2R1bGVmaWxlcyIsfSxbImNfcmVidWlsZFRpbWUiXT1mYWxzZSxbImNfc2hvcnRUaW1lIl09ZmFsc2UsZmFtaWx5PXtbIk1QSSJdPSJtdmFwaWNoMiIsWyJjb21waWxlciJdPSJnY2MiLH0saW5hY3RpdmU9e30sbVQ9e0xpbnV4PXtbIkZOIl09Ii9vcHQvbW9kdWxlZmlsZXMvTGludXgiLFsiZGVmYXVsdCJdPTAsWyJmdWxsTmFtZSJdPSJMaW51eCIsWyJsb2FkT3JkZXIiXT0yLHByb3BUPXt9LFsic2hvcnQiXT0iTGludXgiLFsic3RhdHVzIl09ImFjdGl2ZSIsfSxUQUNDPXtbIkZOIl09Ii9v" MPISPAWN_GENERIC_NAME_9=TACC_SYSTEM MPISPAWN_GENERIC_VALUE_9="stampede" MPISPAWN_GENERIC_NAME_10=WORK MPISPAWN_GENERIC_VALUE_10="/work/03222/tg825207" MPISPAWN_GENERIC_NAME_11=SLURM_GTIDS MPISPAWN_GENERIC_VALUE_11="0" MPISPAWN_GENERIC_NAME_12=GCC_LIB MPISPAWN_GENERIC_VALUE_12="/opt/apps/gcc/4.7.1/lib64" MPISPAWN_GENERIC_NAME_13=SLURM_PRIO_PROCESS MPISPAWN_GENERIC_VALUE_13="0" MPISPAWN_GENERIC_NAME_14=SCRATCH MPISPAWN_GENERIC_VALUE_14="/scratch/03222/tg825207" MPISPAWN_GENERIC_NAME_15=LOGNAME MPISPAWN_GENERIC_VALUE_15="tg825207" MPISPAWN_GENERIC_NAME_16=_ModuleTable002_ MPISPAWN_GENERIC_VALUE_16="cHQvbW9kdWxlZmlsZXMvVEFDQyIsWyJkZWZhdWx0Il09MCxbImZ1bGxOYW1lIl09IlRBQ0MiLFsibG9hZE9yZGVyIl09Nixwcm9wVD17fSxbInNob3J0Il09IlRBQ0MiLFsic3RhdHVzIl09ImFjdGl2ZSIsfSxbIlRBQ0MtcGF0aHMiXT17WyJGTiJdPSIvb3B0L21vZHVsZWZpbGVzL1RBQ0MtcGF0aHMiLFsiZGVmYXVsdCJdPTAsWyJmdWxsTmFtZSJdPSJUQUNDLXBhdGhzIixbImxvYWRPcmRlciJdPTEscHJvcFQ9e30sWyJzaG9ydCJdPSJUQUNDLXBhdGhzIixbInN0YXR1cyJdPSJhY3RpdmUiLH0sY2x1c3Rlcj17WyJGTiJdPSIvb3B0L21vZHVsZWZpbGVzL2NsdXN0ZXIiLFsiZGVmYXVsdCJdPTAsWyJmdWxsTmFtZSJdPSJjbHVzdGVyIixbImxvYWRPcmRlciJdPTUscHJvcFQ9" MPISPAWN_GENERIC_NAME_17=INPUTRC MPISPAWN_GENERIC_VALUE_17="/etc/inputrc" MPISPAWN_GENERIC_NAME_18=LMOD_CMD MPISPAWN_GENERIC_VALUE_18="/opt/apps/lmod/lmod/libexec/lmod" MPISPAWN_GENERIC_NAME_19=I_MPI_OFA_ADAPTER_NAME MPISPAWN_GENERIC_VALUE_19="mlx4_0" MPISPAWN_GENERIC_NAME_20=QTLIB MPISPAWN_GENERIC_VALUE_20="/usr/lib64/qt-3.3/lib" MPISPAWN_GENERIC_NAME_21=NODE_TASKS_PPN_INFO MPISPAWN_GENERIC_VALUE_21=""16,0_"" MPISPAWN_GENERIC_NAME_22=SLURM_CPUS_ON_NODE MPISPAWN_GENERIC_VALUE_22="16" MPISPAWN_GENERIC_NAME_23=PATH MPISPAWN_GENERIC_VALUE_23="/opt/apps/gcc4_7/mvapich2/1.9/bin:/opt/apps/gcc/4.7.1/bin:/opt/apps/xalt/0.4.4/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/opt/apps/xsede/gsi-openssh-5.7/bin:/usr/X11R6/bin:/opt/ofed/bin:/opt/ofed/sbin:." MPISPAWN_GENERIC_NAME_24=SLURM_TACC_JOBNAME MPISPAWN_GENERIC_VALUE_24="stampedeLargeScaleAllTS.sh" MPISPAWN_GENERIC_NAME_25=SLURM_NODEID MPISPAWN_GENERIC_VALUE_25="0" MPISPAWN_GENERIC_NAME_26=LMOD_PKG MPISPAWN_GENERIC_VALUE_26="/opt/apps/lmod/lmod" MPISPAWN_GENERIC_NAME_27=MIC_I_MPI_OFA_ADAPTER_NAME MPISPAWN_GENERIC_VALUE_27="mlx4_0" MPISPAWN_GENERIC_NAME_28=HISTSIZE MPISPAWN_GENERIC_VALUE_28="1000" MPISPAWN_GENERIC_NAME_29=SLURM_NTASKS MPISPAWN_GENERIC_VALUE_29="2048" MPISPAWN_GENERIC_NAME_30=SHLIB_PATH MPISPAWN_GENERIC_VALUE_30="/opt/apps/xsede/gsi-openssh-5.7/lib64" MPISPAWN_GENERIC_NAME_31=TACC_OVERRIDE_PROJECT MPISPAWN_GENERIC_VALUE_31="TG-ASC140020" MPISPAWN_GENERIC_NAME_32=_ModuleTable005_ MPISPAWN_GENERIC_VALUE_32="Z2NjNF83L21vZHVsZWZpbGVzIiwiL29wdC9hcHBzL3hzZWRlL21vZHVsZWZpbGVzIiwiL29wdC9hcHBzL21vZHVsZWZpbGVzIiwiL29wdC9tb2R1bGVmaWxlcyIsfSxbInN5c3RlbUJhc2VNUEFUSCJdPSIvb3B0L2FwcHMveHNlZGUvbW9kdWxlZmlsZXM6L29wdC9hcHBzL21vZHVsZWZpbGVzOi9vcHQvbW9kdWxlZmlsZXMiLFsidmVyc2lvbiJdPTIsfQ==" MPISPAWN_GENERIC_NAME_33=_ModuleTable_Sz_ MPISPAWN_GENERIC_VALUE_33="5" MPISPAWN_GENERIC_NAME_34=MAIL MPISPAWN_GENERIC_VALUE_34="/var/spool/mail/tg825207" MPISPAWN_GENERIC_NAME_35=MV2_USE_HUGEPAGES MPISPAWN_GENERIC_VALUE_35="0" MPISPAWN_GENERIC_NAME_36=SLURM_TOPOLOGY_ADDR MPISPAWN_GENERIC_VALUE_36="c468-601" MPISPAWN_GENERIC_NAME_37=TACC_XALT_DIR MPISPAWN_GENERIC_VALUE_37="/opt/apps/xalt/0.4.4/" MPISPAWN_GENERIC_NAME_38=QTDIR MPISPAWN_GENERIC_VALUE_38="/usr/lib64/qt-3.3" MPISPAWN_GENERIC_NAME_39=SYSTEM MPISPAWN_GENERIC_VALUE_39="linux" MPISPAWN_GENERIC_NAME_40=SLURM_TACC_RUNLIMIT_MINS MPISPAWN_GENERIC_VALUE_40="180" MPISPAWN_GENERIC_NAME_41=SLURM_SUBMIT_DIR MPISPAWN_GENERIC_VALUE_41="/home1/03222/tg825207/parallelnbody" MPISPAWN_GENERIC_NAME_42=LIBPATH MPISPAWN_GENERIC_VALUE_42="/opt/apps/xsede/gsi-openssh-5.7/lib64:/usr/lib:/lib" MPISPAWN_GENERIC_NAME_43=HOSTNAME MPISPAWN_GENERIC_VALUE_43="c468-601" MPISPAWN_GENERIC_NAME_44=LMOD_DEFAULT_MODULEPATH MPISPAWN_GENERIC_VALUE_44="/opt/apps/xsede/modulefiles:/opt/apps/modulefiles:/opt/modulefiles" MPISPAWN_GENERIC_NAME_45=ARCHIVE MPISPAWN_GENERIC_VALUE_45="/home/03222/tg825207" MPISPAWN_GENERIC_NAME_46=_ MPISPAWN_GENERIC_VALUE_46="/usr/local/bin/build_env.pl" MPISPAWN_GENERIC_NAME_47=__BASHRC_SOURCED__ MPISPAWN_GENERIC_VALUE_47="1" MPISPAWN_GENERIC_NAME_48=APPS MPISPAWN_GENERIC_VALUE_48="/opt/apps" MPISPAWN_GENERIC_NAME_49=SHELL MPISPAWN_GENERIC_VALUE_49="/bin/bash" MPISPAWN_GENERIC_NAME_50=ARMCI_OPENIB_DEVICE MPISPAWN_GENERIC_VALUE_50="mlx4_0" MPISPAWN_GENERIC_NAME_51=ENVIRONMENT MPISPAWN_GENERIC_VALUE_51="BATCH" MPISPAWN_GENERIC_NAME_52=TACC_FAMILY_MPI MPISPAWN_GENERIC_VALUE_52="mvapich2" MPISPAWN_GENERIC_NAME_53=MV2_IBA_HCA MPISPAWN_GENERIC_VALUE_53="mlx4_0" MPISPAWN_GENERIC_NAME_54=TACC_PUBLIC_MACHINE MPISPAWN_GENERIC_VALUE_54="1" MPISPAWN_GENERIC_NAME_55=SLURM_TACC_NODES MPISPAWN_GENERIC_VALUE_55="1" MPISPAWN_GENERIC_NAME_56=_ModuleTable003_ MPISPAWN_GENERIC_VALUE_56="e30sWyJzaG9ydCJdPSJjbHVzdGVyIixbInN0YXR1cyJdPSJhY3RpdmUiLH0sWyJjbHVzdGVyLXBhdGhzIl09e1siRk4iXT0iL29wdC9tb2R1bGVmaWxlcy9jbHVzdGVyLXBhdGhzIixbImRlZmF1bHQiXT0wLFsiZnVsbE5hbWUiXT0iY2x1c3Rlci1wYXRocyIsWyJsb2FkT3JkZXIiXT0zLHByb3BUPXt9LFsic2hvcnQiXT0iY2x1c3Rlci1wYXRocyIsWyJzdGF0dXMiXT0iYWN0aXZlIix9LGdjYz17WyJGTiJdPSIvb3B0L2FwcHMvbW9kdWxlZmlsZXMvZ2NjLzQuNy4xLmx1YSIsWyJkZWZhdWx0Il09MCxbImZ1bGxOYW1lIl09ImdjYy80LjcuMSIsWyJsb2FkT3JkZXIiXT03LHByb3BUPXt9LFsic2hvcnQiXT0iZ2NjIixbInN0YXR1cyJdPSJhY3RpdmUiLH0sbXZhcGljaDI9e1si" MPISPAWN_GENERIC_NAME_57=QTINC MPISPAWN_GENERIC_VALUE_57="/usr/lib64/qt-3.3/include" MPISPAWN_GENERIC_NAME_58=CVS_RSH MPISPAWN_GENERIC_VALUE_58="ssh" MPISPAWN_GENERIC_NAME_59=MV2_USE_OLD_BCAST MPISPAWN_GENERIC_VALUE_59="1" MPISPAWN_GENERIC_NAME_60=SLURM_JOB_NUM_NODES MPISPAWN_GENERIC_VALUE_60="128" MPISPAWN_GENERIC_NAME_61=SLURM_TASK_PID MPISPAWN_GENERIC_VALUE_61="31479" MPISPAWN_GENERIC_NAME_62=TACC_MPI_GETMODE MPISPAWN_GENERIC_VALUE_62="mvapich2_ssh" MPISPAWN_GENERIC_NAME_63=LANG MPISPAWN_GENERIC_VALUE_63="en_US.UTF-8" MPISPAWN_GENERIC_NAME_64=USER MPISPAWN_GENERIC_VALUE_64="tg825207" MPISPAWN_GENERIC_NAME_65=MIC_I_MPI_DAPL_PROVIDER MPISPAWN_GENERIC_VALUE_65="ofa-v2-mlx4_0-1u" MPISPAWN_GENERIC_NAME_66=G_BROKEN_FILENAMES MPISPAWN_GENERIC_VALUE_66="1" MPISPAWN_GENERIC_NAME_67=SLURM_LOCALID MPISPAWN_GENERIC_VALUE_67="0" MPISPAWN_GENERIC_NAME_68=SLURM_NPROCS MPISPAWN_GENERIC_VALUE_68="2048" MPISPAWN_GENERIC_NAME_69=SLURM_TACC_NNODES_SET MPISPAWN_GENERIC_VALUE_69="0" MPISPAWN_GENERIC_NAME_70=SLURM_TACC_CORES MPISPAWN_GENERIC_VALUE_70="2048" MPISPAWN_GENERIC_NAME_71=LMOD_DIR MPISPAWN_GENERIC_VALUE_71="/opt/apps/lmod/lmod/libexec" MPISPAWN_GENERIC_NAME_72=LMOD_SETTARG_CMD MPISPAWN_GENERIC_VALUE_72=":" MPISPAWN_GENERIC_NAME_73=OMP_NUM_THREADS MPISPAWN_GENERIC_VALUE_73="1" MPISPAWN_GENERIC_NAME_74=_ModuleTable004_ MPISPAWN_GENERIC_VALUE_74="Rk4iXT0iL29wdC9hcHBzL2djYzRfNy9tb2R1bGVmaWxlcy9tdmFwaWNoMi8xLjlhMiIsWyJkZWZhdWx0Il09MSxbImZ1bGxOYW1lIl09Im12YXBpY2gyLzEuOWEyIixbImxvYWRPcmRlciJdPTgscHJvcFQ9e30sWyJzaG9ydCJdPSJtdmFwaWNoMiIsWyJzdGF0dXMiXT0iYWN0aXZlIix9LHhhbHQ9e1siRk4iXT0iL29wdC9hcHBzL21vZHVsZWZpbGVzL3hhbHQvMC40LjQubHVhIixbImRlZmF1bHQiXT0xLFsiZnVsbE5hbWUiXT0ieGFsdC8wLjQuNCIsWyJsb2FkT3JkZXIiXT00LHByb3BUPXt9LFsic2hvcnQiXT0ieGFsdCIsWyJzdGF0dXMiXT0iYWN0aXZlIix9LH0sbXBhdGhBPXsiL29wdC9hcHBzL2djYzRfNy9tdmFwaWNoMl8xXzkvbW9kdWxlZmlsZXMiLCIvb3B0L2FwcHMv" MPISPAWN_GENERIC_NAME_75=__PERSONAL_PATH__ MPISPAWN_GENERIC_VALUE_75="1" MPISPAWN_GENERIC_NAME_76=LMOD_SYSTEM_DEFAULT_MODULES MPISPAWN_GENERIC_VALUE_76="TACC" MPISPAWN_GENERIC_NAME_77=SLURM_JOB_NAME MPISPAWN_GENERIC_VALUE_77="stampedeLargeScaleAllTS.sh" MPISPAWN_GENERIC_NAME_78=SLURM_SUBMIT_HOST MPISPAWN_GENERIC_VALUE_78="login4.stampede.tacc.utexas.edu" MPISPAWN_GENERIC_NAME_79=LD_LIBRARY_PATH MPISPAWN_GENERIC_VALUE_79="/opt/apps/gcc4_7/mvapich2/1.9/lib:/opt/apps/gcc4_7/mvapich2/1.9/lib/shared:/opt/apps/gcc/4.7.1/lib64:/opt/apps/gcc/4.7.1/lib:/opt/apps/xsede/gsi-openssh-5.7/lib64:/opt/apps/xsede/gsi-openssh-5.7/lib64" MPISPAWN_GENERIC_NAME_80=SLURM_QUEUE MPISPAWN_GENERIC_VALUE_80="normal" MPISPAWN_GENERIC_NAME_81=PKG_CONFIG_PATH MPISPAWN_GENERIC_VALUE_81="/opt/apps/gcc4_7/mvapich2/1.9/lib/pkgconfig" MPISPAWN_GENERIC_NAME_82=LMOD_PREPEND_BLOCK MPISPAWN_GENERIC_VALUE_82="normal" MPISPAWN_GENERIC_NAME_83=TACC_FAMILY_COMPILER MPISPAWN_GENERIC_VALUE_83="gcc" MPISPAWN_GENERIC_NAME_84=TACC_DOMAIN MPISPAWN_GENERIC_VALUE_84="stampede" MPISPAWN_GENERIC_NAME_85=MV2_HOMOGENEOUS_CLUSTER MPISPAWN_GENERIC_VALUE_85="1" MPISPAWN_GENERIC_NAME_86=LMOD_FAMILY_COMPILER MPISPAWN_GENERIC_VALUE_86="gcc" MPISPAWN_GENERIC_NAME_87=MV2_USE_UD_HYBRID MPISPAWN_GENERIC_VALUE_87="0" MPISPAWN_GENERIC_NAME_88=MPICH_HOME MPISPAWN_GENERIC_VALUE_88="/opt/apps/gcc4_7/mvapich2/1.9" MPISPAWN_GENERIC_NAME_89=SLURM_JOB_NODELIST MPISPAWN_GENERIC_VALUE_89="c468-601,c471-503,c472-[401-402],c473-[303-304,401,804,901],c474-[203-204,301,401,403-404],c478-303,c482-404,c483-[001-004,101,504,601,804,901],c484-[101,103-104,201,602,701],c485-502,c486-102,c490-[302,904],c491-004,c492-701,c493-[001,102,201],c494-[101,402-403,602,702-703,904],c495-[001-002,201],c497-[102,202,504,601-603],c498-[304,402,603],c501-[204,301,604,801],c502-104,c503-901,c504-[602,604,702,801],c505-[001,202-204,401,502-504,802,901-904],c506-[302,704],c507-[102,202,503,601,904],c508-[101,204,601,702-703,802,903-904],c509-[002,201,301,303,401,501-502],c510-[003,401,404,501,503],c511-[601-602,902-903],c536-[801,902],c537-902,c538-[001,003,104],c540-[104,201-202,804,901-904]" MPISPAWN_GENERIC_NAME_90=SLURM_PROCID MPISPAWN_GENERIC_VALUE_90="0" MPISPAWN_GENERIC_NAME_91=I_MPI_DAPL_PROVIDER MPISPAWN_GENERIC_VALUE_91="ofa-v2-mlx4_0-1u" MPISPAWN_GENERIC_NAME_92=GLOBUS_LOCATION MPISPAWN_GENERIC_VALUE_92="/opt/apps/xsede/gsi-openssh-5.7" MPISPAWN_GENERIC_NAME_93=LMOD_COLORIZE MPISPAWN_GENERIC_VALUE_93="yes" MPISPAWN_GENERIC_NAME_94=I_MPI_HYDRA_BRANCH_COUNT MPISPAWN_GENERIC_VALUE_94="6000" MPISPAWN_GENERIC_NAME_95=SLURM_NNODES MPISPAWN_GENERIC_VALUE_95="128" MPISPAWN_GENERIC_NAME_96=MV2_DEFAULT_TIME_OUT MPISPAWN_GENERIC_VALUE_96="23" MPISPAWN_GENERIC_NAME_97=SLURM_TACC_NCORES_SET MPISPAWN_GENERIC_VALUE_97="1" MPISPAWN_GENERIC_NAME_98=__Init_Default_Modules MPISPAWN_GENERIC_VALUE_98="1" MPISPAWN_GENERIC_NAME_99=BASH_ENV MPISPAWN_GENERIC_VALUE_99="/etc/tacc/tacc_functions" MPISPAWN_GENERIC_NAME_100=SLURM_TOPOLOGY_ADDR_PATTERN MPISPAWN_GENERIC_VALUE_100="node" MPISPAWN_GENERIC_NAME_101=MV2_USE_RING_STARTUP MPISPAWN_GENERIC_VALUE_101="0" MPISPAWN_GENERIC_NAME_102=TACC_XALT_BIN MPISPAWN_GENERIC_VALUE_102="/opt/apps/xalt/0.4.4/bin" MPISPAWN_GENERIC_NAME_103=X509_USER_PROXY MPISPAWN_GENERIC_VALUE_103="/tmp/x509up_p3084.file9zM3fu.1" MPISPAWN_GENERIC_NAME_104=TMPDIR MPISPAWN_GENERIC_VALUE_104="/tmp" MPISPAWN_GENERIC_NAME_105=LMOD_FULL_SETTARG_SUPPORT MPISPAWN_GENERIC_VALUE_105="no" MPISPAWN_ID=39 MPISPAWN_WORKING_DIR=/home1/03222/tg825207/parallelnbody MPISPAWN_MPIRUN_RANK_0=624 MPISPAWN_MPIRUN_RANK_1=625 MPISPAWN_MPIRUN_RANK_2=626 MPISPAWN_MPIRUN_RANK_3=627 MPISPAWN_MPIRUN_RANK_4=628 MPISPAWN_MPIRUN_RANK_5=629 MPISPAWN_MPIRUN_RANK_6=630 MPISPAWN_MPIRUN_RANK_7=631 MPISPAWN_MPIRUN_RANK_8=632 MPISPAWN_MPIRUN_RANK_9=633 MPISPAWN_MPIRUN_RANK_10=634 MPISPAWN_MPIRUN_RANK_11=635 MPISPAWN_MPIRUN_RANK_12=636 MPISPAWN_MPIRUN_RANK_13=637 MPISPAWN_MPIRUN_RANK_14=638 MPISPAWN_MPIRUN_RANK_15=639 /opt/apps/gcc4_7/mvapich2/1.9/bin/mpispawn 0
[c498-304.stampede.tacc.utexas.edu:mpispawn_57][error_sighandler] Caught error: Segmentation fault (signal 11)
[c497-102.stampede.tacc.utexas.edu:mpispawn_51][error_sighandler] Caught error: Segmentation fault (signal 11)
bash: line 1: 18556 Segmentation fault      /usr/bin/env LD_LIBRARY_PATH=/opt/apps/gcc4_7/mvapich2/1.9/lib:/opt/apps/gcc4_7/mvapich2/1.9/lib/shared:/opt/apps/gcc/4.7.1/lib64:/opt/apps/gcc/4.7.1/lib:/opt/apps/xsede/gsi-openssh-5.7/lib64:/opt/apps/xsede/gsi-openssh-5.7/lib64 MPISPAWN_MPIRUN_MPD=0 USE_LINEAR_SSH=1 MPISPAWN_MPIRUN_HOST=c468-601.stampede.tacc.utexas.edu MPISPAWN_MPIRUN_HOSTIP=129.114.76.165 MPIRUN_RSH_LAUNCH=1 MPISPAWN_CHECKIN_PORT=36101 MPISPAWN_MPIRUN_PORT=36101 MPISPAWN_NNODES=64 MPISPAWN_GLOBAL_NPROCS=1024 MPISPAWN_MPIRUN_ID=44295 MPISPAWN_ARGC=4 MPDMAN_KVS_TEMPLATE=kvs_513_c468-601.stampede.tacc.utexas.edu_44295 MPISPAWN_LOCAL_NPROCS=16 MPISPAWN_ARGV_0='/home1/03222/tg825207/parallelnbody/teamscatter' MPISPAWN_ARGV_1='256000' MPISPAWN_ARGV_2='-c' MPISPAWN_ARGV_3='2' MPISPAWN_ARGC=4 MPISPAWN_GENERIC_ENV_COUNT=106 MPISPAWN_GENERIC_NAME_0=TACC_ENVLEN MPISPAWN_GENERIC_VALUE_0=6578 MPISPAWN_GENERIC_NAME_1=SLURM_JOB_ID MPISPAWN_GENERIC_VALUE_1="4306264" MPISPAWN_GENERIC_NAME_2=SLURMD_NODENAME MPISPAWN_GENERIC_VALUE_2="c468-601" MPISPAWN_GENERIC_NAME_3=STOCKYARD MPISPAWN_GENERIC_VALUE_3="/work/03222/tg825207" MPISPAWN_GENERIC_NAME_4=LC_ALL MPISPAWN_GENERIC_VALUE_4="en_US.UTF-8" MPISPAWN_GENERIC_NAME_5=SLURM_CHECKPOINT_IMAGE_DIR MPISPAWN_GENERIC_VALUE_5="/home1/03222/tg825207/parallelnbody" MPISPAWN_GENERIC_NAME_6=LMOD_FAMILY_MPI MPISPAWN_GENERIC_VALUE_6="mvapich2" MPISPAWN_GENERIC_NAME_7=SLURM_JOBID MPISPAWN_GENERIC_VALUE_7="4306264" MPISPAWN_GENERIC_NAME_8=_ModuleTable001_ MPISPAWN_GENERIC_VALUE_8="X01vZHVsZVRhYmxlXz17WyJhY3RpdmVTaXplIl09OCxiYXNlTXBhdGhBPXsiL29wdC9hcHBzL3hzZWRlL21vZHVsZWZpbGVzIiwiL29wdC9hcHBzL21vZHVsZWZpbGVzIiwiL29wdC9tb2R1bGVmaWxlcyIsfSxbImNfcmVidWlsZFRpbWUiXT1mYWxzZSxbImNfc2hvcnRUaW1lIl09ZmFsc2UsZmFtaWx5PXtbIk1QSSJdPSJtdmFwaWNoMiIsWyJjb21waWxlciJdPSJnY2MiLH0saW5hY3RpdmU9e30sbVQ9e0xpbnV4PXtbIkZOIl09Ii9vcHQvbW9kdWxlZmlsZXMvTGludXgiLFsiZGVmYXVsdCJdPTAsWyJmdWxsTmFtZSJdPSJMaW51eCIsWyJsb2FkT3JkZXIiXT0yLHByb3BUPXt9LFsic2hvcnQiXT0iTGludXgiLFsic3RhdHVzIl09ImFjdGl2ZSIsfSxUQUNDPXtbIkZOIl09Ii9v" MPISPAWN_GENERIC_NAME_9=TACC_SYSTEM MPISPAWN_GENERIC_VALUE_9="stampede" MPISPAWN_GENERIC_NAME_10=WORK MPISPAWN_GENERIC_VALUE_10="/work/03222/tg825207" MPISPAWN_GENERIC_NAME_11=SLURM_GTIDS MPISPAWN_GENERIC_VALUE_11="0" MPISPAWN_GENERIC_NAME_12=GCC_LIB MPISPAWN_GENERIC_VALUE_12="/opt/apps/gcc/4.7.1/lib64" MPISPAWN_GENERIC_NAME_13=SLURM_PRIO_PROCESS MPISPAWN_GENERIC_VALUE_13="0" MPISPAWN_GENERIC_NAME_14=SCRATCH MPISPAWN_GENERIC_VALUE_14="/scratch/03222/tg825207" MPISPAWN_GENERIC_NAME_15=LOGNAME MPISPAWN_GENERIC_VALUE_15="tg825207" MPISPAWN_GENERIC_NAME_16=_ModuleTable002_ MPISPAWN_GENERIC_VALUE_16="cHQvbW9kdWxlZmlsZXMvVEFDQyIsWyJkZWZhdWx0Il09MCxbImZ1bGxOYW1lIl09IlRBQ0MiLFsibG9hZE9yZGVyIl09Nixwcm9wVD17fSxbInNob3J0Il09IlRBQ0MiLFsic3RhdHVzIl09ImFjdGl2ZSIsfSxbIlRBQ0MtcGF0aHMiXT17WyJGTiJdPSIvb3B0L21vZHVsZWZpbGVzL1RBQ0MtcGF0aHMiLFsiZGVmYXVsdCJdPTAsWyJmdWxsTmFtZSJdPSJUQUNDLXBhdGhzIixbImxvYWRPcmRlciJdPTEscHJvcFQ9e30sWyJzaG9ydCJdPSJUQUNDLXBhdGhzIixbInN0YXR1cyJdPSJhY3RpdmUiLH0sY2x1c3Rlcj17WyJGTiJdPSIvb3B0L21vZHVsZWZpbGVzL2NsdXN0ZXIiLFsiZGVmYXVsdCJdPTAsWyJmdWxsTmFtZSJdPSJjbHVzdGVyIixbImxvYWRPcmRlciJdPTUscHJvcFQ9" MPISPAWN_GENERIC_NAME_17=INPUTRC MPISPAWN_GENERIC_VALUE_17="/etc/inputrc" MPISPAWN_GENERIC_NAME_18=LMOD_CMD MPISPAWN_GENERIC_VALUE_18="/opt/apps/lmod/lmod/libexec/lmod" MPISPAWN_GENERIC_NAME_19=I_MPI_OFA_ADAPTER_NAME MPISPAWN_GENERIC_VALUE_19="mlx4_0" MPISPAWN_GENERIC_NAME_20=QTLIB MPISPAWN_GENERIC_VALUE_20="/usr/lib64/qt-3.3/lib" MPISPAWN_GENERIC_NAME_21=NODE_TASKS_PPN_INFO MPISPAWN_GENERIC_VALUE_21=""16,0_"" MPISPAWN_GENERIC_NAME_22=SLURM_CPUS_ON_NODE MPISPAWN_GENERIC_VALUE_22="16" MPISPAWN_GENERIC_NAME_23=PATH MPISPAWN_GENERIC_VALUE_23="/opt/apps/gcc4_7/mvapich2/1.9/bin:/opt/apps/gcc/4.7.1/bin:/opt/apps/xalt/0.4.4/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/opt/apps/xsede/gsi-openssh-5.7/bin:/usr/X11R6/bin:/opt/ofed/bin:/opt/ofed/sbin:." MPISPAWN_GENERIC_NAME_24=SLURM_TACC_JOBNAME MPISPAWN_GENERIC_VALUE_24="stampedeLargeScaleAllTS.sh" MPISPAWN_GENERIC_NAME_25=SLURM_NODEID MPISPAWN_GENERIC_VALUE_25="0" MPISPAWN_GENERIC_NAME_26=LMOD_PKG MPISPAWN_GENERIC_VALUE_26="/opt/apps/lmod/lmod" MPISPAWN_GENERIC_NAME_27=MIC_I_MPI_OFA_ADAPTER_NAME MPISPAWN_GENERIC_VALUE_27="mlx4_0" MPISPAWN_GENERIC_NAME_28=HISTSIZE MPISPAWN_GENERIC_VALUE_28="1000" MPISPAWN_GENERIC_NAME_29=SLURM_NTASKS MPISPAWN_GENERIC_VALUE_29="2048" MPISPAWN_GENERIC_NAME_30=SHLIB_PATH MPISPAWN_GENERIC_VALUE_30="/opt/apps/xsede/gsi-openssh-5.7/lib64" MPISPAWN_GENERIC_NAME_31=TACC_OVERRIDE_PROJECT MPISPAWN_GENERIC_VALUE_31="TG-ASC140020" MPISPAWN_GENERIC_NAME_32=_ModuleTable005_ MPISPAWN_GENERIC_VALUE_32="Z2NjNF83L21vZHVsZWZpbGVzIiwiL29wdC9hcHBzL3hzZWRlL21vZHVsZWZpbGVzIiwiL29wdC9hcHBzL21vZHVsZWZpbGVzIiwiL29wdC9tb2R1bGVmaWxlcyIsfSxbInN5c3RlbUJhc2VNUEFUSCJdPSIvb3B0L2FwcHMveHNlZGUvbW9kdWxlZmlsZXM6L29wdC9hcHBzL21vZHVsZWZpbGVzOi9vcHQvbW9kdWxlZmlsZXMiLFsidmVyc2lvbiJdPTIsfQ==" MPISPAWN_GENERIC_NAME_33=_ModuleTable_Sz_ MPISPAWN_GENERIC_VALUE_33="5" MPISPAWN_GENERIC_NAME_34=MAIL MPISPAWN_GENERIC_VALUE_34="/var/spool/mail/tg825207" MPISPAWN_GENERIC_NAME_35=MV2_USE_HUGEPAGES MPISPAWN_GENERIC_VALUE_35="0" MPISPAWN_GENERIC_NAME_36=SLURM_TOPOLOGY_ADDR MPISPAWN_GENERIC_VALUE_36="c468-601" MPISPAWN_GENERIC_NAME_37=TACC_XALT_DIR MPISPAWN_GENERIC_VALUE_37="/opt/apps/xalt/0.4.4/" MPISPAWN_GENERIC_NAME_38=QTDIR MPISPAWN_GENERIC_VALUE_38="/usr/lib64/qt-3.3" MPISPAWN_GENERIC_NAME_39=SYSTEM MPISPAWN_GENERIC_VALUE_39="linux" MPISPAWN_GENERIC_NAME_40=SLURM_TACC_RUNLIMIT_MINS MPISPAWN_GENERIC_VALUE_40="180" MPISPAWN_GENERIC_NAME_41=SLURM_SUBMIT_DIR MPISPAWN_GENERIC_VALUE_41="/home1/03222/tg825207/parallelnbody" MPISPAWN_GENERIC_NAME_42=LIBPATH MPISPAWN_GENERIC_VALUE_42="/opt/apps/xsede/gsi-openssh-5.7/lib64:/usr/lib:/lib" MPISPAWN_GENERIC_NAME_43=HOSTNAME MPISPAWN_GENERIC_VALUE_43="c468-601" MPISPAWN_GENERIC_NAME_44=LMOD_DEFAULT_MODULEPATH MPISPAWN_GENERIC_VALUE_44="/opt/apps/xsede/modulefiles:/opt/apps/modulefiles:/opt/modulefiles" MPISPAWN_GENERIC_NAME_45=ARCHIVE MPISPAWN_GENERIC_VALUE_45="/home/03222/tg825207" MPISPAWN_GENERIC_NAME_46=_ MPISPAWN_GENERIC_VALUE_46="/usr/local/bin/build_env.pl" MPISPAWN_GENERIC_NAME_47=__BASHRC_SOURCED__ MPISPAWN_GENERIC_VALUE_47="1" MPISPAWN_GENERIC_NAME_48=APPS MPISPAWN_GENERIC_VALUE_48="/opt/apps" MPISPAWN_GENERIC_NAME_49=SHELL MPISPAWN_GENERIC_VALUE_49="/bin/bash" MPISPAWN_GENERIC_NAME_50=ARMCI_OPENIB_DEVICE MPISPAWN_GENERIC_VALUE_50="mlx4_0" MPISPAWN_GENERIC_NAME_51=ENVIRONMENT MPISPAWN_GENERIC_VALUE_51="BATCH" MPISPAWN_GENERIC_NAME_52=TACC_FAMILY_MPI MPISPAWN_GENERIC_VALUE_52="mvapich2" MPISPAWN_GENERIC_NAME_53=MV2_IBA_HCA MPISPAWN_GENERIC_VALUE_53="mlx4_0" MPISPAWN_GENERIC_NAME_54=TACC_PUBLIC_MACHINE MPISPAWN_GENERIC_VALUE_54="1" MPISPAWN_GENERIC_NAME_55=SLURM_TACC_NODES MPISPAWN_GENERIC_VALUE_55="1" MPISPAWN_GENERIC_NAME_56=_ModuleTable003_ MPISPAWN_GENERIC_VALUE_56="e30sWyJzaG9ydCJdPSJjbHVzdGVyIixbInN0YXR1cyJdPSJhY3RpdmUiLH0sWyJjbHVzdGVyLXBhdGhzIl09e1siRk4iXT0iL29wdC9tb2R1bGVmaWxlcy9jbHVzdGVyLXBhdGhzIixbImRlZmF1bHQiXT0wLFsiZnVsbE5hbWUiXT0iY2x1c3Rlci1wYXRocyIsWyJsb2FkT3JkZXIiXT0zLHByb3BUPXt9LFsic2hvcnQiXT0iY2x1c3Rlci1wYXRocyIsWyJzdGF0dXMiXT0iYWN0aXZlIix9LGdjYz17WyJGTiJdPSIvb3B0L2FwcHMvbW9kdWxlZmlsZXMvZ2NjLzQuNy4xLmx1YSIsWyJkZWZhdWx0Il09MCxbImZ1bGxOYW1lIl09ImdjYy80LjcuMSIsWyJsb2FkT3JkZXIiXT03LHByb3BUPXt9LFsic2hvcnQiXT0iZ2NjIixbInN0YXR1cyJdPSJhY3RpdmUiLH0sbXZhcGljaDI9e1si" MPISPAWN_GENERIC_NAME_57=QTINC MPISPAWN_GENERIC_VALUE_57="/usr/lib64/qt-3.3/include" MPISPAWN_GENERIC_NAME_58=CVS_RSH MPISPAWN_GENERIC_VALUE_58="ssh" MPISPAWN_GENERIC_NAME_59=MV2_USE_OLD_BCAST MPISPAWN_GENERIC_VALUE_59="1" MPISPAWN_GENERIC_NAME_60=SLURM_JOB_NUM_NODES MPISPAWN_GENERIC_VALUE_60="128" MPISPAWN_GENERIC_NAME_61=SLURM_TASK_PID MPISPAWN_GENERIC_VALUE_61="31479" MPISPAWN_GENERIC_NAME_62=TACC_MPI_GETMODE MPISPAWN_GENERIC_VALUE_62="mvapich2_ssh" MPISPAWN_GENERIC_NAME_63=LANG MPISPAWN_GENERIC_VALUE_63="en_US.UTF-8" MPISPAWN_GENERIC_NAME_64=USER MPISPAWN_GENERIC_VALUE_64="tg825207" MPISPAWN_GENERIC_NAME_65=MIC_I_MPI_DAPL_PROVIDER MPISPAWN_GENERIC_VALUE_65="ofa-v2-mlx4_0-1u" MPISPAWN_GENERIC_NAME_66=G_BROKEN_FILENAMES MPISPAWN_GENERIC_VALUE_66="1" MPISPAWN_GENERIC_NAME_67=SLURM_LOCALID MPISPAWN_GENERIC_VALUE_67="0" MPISPAWN_GENERIC_NAME_68=SLURM_NPROCS MPISPAWN_GENERIC_VALUE_68="2048" MPISPAWN_GENERIC_NAME_69=SLURM_TACC_NNODES_SET MPISPAWN_GENERIC_VALUE_69="0" MPISPAWN_GENERIC_NAME_70=SLURM_TACC_CORES MPISPAWN_GENERIC_VALUE_70="2048" MPISPAWN_GENERIC_NAME_71=LMOD_DIR MPISPAWN_GENERIC_VALUE_71="/opt/apps/lmod/lmod/libexec" MPISPAWN_GENERIC_NAME_72=LMOD_SETTARG_CMD MPISPAWN_GENERIC_VALUE_72=":" MPISPAWN_GENERIC_NAME_73=OMP_NUM_THREADS MPISPAWN_GENERIC_VALUE_73="1" MPISPAWN_GENERIC_NAME_74=_ModuleTable004_ MPISPAWN_GENERIC_VALUE_74="Rk4iXT0iL29wdC9hcHBzL2djYzRfNy9tb2R1bGVmaWxlcy9tdmFwaWNoMi8xLjlhMiIsWyJkZWZhdWx0Il09MSxbImZ1bGxOYW1lIl09Im12YXBpY2gyLzEuOWEyIixbImxvYWRPcmRlciJdPTgscHJvcFQ9e30sWyJzaG9ydCJdPSJtdmFwaWNoMiIsWyJzdGF0dXMiXT0iYWN0aXZlIix9LHhhbHQ9e1siRk4iXT0iL29wdC9hcHBzL21vZHVsZWZpbGVzL3hhbHQvMC40LjQubHVhIixbImRlZmF1bHQiXT0xLFsiZnVsbE5hbWUiXT0ieGFsdC8wLjQuNCIsWyJsb2FkT3JkZXIiXT00LHByb3BUPXt9LFsic2hvcnQiXT0ieGFsdCIsWyJzdGF0dXMiXT0iYWN0aXZlIix9LH0sbXBhdGhBPXsiL29wdC9hcHBzL2djYzRfNy9tdmFwaWNoMl8xXzkvbW9kdWxlZmlsZXMiLCIvb3B0L2FwcHMv" MPISPAWN_GENERIC_NAME_75=__PERSONAL_PATH__ MPISPAWN_GENERIC_VALUE_75="1" MPISPAWN_GENERIC_NAME_76=LMOD_SYSTEM_DEFAULT_MODULES MPISPAWN_GENERIC_VALUE_76="TACC" MPISPAWN_GENERIC_NAME_77=SLURM_JOB_NAME MPISPAWN_GENERIC_VALUE_77="stampedeLargeScaleAllTS.sh" MPISPAWN_GENERIC_NAME_78=SLURM_SUBMIT_HOST MPISPAWN_GENERIC_VALUE_78="login4.stampede.tacc.utexas.edu" MPISPAWN_GENERIC_NAME_79=LD_LIBRARY_PATH MPISPAWN_GENERIC_VALUE_79="/opt/apps/gcc4_7/mvapich2/1.9/lib:/opt/apps/gcc4_7/mvapich2/1.9/lib/shared:/opt/apps/gcc/4.7.1/lib64:/opt/apps/gcc/4.7.1/lib:/opt/apps/xsede/gsi-openssh-5.7/lib64:/opt/apps/xsede/gsi-openssh-5.7/lib64" MPISPAWN_GENERIC_NAME_80=SLURM_QUEUE MPISPAWN_GENERIC_VALUE_80="normal" MPISPAWN_GENERIC_NAME_81=PKG_CONFIG_PATH MPISPAWN_GENERIC_VALUE_81="/opt/apps/gcc4_7/mvapich2/1.9/lib/pkgconfig" MPISPAWN_GENERIC_NAME_82=LMOD_PREPEND_BLOCK MPISPAWN_GENERIC_VALUE_82="normal" MPISPAWN_GENERIC_NAME_83=TACC_FAMILY_COMPILER MPISPAWN_GENERIC_VALUE_83="gcc" MPISPAWN_GENERIC_NAME_84=TACC_DOMAIN MPISPAWN_GENERIC_VALUE_84="stampede" MPISPAWN_GENERIC_NAME_85=MV2_HOMOGENEOUS_CLUSTER MPISPAWN_GENERIC_VALUE_85="1" MPISPAWN_GENERIC_NAME_86=LMOD_FAMILY_COMPILER MPISPAWN_GENERIC_VALUE_86="gcc" MPISPAWN_GENERIC_NAME_87=MV2_USE_UD_HYBRID MPISPAWN_GENERIC_VALUE_87="0" MPISPAWN_GENERIC_NAME_88=MPICH_HOME MPISPAWN_GENERIC_VALUE_88="/opt/apps/gcc4_7/mvapich2/1.9" MPISPAWN_GENERIC_NAME_89=SLURM_JOB_NODELIST MPISPAWN_GENERIC_VALUE_89="c468-601,c471-503,c472-[401-402],c473-[303-304,401,804,901],c474-[203-204,301,401,403-404],c478-303,c482-404,c483-[001-004,101,504,601,804,901],c484-[101,103-104,201,602,701],c485-502,c486-102,c490-[302,904],c491-004,c492-701,c493-[001,102,201],c494-[101,402-403,602,702-703,904],c495-[001-002,201],c497-[102,202,504,601-603],c498-[304,402,603],c501-[204,301,604,801],c502-104,c503-901,c504-[602,604,702,801],c505-[001,202-204,401,502-504,802,901-904],c506-[302,704],c507-[102,202,503,601,904],c508-[101,204,601,702-703,802,903-904],c509-[002,201,301,303,401,501-502],c510-[003,401,404,501,503],c511-[601-602,902-903],c536-[801,902],c537-902,c538-[001,003,104],c540-[104,201-202,804,901-904]" MPISPAWN_GENERIC_NAME_90=SLURM_PROCID MPISPAWN_GENERIC_VALUE_90="0" MPISPAWN_GENERIC_NAME_91=I_MPI_DAPL_PROVIDER MPISPAWN_GENERIC_VALUE_91="ofa-v2-mlx4_0-1u" MPISPAWN_GENERIC_NAME_92=GLOBUS_LOCATION MPISPAWN_GENERIC_VALUE_92="/opt/apps/xsede/gsi-openssh-5.7" MPISPAWN_GENERIC_NAME_93=LMOD_COLORIZE MPISPAWN_GENERIC_VALUE_93="yes" MPISPAWN_GENERIC_NAME_94=I_MPI_HYDRA_BRANCH_COUNT MPISPAWN_GENERIC_VALUE_94="6000" MPISPAWN_GENERIC_NAME_95=SLURM_NNODES MPISPAWN_GENERIC_VALUE_95="128" MPISPAWN_GENERIC_NAME_96=MV2_DEFAULT_TIME_OUT MPISPAWN_GENERIC_VALUE_96="23" MPISPAWN_GENERIC_NAME_97=SLURM_TACC_NCORES_SET MPISPAWN_GENERIC_VALUE_97="1" MPISPAWN_GENERIC_NAME_98=__Init_Default_Modules MPISPAWN_GENERIC_VALUE_98="1" MPISPAWN_GENERIC_NAME_99=BASH_ENV MPISPAWN_GENERIC_VALUE_99="/etc/tacc/tacc_functions" MPISPAWN_GENERIC_NAME_100=SLURM_TOPOLOGY_ADDR_PATTERN MPISPAWN_GENERIC_VALUE_100="node" MPISPAWN_GENERIC_NAME_101=MV2_USE_RING_STARTUP MPISPAWN_GENERIC_VALUE_101="0" MPISPAWN_GENERIC_NAME_102=TACC_XALT_BIN MPISPAWN_GENERIC_VALUE_102="/opt/apps/xalt/0.4.4/bin" MPISPAWN_GENERIC_NAME_103=X509_USER_PROXY MPISPAWN_GENERIC_VALUE_103="/tmp/x509up_p3084.file9zM3fu.1" MPISPAWN_GENERIC_NAME_104=TMPDIR MPISPAWN_GENERIC_VALUE_104="/tmp" MPISPAWN_GENERIC_NAME_105=LMOD_FULL_SETTARG_SUPPORT MPISPAWN_GENERIC_VALUE_105="no" MPISPAWN_ID=51 MPISPAWN_WORKING_DIR=/home1/03222/tg825207/parallelnbody MPISPAWN_MPIRUN_RANK_0=816 MPISPAWN_MPIRUN_RANK_1=817 MPISPAWN_MPIRUN_RANK_2=818 MPISPAWN_MPIRUN_RANK_3=819 MPISPAWN_MPIRUN_RANK_4=820 MPISPAWN_MPIRUN_RANK_5=821 MPISPAWN_MPIRUN_RANK_6=822 MPISPAWN_MPIRUN_RANK_7=823 MPISPAWN_MPIRUN_RANK_8=824 MPISPAWN_MPIRUN_RANK_9=825 MPISPAWN_MPIRUN_RANK_10=826 MPISPAWN_MPIRUN_RANK_11=827 MPISPAWN_MPIRUN_RANK_12=828 MPISPAWN_MPIRUN_RANK_13=829 MPISPAWN_MPIRUN_RANK_14=830 MPISPAWN_MPIRUN_RANK_15=831 /opt/apps/gcc4_7/mvapich2/1.9/bin/mpispawn 0
bash: line 1: 36357 Segmentation fault      /usr/bin/env LD_LIBRARY_PATH=/opt/apps/gcc4_7/mvapich2/1.9/lib:/opt/apps/gcc4_7/mvapich2/1.9/lib/shared:/opt/apps/gcc/4.7.1/lib64:/opt/apps/gcc/4.7.1/lib:/opt/apps/xsede/gsi-openssh-5.7/lib64:/opt/apps/xsede/gsi-openssh-5.7/lib64 MPISPAWN_MPIRUN_MPD=0 USE_LINEAR_SSH=1 MPISPAWN_MPIRUN_HOST=c468-601.stampede.tacc.utexas.edu MPISPAWN_MPIRUN_HOSTIP=129.114.76.165 MPIRUN_RSH_LAUNCH=1 MPISPAWN_CHECKIN_PORT=36101 MPISPAWN_MPIRUN_PORT=36101 MPISPAWN_NNODES=64 MPISPAWN_GLOBAL_NPROCS=1024 MPISPAWN_MPIRUN_ID=44295 MPISPAWN_ARGC=4 MPDMAN_KVS_TEMPLATE=kvs_513_c468-601.stampede.tacc.utexas.edu_44295 MPISPAWN_LOCAL_NPROCS=16 MPISPAWN_ARGV_0='/home1/03222/tg825207/parallelnbody/teamscatter' MPISPAWN_ARGV_1='256000' MPISPAWN_ARGV_2='-c' MPISPAWN_ARGV_3='2' MPISPAWN_ARGC=4 MPISPAWN_GENERIC_ENV_COUNT=106 MPISPAWN_GENERIC_NAME_0=TACC_ENVLEN MPISPAWN_GENERIC_VALUE_0=6578 MPISPAWN_GENERIC_NAME_1=SLURM_JOB_ID MPISPAWN_GENERIC_VALUE_1="4306264" MPISPAWN_GENERIC_NAME_2=SLURMD_NODENAME MPISPAWN_GENERIC_VALUE_2="c468-601" MPISPAWN_GENERIC_NAME_3=STOCKYARD MPISPAWN_GENERIC_VALUE_3="/work/03222/tg825207" MPISPAWN_GENERIC_NAME_4=LC_ALL MPISPAWN_GENERIC_VALUE_4="en_US.UTF-8" MPISPAWN_GENERIC_NAME_5=SLURM_CHECKPOINT_IMAGE_DIR MPISPAWN_GENERIC_VALUE_5="/home1/03222/tg825207/parallelnbody" MPISPAWN_GENERIC_NAME_6=LMOD_FAMILY_MPI MPISPAWN_GENERIC_VALUE_6="mvapich2" MPISPAWN_GENERIC_NAME_7=SLURM_JOBID MPISPAWN_GENERIC_VALUE_7="4306264" MPISPAWN_GENERIC_NAME_8=_ModuleTable001_ MPISPAWN_GENERIC_VALUE_8="X01vZHVsZVRhYmxlXz17WyJhY3RpdmVTaXplIl09OCxiYXNlTXBhdGhBPXsiL29wdC9hcHBzL3hzZWRlL21vZHVsZWZpbGVzIiwiL29wdC9hcHBzL21vZHVsZWZpbGVzIiwiL29wdC9tb2R1bGVmaWxlcyIsfSxbImNfcmVidWlsZFRpbWUiXT1mYWxzZSxbImNfc2hvcnRUaW1lIl09ZmFsc2UsZmFtaWx5PXtbIk1QSSJdPSJtdmFwaWNoMiIsWyJjb21waWxlciJdPSJnY2MiLH0saW5hY3RpdmU9e30sbVQ9e0xpbnV4PXtbIkZOIl09Ii9vcHQvbW9kdWxlZmlsZXMvTGludXgiLFsiZGVmYXVsdCJdPTAsWyJmdWxsTmFtZSJdPSJMaW51eCIsWyJsb2FkT3JkZXIiXT0yLHByb3BUPXt9LFsic2hvcnQiXT0iTGludXgiLFsic3RhdHVzIl09ImFjdGl2ZSIsfSxUQUNDPXtbIkZOIl09Ii9v" MPISPAWN_GENERIC_NAME_9=TACC_SYSTEM MPISPAWN_GENERIC_VALUE_9="stampede" MPISPAWN_GENERIC_NAME_10=WORK MPISPAWN_GENERIC_VALUE_10="/work/03222/tg825207" MPISPAWN_GENERIC_NAME_11=SLURM_GTIDS MPISPAWN_GENERIC_VALUE_11="0" MPISPAWN_GENERIC_NAME_12=GCC_LIB MPISPAWN_GENERIC_VALUE_12="/opt/apps/gcc/4.7.1/lib64" MPISPAWN_GENERIC_NAME_13=SLURM_PRIO_PROCESS MPISPAWN_GENERIC_VALUE_13="0" MPISPAWN_GENERIC_NAME_14=SCRATCH MPISPAWN_GENERIC_VALUE_14="/scratch/03222/tg825207" MPISPAWN_GENERIC_NAME_15=LOGNAME MPISPAWN_GENERIC_VALUE_15="tg825207" MPISPAWN_GENERIC_NAME_16=_ModuleTable002_ MPISPAWN_GENERIC_VALUE_16="cHQvbW9kdWxlZmlsZXMvVEFDQyIsWyJkZWZhdWx0Il09MCxbImZ1bGxOYW1lIl09IlRBQ0MiLFsibG9hZE9yZGVyIl09Nixwcm9wVD17fSxbInNob3J0Il09IlRBQ0MiLFsic3RhdHVzIl09ImFjdGl2ZSIsfSxbIlRBQ0MtcGF0aHMiXT17WyJGTiJdPSIvb3B0L21vZHVsZWZpbGVzL1RBQ0MtcGF0aHMiLFsiZGVmYXVsdCJdPTAsWyJmdWxsTmFtZSJdPSJUQUNDLXBhdGhzIixbImxvYWRPcmRlciJdPTEscHJvcFQ9e30sWyJzaG9ydCJdPSJUQUNDLXBhdGhzIixbInN0YXR1cyJdPSJhY3RpdmUiLH0sY2x1c3Rlcj17WyJGTiJdPSIvb3B0L21vZHVsZWZpbGVzL2NsdXN0ZXIiLFsiZGVmYXVsdCJdPTAsWyJmdWxsTmFtZSJdPSJjbHVzdGVyIixbImxvYWRPcmRlciJdPTUscHJvcFQ9" MPISPAWN_GENERIC_NAME_17=INPUTRC MPISPAWN_GENERIC_VALUE_17="/etc/inputrc" MPISPAWN_GENERIC_NAME_18=LMOD_CMD MPISPAWN_GENERIC_VALUE_18="/opt/apps/lmod/lmod/libexec/lmod" MPISPAWN_GENERIC_NAME_19=I_MPI_OFA_ADAPTER_NAME MPISPAWN_GENERIC_VALUE_19="mlx4_0" MPISPAWN_GENERIC_NAME_20=QTLIB MPISPAWN_GENERIC_VALUE_20="/usr/lib64/qt-3.3/lib" MPISPAWN_GENERIC_NAME_21=NODE_TASKS_PPN_INFO MPISPAWN_GENERIC_VALUE_21=""16,0_"" MPISPAWN_GENERIC_NAME_22=SLURM_CPUS_ON_NODE MPISPAWN_GENERIC_VALUE_22="16" MPISPAWN_GENERIC_NAME_23=PATH MPISPAWN_GENERIC_VALUE_23="/opt/apps/gcc4_7/mvapich2/1.9/bin:/opt/apps/gcc/4.7.1/bin:/opt/apps/xalt/0.4.4/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/opt/apps/xsede/gsi-openssh-5.7/bin:/usr/X11R6/bin:/opt/ofed/bin:/opt/ofed/sbin:." MPISPAWN_GENERIC_NAME_24=SLURM_TACC_JOBNAME MPISPAWN_GENERIC_VALUE_24="stampedeLargeScaleAllTS.sh" MPISPAWN_GENERIC_NAME_25=SLURM_NODEID MPISPAWN_GENERIC_VALUE_25="0" MPISPAWN_GENERIC_NAME_26=LMOD_PKG MPISPAWN_GENERIC_VALUE_26="/opt/apps/lmod/lmod" MPISPAWN_GENERIC_NAME_27=MIC_I_MPI_OFA_ADAPTER_NAME MPISPAWN_GENERIC_VALUE_27="mlx4_0" MPISPAWN_GENERIC_NAME_28=HISTSIZE MPISPAWN_GENERIC_VALUE_28="1000" MPISPAWN_GENERIC_NAME_29=SLURM_NTASKS MPISPAWN_GENERIC_VALUE_29="2048" MPISPAWN_GENERIC_NAME_30=SHLIB_PATH MPISPAWN_GENERIC_VALUE_30="/opt/apps/xsede/gsi-openssh-5.7/lib64" MPISPAWN_GENERIC_NAME_31=TACC_OVERRIDE_PROJECT MPISPAWN_GENERIC_VALUE_31="TG-ASC140020" MPISPAWN_GENERIC_NAME_32=_ModuleTable005_ MPISPAWN_GENERIC_VALUE_32="Z2NjNF83L21vZHVsZWZpbGVzIiwiL29wdC9hcHBzL3hzZWRlL21vZHVsZWZpbGVzIiwiL29wdC9hcHBzL21vZHVsZWZpbGVzIiwiL29wdC9tb2R1bGVmaWxlcyIsfSxbInN5c3RlbUJhc2VNUEFUSCJdPSIvb3B0L2FwcHMveHNlZGUvbW9kdWxlZmlsZXM6L29wdC9hcHBzL21vZHVsZWZpbGVzOi9vcHQvbW9kdWxlZmlsZXMiLFsidmVyc2lvbiJdPTIsfQ==" MPISPAWN_GENERIC_NAME_33=_ModuleTable_Sz_ MPISPAWN_GENERIC_VALUE_33="5" MPISPAWN_GENERIC_NAME_34=MAIL MPISPAWN_GENERIC_VALUE_34="/var/spool/mail/tg825207" MPISPAWN_GENERIC_NAME_35=MV2_USE_HUGEPAGES MPISPAWN_GENERIC_VALUE_35="0" MPISPAWN_GENERIC_NAME_36=SLURM_TOPOLOGY_ADDR MPISPAWN_GENERIC_VALUE_36="c468-601" MPISPAWN_GENERIC_NAME_37=TACC_XALT_DIR MPISPAWN_GENERIC_VALUE_37="/opt/apps/xalt/0.4.4/" MPISPAWN_GENERIC_NAME_38=QTDIR MPISPAWN_GENERIC_VALUE_38="/usr/lib64/qt-3.3" MPISPAWN_GENERIC_NAME_39=SYSTEM MPISPAWN_GENERIC_VALUE_39="linux" MPISPAWN_GENERIC_NAME_40=SLURM_TACC_RUNLIMIT_MINS MPISPAWN_GENERIC_VALUE_40="180" MPISPAWN_GENERIC_NAME_41=SLURM_SUBMIT_DIR MPISPAWN_GENERIC_VALUE_41="/home1/03222/tg825207/parallelnbody" MPISPAWN_GENERIC_NAME_42=LIBPATH MPISPAWN_GENERIC_VALUE_42="/opt/apps/xsede/gsi-openssh-5.7/lib64:/usr/lib:/lib" MPISPAWN_GENERIC_NAME_43=HOSTNAME MPISPAWN_GENERIC_VALUE_43="c468-601" MPISPAWN_GENERIC_NAME_44=LMOD_DEFAULT_MODULEPATH MPISPAWN_GENERIC_VALUE_44="/opt/apps/xsede/modulefiles:/opt/apps/modulefiles:/opt/modulefiles" MPISPAWN_GENERIC_NAME_45=ARCHIVE MPISPAWN_GENERIC_VALUE_45="/home/03222/tg825207" MPISPAWN_GENERIC_NAME_46=_ MPISPAWN_GENERIC_VALUE_46="/usr/local/bin/build_env.pl" MPISPAWN_GENERIC_NAME_47=__BASHRC_SOURCED__ MPISPAWN_GENERIC_VALUE_47="1" MPISPAWN_GENERIC_NAME_48=APPS MPISPAWN_GENERIC_VALUE_48="/opt/apps" MPISPAWN_GENERIC_NAME_49=SHELL MPISPAWN_GENERIC_VALUE_49="/bin/bash" MPISPAWN_GENERIC_NAME_50=ARMCI_OPENIB_DEVICE MPISPAWN_GENERIC_VALUE_50="mlx4_0" MPISPAWN_GENERIC_NAME_51=ENVIRONMENT MPISPAWN_GENERIC_VALUE_51="BATCH" MPISPAWN_GENERIC_NAME_52=TACC_FAMILY_MPI MPISPAWN_GENERIC_VALUE_52="mvapich2" MPISPAWN_GENERIC_NAME_53=MV2_IBA_HCA MPISPAWN_GENERIC_VALUE_53="mlx4_0" MPISPAWN_GENERIC_NAME_54=TACC_PUBLIC_MACHINE MPISPAWN_GENERIC_VALUE_54="1" MPISPAWN_GENERIC_NAME_55=SLURM_TACC_NODES MPISPAWN_GENERIC_VALUE_55="1" MPISPAWN_GENERIC_NAME_56=_ModuleTable003_ MPISPAWN_GENERIC_VALUE_56="e30sWyJzaG9ydCJdPSJjbHVzdGVyIixbInN0YXR1cyJdPSJhY3RpdmUiLH0sWyJjbHVzdGVyLXBhdGhzIl09e1siRk4iXT0iL29wdC9tb2R1bGVmaWxlcy9jbHVzdGVyLXBhdGhzIixbImRlZmF1bHQiXT0wLFsiZnVsbE5hbWUiXT0iY2x1c3Rlci1wYXRocyIsWyJsb2FkT3JkZXIiXT0zLHByb3BUPXt9LFsic2hvcnQiXT0iY2x1c3Rlci1wYXRocyIsWyJzdGF0dXMiXT0iYWN0aXZlIix9LGdjYz17WyJGTiJdPSIvb3B0L2FwcHMvbW9kdWxlZmlsZXMvZ2NjLzQuNy4xLmx1YSIsWyJkZWZhdWx0Il09MCxbImZ1bGxOYW1lIl09ImdjYy80LjcuMSIsWyJsb2FkT3JkZXIiXT03LHByb3BUPXt9LFsic2hvcnQiXT0iZ2NjIixbInN0YXR1cyJdPSJhY3RpdmUiLH0sbXZhcGljaDI9e1si" MPISPAWN_GENERIC_NAME_57=QTINC MPISPAWN_GENERIC_VALUE_57="/usr/lib64/qt-3.3/include" MPISPAWN_GENERIC_NAME_58=CVS_RSH MPISPAWN_GENERIC_VALUE_58="ssh" MPISPAWN_GENERIC_NAME_59=MV2_USE_OLD_BCAST MPISPAWN_GENERIC_VALUE_59="1" MPISPAWN_GENERIC_NAME_60=SLURM_JOB_NUM_NODES MPISPAWN_GENERIC_VALUE_60="128" MPISPAWN_GENERIC_NAME_61=SLURM_TASK_PID MPISPAWN_GENERIC_VALUE_61="31479" MPISPAWN_GENERIC_NAME_62=TACC_MPI_GETMODE MPISPAWN_GENERIC_VALUE_62="mvapich2_ssh" MPISPAWN_GENERIC_NAME_63=LANG MPISPAWN_GENERIC_VALUE_63="en_US.UTF-8" MPISPAWN_GENERIC_NAME_64=USER MPISPAWN_GENERIC_VALUE_64="tg825207" MPISPAWN_GENERIC_NAME_65=MIC_I_MPI_DAPL_PROVIDER MPISPAWN_GENERIC_VALUE_65="ofa-v2-mlx4_0-1u" MPISPAWN_GENERIC_NAME_66=G_BROKEN_FILENAMES MPISPAWN_GENERIC_VALUE_66="1" MPISPAWN_GENERIC_NAME_67=SLURM_LOCALID MPISPAWN_GENERIC_VALUE_67="0" MPISPAWN_GENERIC_NAME_68=SLURM_NPROCS MPISPAWN_GENERIC_VALUE_68="2048" MPISPAWN_GENERIC_NAME_69=SLURM_TACC_NNODES_SET MPISPAWN_GENERIC_VALUE_69="0" MPISPAWN_GENERIC_NAME_70=SLURM_TACC_CORES MPISPAWN_GENERIC_VALUE_70="2048" MPISPAWN_GENERIC_NAME_71=LMOD_DIR MPISPAWN_GENERIC_VALUE_71="/opt/apps/lmod/lmod/libexec" MPISPAWN_GENERIC_NAME_72=LMOD_SETTARG_CMD MPISPAWN_GENERIC_VALUE_72=":" MPISPAWN_GENERIC_NAME_73=OMP_NUM_THREADS MPISPAWN_GENERIC_VALUE_73="1" MPISPAWN_GENERIC_NAME_74=_ModuleTable004_ MPISPAWN_GENERIC_VALUE_74="Rk4iXT0iL29wdC9hcHBzL2djYzRfNy9tb2R1bGVmaWxlcy9tdmFwaWNoMi8xLjlhMiIsWyJkZWZhdWx0Il09MSxbImZ1bGxOYW1lIl09Im12YXBpY2gyLzEuOWEyIixbImxvYWRPcmRlciJdPTgscHJvcFQ9e30sWyJzaG9ydCJdPSJtdmFwaWNoMiIsWyJzdGF0dXMiXT0iYWN0aXZlIix9LHhhbHQ9e1siRk4iXT0iL29wdC9hcHBzL21vZHVsZWZpbGVzL3hhbHQvMC40LjQubHVhIixbImRlZmF1bHQiXT0xLFsiZnVsbE5hbWUiXT0ieGFsdC8wLjQuNCIsWyJsb2FkT3JkZXIiXT00LHByb3BUPXt9LFsic2hvcnQiXT0ieGFsdCIsWyJzdGF0dXMiXT0iYWN0aXZlIix9LH0sbXBhdGhBPXsiL29wdC9hcHBzL2djYzRfNy9tdmFwaWNoMl8xXzkvbW9kdWxlZmlsZXMiLCIvb3B0L2FwcHMv" MPISPAWN_GENERIC_NAME_75=__PERSONAL_PATH__ MPISPAWN_GENERIC_VALUE_75="1" MPISPAWN_GENERIC_NAME_76=LMOD_SYSTEM_DEFAULT_MODULES MPISPAWN_GENERIC_VALUE_76="TACC" MPISPAWN_GENERIC_NAME_77=SLURM_JOB_NAME MPISPAWN_GENERIC_VALUE_77="stampedeLargeScaleAllTS.sh" MPISPAWN_GENERIC_NAME_78=SLURM_SUBMIT_HOST MPISPAWN_GENERIC_VALUE_78="login4.stampede.tacc.utexas.edu" MPISPAWN_GENERIC_NAME_79=LD_LIBRARY_PATH MPISPAWN_GENERIC_VALUE_79="/opt/apps/gcc4_7/mvapich2/1.9/lib:/opt/apps/gcc4_7/mvapich2/1.9/lib/shared:/opt/apps/gcc/4.7.1/lib64:/opt/apps/gcc/4.7.1/lib:/opt/apps/xsede/gsi-openssh-5.7/lib64:/opt/apps/xsede/gsi-openssh-5.7/lib64" MPISPAWN_GENERIC_NAME_80=SLURM_QUEUE MPISPAWN_GENERIC_VALUE_80="normal" MPISPAWN_GENERIC_NAME_81=PKG_CONFIG_PATH MPISPAWN_GENERIC_VALUE_81="/opt/apps/gcc4_7/mvapich2/1.9/lib/pkgconfig" MPISPAWN_GENERIC_NAME_82=LMOD_PREPEND_BLOCK MPISPAWN_GENERIC_VALUE_82="normal" MPISPAWN_GENERIC_NAME_83=TACC_FAMILY_COMPILER MPISPAWN_GENERIC_VALUE_83="gcc" MPISPAWN_GENERIC_NAME_84=TACC_DOMAIN MPISPAWN_GENERIC_VALUE_84="stampede" MPISPAWN_GENERIC_NAME_85=MV2_HOMOGENEOUS_CLUSTER MPISPAWN_GENERIC_VALUE_85="1" MPISPAWN_GENERIC_NAME_86=LMOD_FAMILY_COMPILER MPISPAWN_GENERIC_VALUE_86="gcc" MPISPAWN_GENERIC_NAME_87=MV2_USE_UD_HYBRID MPISPAWN_GENERIC_VALUE_87="0" MPISPAWN_GENERIC_NAME_88=MPICH_HOME MPISPAWN_GENERIC_VALUE_88="/opt/apps/gcc4_7/mvapich2/1.9" MPISPAWN_GENERIC_NAME_89=SLURM_JOB_NODELIST MPISPAWN_GENERIC_VALUE_89="c468-601,c471-503,c472-[401-402],c473-[303-304,401,804,901],c474-[203-204,301,401,403-404],c478-303,c482-404,c483-[001-004,101,504,601,804,901],c484-[101,103-104,201,602,701],c485-502,c486-102,c490-[302,904],c491-004,c492-701,c493-[001,102,201],c494-[101,402-403,602,702-703,904],c495-[001-002,201],c497-[102,202,504,601-603],c498-[304,402,603],c501-[204,301,604,801],c502-104,c503-901,c504-[602,604,702,801],c505-[001,202-204,401,502-504,802,901-904],c506-[302,704],c507-[102,202,503,601,904],c508-[101,204,601,702-703,802,903-904],c509-[002,201,301,303,401,501-502],c510-[003,401,404,501,503],c511-[601-602,902-903],c536-[801,902],c537-902,c538-[001,003,104],c540-[104,201-202,804,901-904]" MPISPAWN_GENERIC_NAME_90=SLURM_PROCID MPISPAWN_GENERIC_VALUE_90="0" MPISPAWN_GENERIC_NAME_91=I_MPI_DAPL_PROVIDER MPISPAWN_GENERIC_VALUE_91="ofa-v2-mlx4_0-1u" MPISPAWN_GENERIC_NAME_92=GLOBUS_LOCATION MPISPAWN_GENERIC_VALUE_92="/opt/apps/xsede/gsi-openssh-5.7" MPISPAWN_GENERIC_NAME_93=LMOD_COLORIZE MPISPAWN_GENERIC_VALUE_93="yes" MPISPAWN_GENERIC_NAME_94=I_MPI_HYDRA_BRANCH_COUNT MPISPAWN_GENERIC_VALUE_94="6000" MPISPAWN_GENERIC_NAME_95=SLURM_NNODES MPISPAWN_GENERIC_VALUE_95="128" MPISPAWN_GENERIC_NAME_96=MV2_DEFAULT_TIME_OUT MPISPAWN_GENERIC_VALUE_96="23" MPISPAWN_GENERIC_NAME_97=SLURM_TACC_NCORES_SET MPISPAWN_GENERIC_VALUE_97="1" MPISPAWN_GENERIC_NAME_98=__Init_Default_Modules MPISPAWN_GENERIC_VALUE_98="1" MPISPAWN_GENERIC_NAME_99=BASH_ENV MPISPAWN_GENERIC_VALUE_99="/etc/tacc/tacc_functions" MPISPAWN_GENERIC_NAME_100=SLURM_TOPOLOGY_ADDR_PATTERN MPISPAWN_GENERIC_VALUE_100="node" MPISPAWN_GENERIC_NAME_101=MV2_USE_RING_STARTUP MPISPAWN_GENERIC_VALUE_101="0" MPISPAWN_GENERIC_NAME_102=TACC_XALT_BIN MPISPAWN_GENERIC_VALUE_102="/opt/apps/xalt/0.4.4/bin" MPISPAWN_GENERIC_NAME_103=X509_USER_PROXY MPISPAWN_GENERIC_VALUE_103="/tmp/x509up_p3084.file9zM3fu.1" MPISPAWN_GENERIC_NAME_104=TMPDIR MPISPAWN_GENERIC_VALUE_104="/tmp" MPISPAWN_GENERIC_NAME_105=LMOD_FULL_SETTARG_SUPPORT MPISPAWN_GENERIC_VALUE_105="no" MPISPAWN_ID=57 MPISPAWN_WORKING_DIR=/home1/03222/tg825207/parallelnbody MPISPAWN_MPIRUN_RANK_0=912 MPISPAWN_MPIRUN_RANK_1=913 MPISPAWN_MPIRUN_RANK_2=914 MPISPAWN_MPIRUN_RANK_3=915 MPISPAWN_MPIRUN_RANK_4=916 MPISPAWN_MPIRUN_RANK_5=917 MPISPAWN_MPIRUN_RANK_6=918 MPISPAWN_MPIRUN_RANK_7=919 MPISPAWN_MPIRUN_RANK_8=920 MPISPAWN_MPIRUN_RANK_9=921 MPISPAWN_MPIRUN_RANK_10=922 MPISPAWN_MPIRUN_RANK_11=923 MPISPAWN_MPIRUN_RANK_12=924 MPISPAWN_MPIRUN_RANK_13=925 MPISPAWN_MPIRUN_RANK_14=926 MPISPAWN_MPIRUN_RANK_15=927 /opt/apps/gcc4_7/mvapich2/1.9/bin/mpispawn 0
[c501-301.stampede.tacc.utexas.edu:mpispawn_61][error_sighandler] Caught error: Segmentation fault (signal 11)
[c494-702.stampede.tacc.utexas.edu:mpispawn_45][error_sighandler] Caught error: Segmentation fault (signal 11)
bash: line 1: 55059 Segmentation fault      /usr/bin/env LD_LIBRARY_PATH=/opt/apps/gcc4_7/mvapich2/1.9/lib:/opt/apps/gcc4_7/mvapich2/1.9/lib/shared:/opt/apps/gcc/4.7.1/lib64:/opt/apps/gcc/4.7.1/lib:/opt/apps/xsede/gsi-openssh-5.7/lib64:/opt/apps/xsede/gsi-openssh-5.7/lib64 MPISPAWN_MPIRUN_MPD=0 USE_LINEAR_SSH=1 MPISPAWN_MPIRUN_HOST=c468-601.stampede.tacc.utexas.edu MPISPAWN_MPIRUN_HOSTIP=129.114.76.165 MPIRUN_RSH_LAUNCH=1 MPISPAWN_CHECKIN_PORT=36101 MPISPAWN_MPIRUN_PORT=36101 MPISPAWN_NNODES=64 MPISPAWN_GLOBAL_NPROCS=1024 MPISPAWN_MPIRUN_ID=44295 MPISPAWN_ARGC=4 MPDMAN_KVS_TEMPLATE=kvs_513_c468-601.stampede.tacc.utexas.edu_44295 MPISPAWN_LOCAL_NPROCS=16 MPISPAWN_ARGV_0='/home1/03222/tg825207/parallelnbody/teamscatter' MPISPAWN_ARGV_1='256000' MPISPAWN_ARGV_2='-c' MPISPAWN_ARGV_3='2' MPISPAWN_ARGC=4 MPISPAWN_GENERIC_ENV_COUNT=106 MPISPAWN_GENERIC_NAME_0=TACC_ENVLEN MPISPAWN_GENERIC_VALUE_0=6578 MPISPAWN_GENERIC_NAME_1=SLURM_JOB_ID MPISPAWN_GENERIC_VALUE_1="4306264" MPISPAWN_GENERIC_NAME_2=SLURMD_NODENAME MPISPAWN_GENERIC_VALUE_2="c468-601" MPISPAWN_GENERIC_NAME_3=STOCKYARD MPISPAWN_GENERIC_VALUE_3="/work/03222/tg825207" MPISPAWN_GENERIC_NAME_4=LC_ALL MPISPAWN_GENERIC_VALUE_4="en_US.UTF-8" MPISPAWN_GENERIC_NAME_5=SLURM_CHECKPOINT_IMAGE_DIR MPISPAWN_GENERIC_VALUE_5="/home1/03222/tg825207/parallelnbody" MPISPAWN_GENERIC_NAME_6=LMOD_FAMILY_MPI MPISPAWN_GENERIC_VALUE_6="mvapich2" MPISPAWN_GENERIC_NAME_7=SLURM_JOBID MPISPAWN_GENERIC_VALUE_7="4306264" MPISPAWN_GENERIC_NAME_8=_ModuleTable001_ MPISPAWN_GENERIC_VALUE_8="X01vZHVsZVRhYmxlXz17WyJhY3RpdmVTaXplIl09OCxiYXNlTXBhdGhBPXsiL29wdC9hcHBzL3hzZWRlL21vZHVsZWZpbGVzIiwiL29wdC9hcHBzL21vZHVsZWZpbGVzIiwiL29wdC9tb2R1bGVmaWxlcyIsfSxbImNfcmVidWlsZFRpbWUiXT1mYWxzZSxbImNfc2hvcnRUaW1lIl09ZmFsc2UsZmFtaWx5PXtbIk1QSSJdPSJtdmFwaWNoMiIsWyJjb21waWxlciJdPSJnY2MiLH0saW5hY3RpdmU9e30sbVQ9e0xpbnV4PXtbIkZOIl09Ii9vcHQvbW9kdWxlZmlsZXMvTGludXgiLFsiZGVmYXVsdCJdPTAsWyJmdWxsTmFtZSJdPSJMaW51eCIsWyJsb2FkT3JkZXIiXT0yLHByb3BUPXt9LFsic2hvcnQiXT0iTGludXgiLFsic3RhdHVzIl09ImFjdGl2ZSIsfSxUQUNDPXtbIkZOIl09Ii9v" MPISPAWN_GENERIC_NAME_9=TACC_SYSTEM MPISPAWN_GENERIC_VALUE_9="stampede" MPISPAWN_GENERIC_NAME_10=WORK MPISPAWN_GENERIC_VALUE_10="/work/03222/tg825207" MPISPAWN_GENERIC_NAME_11=SLURM_GTIDS MPISPAWN_GENERIC_VALUE_11="0" MPISPAWN_GENERIC_NAME_12=GCC_LIB MPISPAWN_GENERIC_VALUE_12="/opt/apps/gcc/4.7.1/lib64" MPISPAWN_GENERIC_NAME_13=SLURM_PRIO_PROCESS MPISPAWN_GENERIC_VALUE_13="0" MPISPAWN_GENERIC_NAME_14=SCRATCH MPISPAWN_GENERIC_VALUE_14="/scratch/03222/tg825207" MPISPAWN_GENERIC_NAME_15=LOGNAME MPISPAWN_GENERIC_VALUE_15="tg825207" MPISPAWN_GENERIC_NAME_16=_ModuleTable002_ MPISPAWN_GENERIC_VALUE_16="cHQvbW9kdWxlZmlsZXMvVEFDQyIsWyJkZWZhdWx0Il09MCxbImZ1bGxOYW1lIl09IlRBQ0MiLFsibG9hZE9yZGVyIl09Nixwcm9wVD17fSxbInNob3J0Il09IlRBQ0MiLFsic3RhdHVzIl09ImFjdGl2ZSIsfSxbIlRBQ0MtcGF0aHMiXT17WyJGTiJdPSIvb3B0L21vZHVsZWZpbGVzL1RBQ0MtcGF0aHMiLFsiZGVmYXVsdCJdPTAsWyJmdWxsTmFtZSJdPSJUQUNDLXBhdGhzIixbImxvYWRPcmRlciJdPTEscHJvcFQ9e30sWyJzaG9ydCJdPSJUQUNDLXBhdGhzIixbInN0YXR1cyJdPSJhY3RpdmUiLH0sY2x1c3Rlcj17WyJGTiJdPSIvb3B0L21vZHVsZWZpbGVzL2NsdXN0ZXIiLFsiZGVmYXVsdCJdPTAsWyJmdWxsTmFtZSJdPSJjbHVzdGVyIixbImxvYWRPcmRlciJdPTUscHJvcFQ9" MPISPAWN_GENERIC_NAME_17=INPUTRC MPISPAWN_GENERIC_VALUE_17="/etc/inputrc" MPISPAWN_GENERIC_NAME_18=LMOD_CMD MPISPAWN_GENERIC_VALUE_18="/opt/apps/lmod/lmod/libexec/lmod" MPISPAWN_GENERIC_NAME_19=I_MPI_OFA_ADAPTER_NAME MPISPAWN_GENERIC_VALUE_19="mlx4_0" MPISPAWN_GENERIC_NAME_20=QTLIB MPISPAWN_GENERIC_VALUE_20="/usr/lib64/qt-3.3/lib" MPISPAWN_GENERIC_NAME_21=NODE_TASKS_PPN_INFO MPISPAWN_GENERIC_VALUE_21=""16,0_"" MPISPAWN_GENERIC_NAME_22=SLURM_CPUS_ON_NODE MPISPAWN_GENERIC_VALUE_22="16" MPISPAWN_GENERIC_NAME_23=PATH MPISPAWN_GENERIC_VALUE_23="/opt/apps/gcc4_7/mvapich2/1.9/bin:/opt/apps/gcc/4.7.1/bin:/opt/apps/xalt/0.4.4/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/opt/apps/xsede/gsi-openssh-5.7/bin:/usr/X11R6/bin:/opt/ofed/bin:/opt/ofed/sbin:." MPISPAWN_GENERIC_NAME_24=SLURM_TACC_JOBNAME MPISPAWN_GENERIC_VALUE_24="stampedeLargeScaleAllTS.sh" MPISPAWN_GENERIC_NAME_25=SLURM_NODEID MPISPAWN_GENERIC_VALUE_25="0" MPISPAWN_GENERIC_NAME_26=LMOD_PKG MPISPAWN_GENERIC_VALUE_26="/opt/apps/lmod/lmod" MPISPAWN_GENERIC_NAME_27=MIC_I_MPI_OFA_ADAPTER_NAME MPISPAWN_GENERIC_VALUE_27="mlx4_0" MPISPAWN_GENERIC_NAME_28=HISTSIZE MPISPAWN_GENERIC_VALUE_28="1000" MPISPAWN_GENERIC_NAME_29=SLURM_NTASKS MPISPAWN_GENERIC_VALUE_29="2048" MPISPAWN_GENERIC_NAME_30=SHLIB_PATH MPISPAWN_GENERIC_VALUE_30="/opt/apps/xsede/gsi-openssh-5.7/lib64" MPISPAWN_GENERIC_NAME_31=TACC_OVERRIDE_PROJECT MPISPAWN_GENERIC_VALUE_31="TG-ASC140020" MPISPAWN_GENERIC_NAME_32=_ModuleTable005_ MPISPAWN_GENERIC_VALUE_32="Z2NjNF83L21vZHVsZWZpbGVzIiwiL29wdC9hcHBzL3hzZWRlL21vZHVsZWZpbGVzIiwiL29wdC9hcHBzL21vZHVsZWZpbGVzIiwiL29wdC9tb2R1bGVmaWxlcyIsfSxbInN5c3RlbUJhc2VNUEFUSCJdPSIvb3B0L2FwcHMveHNlZGUvbW9kdWxlZmlsZXM6L29wdC9hcHBzL21vZHVsZWZpbGVzOi9vcHQvbW9kdWxlZmlsZXMiLFsidmVyc2lvbiJdPTIsfQ==" MPISPAWN_GENERIC_NAME_33=_ModuleTable_Sz_ MPISPAWN_GENERIC_VALUE_33="5" MPISPAWN_GENERIC_NAME_34=MAIL MPISPAWN_GENERIC_VALUE_34="/var/spool/mail/tg825207" MPISPAWN_GENERIC_NAME_35=MV2_USE_HUGEPAGES MPISPAWN_GENERIC_VALUE_35="0" MPISPAWN_GENERIC_NAME_36=SLURM_TOPOLOGY_ADDR MPISPAWN_GENERIC_VALUE_36="c468-601" MPISPAWN_GENERIC_NAME_37=TACC_XALT_DIR MPISPAWN_GENERIC_VALUE_37="/opt/apps/xalt/0.4.4/" MPISPAWN_GENERIC_NAME_38=QTDIR MPISPAWN_GENERIC_VALUE_38="/usr/lib64/qt-3.3" MPISPAWN_GENERIC_NAME_39=SYSTEM MPISPAWN_GENERIC_VALUE_39="linux" MPISPAWN_GENERIC_NAME_40=SLURM_TACC_RUNLIMIT_MINS MPISPAWN_GENERIC_VALUE_40="180" MPISPAWN_GENERIC_NAME_41=SLURM_SUBMIT_DIR MPISPAWN_GENERIC_VALUE_41="/home1/03222/tg825207/parallelnbody" MPISPAWN_GENERIC_NAME_42=LIBPATH MPISPAWN_GENERIC_VALUE_42="/opt/apps/xsede/gsi-openssh-5.7/lib64:/usr/lib:/lib" MPISPAWN_GENERIC_NAME_43=HOSTNAME MPISPAWN_GENERIC_VALUE_43="c468-601" MPISPAWN_GENERIC_NAME_44=LMOD_DEFAULT_MODULEPATH MPISPAWN_GENERIC_VALUE_44="/opt/apps/xsede/modulefiles:/opt/apps/modulefiles:/opt/modulefiles" MPISPAWN_GENERIC_NAME_45=ARCHIVE MPISPAWN_GENERIC_VALUE_45="/home/03222/tg825207" MPISPAWN_GENERIC_NAME_46=_ MPISPAWN_GENERIC_VALUE_46="/usr/local/bin/build_env.pl" MPISPAWN_GENERIC_NAME_47=__BASHRC_SOURCED__ MPISPAWN_GENERIC_VALUE_47="1" MPISPAWN_GENERIC_NAME_48=APPS MPISPAWN_GENERIC_VALUE_48="/opt/apps" MPISPAWN_GENERIC_NAME_49=SHELL MPISPAWN_GENERIC_VALUE_49="/bin/bash" MPISPAWN_GENERIC_NAME_50=ARMCI_OPENIB_DEVICE MPISPAWN_GENERIC_VALUE_50="mlx4_0" MPISPAWN_GENERIC_NAME_51=ENVIRONMENT MPISPAWN_GENERIC_VALUE_51="BATCH" MPISPAWN_GENERIC_NAME_52=TACC_FAMILY_MPI MPISPAWN_GENERIC_VALUE_52="mvapich2" MPISPAWN_GENERIC_NAME_53=MV2_IBA_HCA MPISPAWN_GENERIC_VALUE_53="mlx4_0" MPISPAWN_GENERIC_NAME_54=TACC_PUBLIC_MACHINE MPISPAWN_GENERIC_VALUE_54="1" MPISPAWN_GENERIC_NAME_55=SLURM_TACC_NODES MPISPAWN_GENERIC_VALUE_55="1" MPISPAWN_GENERIC_NAME_56=_ModuleTable003_ MPISPAWN_GENERIC_VALUE_56="e30sWyJzaG9ydCJdPSJjbHVzdGVyIixbInN0YXR1cyJdPSJhY3RpdmUiLH0sWyJjbHVzdGVyLXBhdGhzIl09e1siRk4iXT0iL29wdC9tb2R1bGVmaWxlcy9jbHVzdGVyLXBhdGhzIixbImRlZmF1bHQiXT0wLFsiZnVsbE5hbWUiXT0iY2x1c3Rlci1wYXRocyIsWyJsb2FkT3JkZXIiXT0zLHByb3BUPXt9LFsic2hvcnQiXT0iY2x1c3Rlci1wYXRocyIsWyJzdGF0dXMiXT0iYWN0aXZlIix9LGdjYz17WyJGTiJdPSIvb3B0L2FwcHMvbW9kdWxlZmlsZXMvZ2NjLzQuNy4xLmx1YSIsWyJkZWZhdWx0Il09MCxbImZ1bGxOYW1lIl09ImdjYy80LjcuMSIsWyJsb2FkT3JkZXIiXT03LHByb3BUPXt9LFsic2hvcnQiXT0iZ2NjIixbInN0YXR1cyJdPSJhY3RpdmUiLH0sbXZhcGljaDI9e1si" MPISPAWN_GENERIC_NAME_57=QTINC MPISPAWN_GENERIC_VALUE_57="/usr/lib64/qt-3.3/include" MPISPAWN_GENERIC_NAME_58=CVS_RSH MPISPAWN_GENERIC_VALUE_58="ssh" MPISPAWN_GENERIC_NAME_59=MV2_USE_OLD_BCAST MPISPAWN_GENERIC_VALUE_59="1" MPISPAWN_GENERIC_NAME_60=SLURM_JOB_NUM_NODES MPISPAWN_GENERIC_VALUE_60="128" MPISPAWN_GENERIC_NAME_61=SLURM_TASK_PID MPISPAWN_GENERIC_VALUE_61="31479" MPISPAWN_GENERIC_NAME_62=TACC_MPI_GETMODE MPISPAWN_GENERIC_VALUE_62="mvapich2_ssh" MPISPAWN_GENERIC_NAME_63=LANG MPISPAWN_GENERIC_VALUE_63="en_US.UTF-8" MPISPAWN_GENERIC_NAME_64=USER MPISPAWN_GENERIC_VALUE_64="tg825207" MPISPAWN_GENERIC_NAME_65=MIC_I_MPI_DAPL_PROVIDER MPISPAWN_GENERIC_VALUE_65="ofa-v2-mlx4_0-1u" MPISPAWN_GENERIC_NAME_66=G_BROKEN_FILENAMES MPISPAWN_GENERIC_VALUE_66="1" MPISPAWN_GENERIC_NAME_67=SLURM_LOCALID MPISPAWN_GENERIC_VALUE_67="0" MPISPAWN_GENERIC_NAME_68=SLURM_NPROCS MPISPAWN_GENERIC_VALUE_68="2048" MPISPAWN_GENERIC_NAME_69=SLURM_TACC_NNODES_SET MPISPAWN_GENERIC_VALUE_69="0" MPISPAWN_GENERIC_NAME_70=SLURM_TACC_CORES MPISPAWN_GENERIC_VALUE_70="2048" MPISPAWN_GENERIC_NAME_71=LMOD_DIR MPISPAWN_GENERIC_VALUE_71="/opt/apps/lmod/lmod/libexec" MPISPAWN_GENERIC_NAME_72=LMOD_SETTARG_CMD MPISPAWN_GENERIC_VALUE_72=":" MPISPAWN_GENERIC_NAME_73=OMP_NUM_THREADS MPISPAWN_GENERIC_VALUE_73="1" MPISPAWN_GENERIC_NAME_74=_ModuleTable004_ MPISPAWN_GENERIC_VALUE_74="Rk4iXT0iL29wdC9hcHBzL2djYzRfNy9tb2R1bGVmaWxlcy9tdmFwaWNoMi8xLjlhMiIsWyJkZWZhdWx0Il09MSxbImZ1bGxOYW1lIl09Im12YXBpY2gyLzEuOWEyIixbImxvYWRPcmRlciJdPTgscHJvcFQ9e30sWyJzaG9ydCJdPSJtdmFwaWNoMiIsWyJzdGF0dXMiXT0iYWN0aXZlIix9LHhhbHQ9e1siRk4iXT0iL29wdC9hcHBzL21vZHVsZWZpbGVzL3hhbHQvMC40LjQubHVhIixbImRlZmF1bHQiXT0xLFsiZnVsbE5hbWUiXT0ieGFsdC8wLjQuNCIsWyJsb2FkT3JkZXIiXT00LHByb3BUPXt9LFsic2hvcnQiXT0ieGFsdCIsWyJzdGF0dXMiXT0iYWN0aXZlIix9LH0sbXBhdGhBPXsiL29wdC9hcHBzL2djYzRfNy9tdmFwaWNoMl8xXzkvbW9kdWxlZmlsZXMiLCIvb3B0L2FwcHMv" MPISPAWN_GENERIC_NAME_75=__PERSONAL_PATH__ MPISPAWN_GENERIC_VALUE_75="1" MPISPAWN_GENERIC_NAME_76=LMOD_SYSTEM_DEFAULT_MODULES MPISPAWN_GENERIC_VALUE_76="TACC" MPISPAWN_GENERIC_NAME_77=SLURM_JOB_NAME MPISPAWN_GENERIC_VALUE_77="stampedeLargeScaleAllTS.sh" MPISPAWN_GENERIC_NAME_78=SLURM_SUBMIT_HOST MPISPAWN_GENERIC_VALUE_78="login4.stampede.tacc.utexas.edu" MPISPAWN_GENERIC_NAME_79=LD_LIBRARY_PATH MPISPAWN_GENERIC_VALUE_79="/opt/apps/gcc4_7/mvapich2/1.9/lib:/opt/apps/gcc4_7/mvapich2/1.9/lib/shared:/opt/apps/gcc/4.7.1/lib64:/opt/apps/gcc/4.7.1/lib:/opt/apps/xsede/gsi-openssh-5.7/lib64:/opt/apps/xsede/gsi-openssh-5.7/lib64" MPISPAWN_GENERIC_NAME_80=SLURM_QUEUE MPISPAWN_GENERIC_VALUE_80="normal" MPISPAWN_GENERIC_NAME_81=PKG_CONFIG_PATH MPISPAWN_GENERIC_VALUE_81="/opt/apps/gcc4_7/mvapich2/1.9/lib/pkgconfig" MPISPAWN_GENERIC_NAME_82=LMOD_PREPEND_BLOCK MPISPAWN_GENERIC_VALUE_82="normal" MPISPAWN_GENERIC_NAME_83=TACC_FAMILY_COMPILER MPISPAWN_GENERIC_VALUE_83="gcc" MPISPAWN_GENERIC_NAME_84=TACC_DOMAIN MPISPAWN_GENERIC_VALUE_84="stampede" MPISPAWN_GENERIC_NAME_85=MV2_HOMOGENEOUS_CLUSTER MPISPAWN_GENERIC_VALUE_85="1" MPISPAWN_GENERIC_NAME_86=LMOD_FAMILY_COMPILER MPISPAWN_GENERIC_VALUE_86="gcc" MPISPAWN_GENERIC_NAME_87=MV2_USE_UD_HYBRID MPISPAWN_GENERIC_VALUE_87="0" MPISPAWN_GENERIC_NAME_88=MPICH_HOME MPISPAWN_GENERIC_VALUE_88="/opt/apps/gcc4_7/mvapich2/1.9" MPISPAWN_GENERIC_NAME_89=SLURM_JOB_NODELIST MPISPAWN_GENERIC_VALUE_89="c468-601,c471-503,c472-[401-402],c473-[303-304,401,804,901],c474-[203-204,301,401,403-404],c478-303,c482-404,c483-[001-004,101,504,601,804,901],c484-[101,103-104,201,602,701],c485-502,c486-102,c490-[302,904],c491-004,c492-701,c493-[001,102,201],c494-[101,402-403,602,702-703,904],c495-[001-002,201],c497-[102,202,504,601-603],c498-[304,402,603],c501-[204,301,604,801],c502-104,c503-901,c504-[602,604,702,801],c505-[001,202-204,401,502-504,802,901-904],c506-[302,704],c507-[102,202,503,601,904],c508-[101,204,601,702-703,802,903-904],c509-[002,201,301,303,401,501-502],c510-[003,401,404,501,503],c511-[601-602,902-903],c536-[801,902],c537-902,c538-[001,003,104],c540-[104,201-202,804,901-904]" MPISPAWN_GENERIC_NAME_90=SLURM_PROCID MPISPAWN_GENERIC_VALUE_90="0" MPISPAWN_GENERIC_NAME_91=I_MPI_DAPL_PROVIDER MPISPAWN_GENERIC_VALUE_91="ofa-v2-mlx4_0-1u" MPISPAWN_GENERIC_NAME_92=GLOBUS_LOCATION MPISPAWN_GENERIC_VALUE_92="/opt/apps/xsede/gsi-openssh-5.7" MPISPAWN_GENERIC_NAME_93=LMOD_COLORIZE MPISPAWN_GENERIC_VALUE_93="yes" MPISPAWN_GENERIC_NAME_94=I_MPI_HYDRA_BRANCH_COUNT MPISPAWN_GENERIC_VALUE_94="6000" MPISPAWN_GENERIC_NAME_95=SLURM_NNODES MPISPAWN_GENERIC_VALUE_95="128" MPISPAWN_GENERIC_NAME_96=MV2_DEFAULT_TIME_OUT MPISPAWN_GENERIC_VALUE_96="23" MPISPAWN_GENERIC_NAME_97=SLURM_TACC_NCORES_SET MPISPAWN_GENERIC_VALUE_97="1" MPISPAWN_GENERIC_NAME_98=__Init_Default_Modules MPISPAWN_GENERIC_VALUE_98="1" MPISPAWN_GENERIC_NAME_99=BASH_ENV MPISPAWN_GENERIC_VALUE_99="/etc/tacc/tacc_functions" MPISPAWN_GENERIC_NAME_100=SLURM_TOPOLOGY_ADDR_PATTERN MPISPAWN_GENERIC_VALUE_100="node" MPISPAWN_GENERIC_NAME_101=MV2_USE_RING_STARTUP MPISPAWN_GENERIC_VALUE_101="0" MPISPAWN_GENERIC_NAME_102=TACC_XALT_BIN MPISPAWN_GENERIC_VALUE_102="/opt/apps/xalt/0.4.4/bin" MPISPAWN_GENERIC_NAME_103=X509_USER_PROXY MPISPAWN_GENERIC_VALUE_103="/tmp/x509up_p3084.file9zM3fu.1" MPISPAWN_GENERIC_NAME_104=TMPDIR MPISPAWN_GENERIC_VALUE_104="/tmp" MPISPAWN_GENERIC_NAME_105=LMOD_FULL_SETTARG_SUPPORT MPISPAWN_GENERIC_VALUE_105="no" MPISPAWN_ID=61 MPISPAWN_WORKING_DIR=/home1/03222/tg825207/parallelnbody MPISPAWN_MPIRUN_RANK_0=976 MPISPAWN_MPIRUN_RANK_1=977 MPISPAWN_MPIRUN_RANK_2=978 MPISPAWN_MPIRUN_RANK_3=979 MPISPAWN_MPIRUN_RANK_4=980 MPISPAWN_MPIRUN_RANK_5=981 MPISPAWN_MPIRUN_RANK_6=982 MPISPAWN_MPIRUN_RANK_7=983 MPISPAWN_MPIRUN_RANK_8=984 MPISPAWN_MPIRUN_RANK_9=985 MPISPAWN_MPIRUN_RANK_10=986 MPISPAWN_MPIRUN_RANK_11=987 MPISPAWN_MPIRUN_RANK_12=988 MPISPAWN_MPIRUN_RANK_13=989 MPISPAWN_MPIRUN_RANK_14=990 MPISPAWN_MPIRUN_RANK_15=991 /opt/apps/gcc4_7/mvapich2/1.9/bin/mpispawn 0
bash: line 1: 56970 Segmentation fault      /usr/bin/env LD_LIBRARY_PATH=/opt/apps/gcc4_7/mvapich2/1.9/lib:/opt/apps/gcc4_7/mvapich2/1.9/lib/shared:/opt/apps/gcc/4.7.1/lib64:/opt/apps/gcc/4.7.1/lib:/opt/apps/xsede/gsi-openssh-5.7/lib64:/opt/apps/xsede/gsi-openssh-5.7/lib64 MPISPAWN_MPIRUN_MPD=0 USE_LINEAR_SSH=1 MPISPAWN_MPIRUN_HOST=c468-601.stampede.tacc.utexas.edu MPISPAWN_MPIRUN_HOSTIP=129.114.76.165 MPIRUN_RSH_LAUNCH=1 MPISPAWN_CHECKIN_PORT=36101 MPISPAWN_MPIRUN_PORT=36101 MPISPAWN_NNODES=64 MPISPAWN_GLOBAL_NPROCS=1024 MPISPAWN_MPIRUN_ID=44295 MPISPAWN_ARGC=4 MPDMAN_KVS_TEMPLATE=kvs_513_c468-601.stampede.tacc.utexas.edu_44295 MPISPAWN_LOCAL_NPROCS=16 MPISPAWN_ARGV_0='/home1/03222/tg825207/parallelnbody/teamscatter' MPISPAWN_ARGV_1='256000' MPISPAWN_ARGV_2='-c' MPISPAWN_ARGV_3='2' MPISPAWN_ARGC=4 MPISPAWN_GENERIC_ENV_COUNT=106 MPISPAWN_GENERIC_NAME_0=TACC_ENVLEN MPISPAWN_GENERIC_VALUE_0=6578 MPISPAWN_GENERIC_NAME_1=SLURM_JOB_ID MPISPAWN_GENERIC_VALUE_1="4306264" MPISPAWN_GENERIC_NAME_2=SLURMD_NODENAME MPISPAWN_GENERIC_VALUE_2="c468-601" MPISPAWN_GENERIC_NAME_3=STOCKYARD MPISPAWN_GENERIC_VALUE_3="/work/03222/tg825207" MPISPAWN_GENERIC_NAME_4=LC_ALL MPISPAWN_GENERIC_VALUE_4="en_US.UTF-8" MPISPAWN_GENERIC_NAME_5=SLURM_CHECKPOINT_IMAGE_DIR MPISPAWN_GENERIC_VALUE_5="/home1/03222/tg825207/parallelnbody" MPISPAWN_GENERIC_NAME_6=LMOD_FAMILY_MPI MPISPAWN_GENERIC_VALUE_6="mvapich2" MPISPAWN_GENERIC_NAME_7=SLURM_JOBID MPISPAWN_GENERIC_VALUE_7="4306264" MPISPAWN_GENERIC_NAME_8=_ModuleTable001_ MPISPAWN_GENERIC_VALUE_8="X01vZHVsZVRhYmxlXz17WyJhY3RpdmVTaXplIl09OCxiYXNlTXBhdGhBPXsiL29wdC9hcHBzL3hzZWRlL21vZHVsZWZpbGVzIiwiL29wdC9hcHBzL21vZHVsZWZpbGVzIiwiL29wdC9tb2R1bGVmaWxlcyIsfSxbImNfcmVidWlsZFRpbWUiXT1mYWxzZSxbImNfc2hvcnRUaW1lIl09ZmFsc2UsZmFtaWx5PXtbIk1QSSJdPSJtdmFwaWNoMiIsWyJjb21waWxlciJdPSJnY2MiLH0saW5hY3RpdmU9e30sbVQ9e0xpbnV4PXtbIkZOIl09Ii9vcHQvbW9kdWxlZmlsZXMvTGludXgiLFsiZGVmYXVsdCJdPTAsWyJmdWxsTmFtZSJdPSJMaW51eCIsWyJsb2FkT3JkZXIiXT0yLHByb3BUPXt9LFsic2hvcnQiXT0iTGludXgiLFsic3RhdHVzIl09ImFjdGl2ZSIsfSxUQUNDPXtbIkZOIl09Ii9v" MPISPAWN_GENERIC_NAME_9=TACC_SYSTEM MPISPAWN_GENERIC_VALUE_9="stampede" MPISPAWN_GENERIC_NAME_10=WORK MPISPAWN_GENERIC_VALUE_10="/work/03222/tg825207" MPISPAWN_GENERIC_NAME_11=SLURM_GTIDS MPISPAWN_GENERIC_VALUE_11="0" MPISPAWN_GENERIC_NAME_12=GCC_LIB MPISPAWN_GENERIC_VALUE_12="/opt/apps/gcc/4.7.1/lib64" MPISPAWN_GENERIC_NAME_13=SLURM_PRIO_PROCESS MPISPAWN_GENERIC_VALUE_13="0" MPISPAWN_GENERIC_NAME_14=SCRATCH MPISPAWN_GENERIC_VALUE_14="/scratch/03222/tg825207" MPISPAWN_GENERIC_NAME_15=LOGNAME MPISPAWN_GENERIC_VALUE_15="tg825207" MPISPAWN_GENERIC_NAME_16=_ModuleTable002_ MPISPAWN_GENERIC_VALUE_16="cHQvbW9kdWxlZmlsZXMvVEFDQyIsWyJkZWZhdWx0Il09MCxbImZ1bGxOYW1lIl09IlRBQ0MiLFsibG9hZE9yZGVyIl09Nixwcm9wVD17fSxbInNob3J0Il09IlRBQ0MiLFsic3RhdHVzIl09ImFjdGl2ZSIsfSxbIlRBQ0MtcGF0aHMiXT17WyJGTiJdPSIvb3B0L21vZHVsZWZpbGVzL1RBQ0MtcGF0aHMiLFsiZGVmYXVsdCJdPTAsWyJmdWxsTmFtZSJdPSJUQUNDLXBhdGhzIixbImxvYWRPcmRlciJdPTEscHJvcFQ9e30sWyJzaG9ydCJdPSJUQUNDLXBhdGhzIixbInN0YXR1cyJdPSJhY3RpdmUiLH0sY2x1c3Rlcj17WyJGTiJdPSIvb3B0L21vZHVsZWZpbGVzL2NsdXN0ZXIiLFsiZGVmYXVsdCJdPTAsWyJmdWxsTmFtZSJdPSJjbHVzdGVyIixbImxvYWRPcmRlciJdPTUscHJvcFQ9" MPISPAWN_GENERIC_NAME_17=INPUTRC MPISPAWN_GENERIC_VALUE_17="/etc/inputrc" MPISPAWN_GENERIC_NAME_18=LMOD_CMD MPISPAWN_GENERIC_VALUE_18="/opt/apps/lmod/lmod/libexec/lmod" MPISPAWN_GENERIC_NAME_19=I_MPI_OFA_ADAPTER_NAME MPISPAWN_GENERIC_VALUE_19="mlx4_0" MPISPAWN_GENERIC_NAME_20=QTLIB MPISPAWN_GENERIC_VALUE_20="/usr/lib64/qt-3.3/lib" MPISPAWN_GENERIC_NAME_21=NODE_TASKS_PPN_INFO MPISPAWN_GENERIC_VALUE_21=""16,0_"" MPISPAWN_GENERIC_NAME_22=SLURM_CPUS_ON_NODE MPISPAWN_GENERIC_VALUE_22="16" MPISPAWN_GENERIC_NAME_23=PATH MPISPAWN_GENERIC_VALUE_23="/opt/apps/gcc4_7/mvapich2/1.9/bin:/opt/apps/gcc/4.7.1/bin:/opt/apps/xalt/0.4.4/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/opt/apps/xsede/gsi-openssh-5.7/bin:/usr/X11R6/bin:/opt/ofed/bin:/opt/ofed/sbin:." MPISPAWN_GENERIC_NAME_24=SLURM_TACC_JOBNAME MPISPAWN_GENERIC_VALUE_24="stampedeLargeScaleAllTS.sh" MPISPAWN_GENERIC_NAME_25=SLURM_NODEID MPISPAWN_GENERIC_VALUE_25="0" MPISPAWN_GENERIC_NAME_26=LMOD_PKG MPISPAWN_GENERIC_VALUE_26="/opt/apps/lmod/lmod" MPISPAWN_GENERIC_NAME_27=MIC_I_MPI_OFA_ADAPTER_NAME MPISPAWN_GENERIC_VALUE_27="mlx4_0" MPISPAWN_GENERIC_NAME_28=HISTSIZE MPISPAWN_GENERIC_VALUE_28="1000" MPISPAWN_GENERIC_NAME_29=SLURM_NTASKS MPISPAWN_GENERIC_VALUE_29="2048" MPISPAWN_GENERIC_NAME_30=SHLIB_PATH MPISPAWN_GENERIC_VALUE_30="/opt/apps/xsede/gsi-openssh-5.7/lib64" MPISPAWN_GENERIC_NAME_31=TACC_OVERRIDE_PROJECT MPISPAWN_GENERIC_VALUE_31="TG-ASC140020" MPISPAWN_GENERIC_NAME_32=_ModuleTable005_ MPISPAWN_GENERIC_VALUE_32="Z2NjNF83L21vZHVsZWZpbGVzIiwiL29wdC9hcHBzL3hzZWRlL21vZHVsZWZpbGVzIiwiL29wdC9hcHBzL21vZHVsZWZpbGVzIiwiL29wdC9tb2R1bGVmaWxlcyIsfSxbInN5c3RlbUJhc2VNUEFUSCJdPSIvb3B0L2FwcHMveHNlZGUvbW9kdWxlZmlsZXM6L29wdC9hcHBzL21vZHVsZWZpbGVzOi9vcHQvbW9kdWxlZmlsZXMiLFsidmVyc2lvbiJdPTIsfQ==" MPISPAWN_GENERIC_NAME_33=_ModuleTable_Sz_ MPISPAWN_GENERIC_VALUE_33="5" MPISPAWN_GENERIC_NAME_34=MAIL MPISPAWN_GENERIC_VALUE_34="/var/spool/mail/tg825207" MPISPAWN_GENERIC_NAME_35=MV2_USE_HUGEPAGES MPISPAWN_GENERIC_VALUE_35="0" MPISPAWN_GENERIC_NAME_36=SLURM_TOPOLOGY_ADDR MPISPAWN_GENERIC_VALUE_36="c468-601" MPISPAWN_GENERIC_NAME_37=TACC_XALT_DIR MPISPAWN_GENERIC_VALUE_37="/opt/apps/xalt/0.4.4/" MPISPAWN_GENERIC_NAME_38=QTDIR MPISPAWN_GENERIC_VALUE_38="/usr/lib64/qt-3.3" MPISPAWN_GENERIC_NAME_39=SYSTEM MPISPAWN_GENERIC_VALUE_39="linux" MPISPAWN_GENERIC_NAME_40=SLURM_TACC_RUNLIMIT_MINS MPISPAWN_GENERIC_VALUE_40="180" MPISPAWN_GENERIC_NAME_41=SLURM_SUBMIT_DIR MPISPAWN_GENERIC_VALUE_41="/home1/03222/tg825207/parallelnbody" MPISPAWN_GENERIC_NAME_42=LIBPATH MPISPAWN_GENERIC_VALUE_42="/opt/apps/xsede/gsi-openssh-5.7/lib64:/usr/lib:/lib" MPISPAWN_GENERIC_NAME_43=HOSTNAME MPISPAWN_GENERIC_VALUE_43="c468-601" MPISPAWN_GENERIC_NAME_44=LMOD_DEFAULT_MODULEPATH MPISPAWN_GENERIC_VALUE_44="/opt/apps/xsede/modulefiles:/opt/apps/modulefiles:/opt/modulefiles" MPISPAWN_GENERIC_NAME_45=ARCHIVE MPISPAWN_GENERIC_VALUE_45="/home/03222/tg825207" MPISPAWN_GENERIC_NAME_46=_ MPISPAWN_GENERIC_VALUE_46="/usr/local/bin/build_env.pl" MPISPAWN_GENERIC_NAME_47=__BASHRC_SOURCED__ MPISPAWN_GENERIC_VALUE_47="1" MPISPAWN_GENERIC_NAME_48=APPS MPISPAWN_GENERIC_VALUE_48="/opt/apps" MPISPAWN_GENERIC_NAME_49=SHELL MPISPAWN_GENERIC_VALUE_49="/bin/bash" MPISPAWN_GENERIC_NAME_50=ARMCI_OPENIB_DEVICE MPISPAWN_GENERIC_VALUE_50="mlx4_0" MPISPAWN_GENERIC_NAME_51=ENVIRONMENT MPISPAWN_GENERIC_VALUE_51="BATCH" MPISPAWN_GENERIC_NAME_52=TACC_FAMILY_MPI MPISPAWN_GENERIC_VALUE_52="mvapich2" MPISPAWN_GENERIC_NAME_53=MV2_IBA_HCA MPISPAWN_GENERIC_VALUE_53="mlx4_0" MPISPAWN_GENERIC_NAME_54=TACC_PUBLIC_MACHINE MPISPAWN_GENERIC_VALUE_54="1" MPISPAWN_GENERIC_NAME_55=SLURM_TACC_NODES MPISPAWN_GENERIC_VALUE_55="1" MPISPAWN_GENERIC_NAME_56=_ModuleTable003_ MPISPAWN_GENERIC_VALUE_56="e30sWyJzaG9ydCJdPSJjbHVzdGVyIixbInN0YXR1cyJdPSJhY3RpdmUiLH0sWyJjbHVzdGVyLXBhdGhzIl09e1siRk4iXT0iL29wdC9tb2R1bGVmaWxlcy9jbHVzdGVyLXBhdGhzIixbImRlZmF1bHQiXT0wLFsiZnVsbE5hbWUiXT0iY2x1c3Rlci1wYXRocyIsWyJsb2FkT3JkZXIiXT0zLHByb3BUPXt9LFsic2hvcnQiXT0iY2x1c3Rlci1wYXRocyIsWyJzdGF0dXMiXT0iYWN0aXZlIix9LGdjYz17WyJGTiJdPSIvb3B0L2FwcHMvbW9kdWxlZmlsZXMvZ2NjLzQuNy4xLmx1YSIsWyJkZWZhdWx0Il09MCxbImZ1bGxOYW1lIl09ImdjYy80LjcuMSIsWyJsb2FkT3JkZXIiXT03LHByb3BUPXt9LFsic2hvcnQiXT0iZ2NjIixbInN0YXR1cyJdPSJhY3RpdmUiLH0sbXZhcGljaDI9e1si" MPISPAWN_GENERIC_NAME_57=QTINC MPISPAWN_GENERIC_VALUE_57="/usr/lib64/qt-3.3/include" MPISPAWN_GENERIC_NAME_58=CVS_RSH MPISPAWN_GENERIC_VALUE_58="ssh" MPISPAWN_GENERIC_NAME_59=MV2_USE_OLD_BCAST MPISPAWN_GENERIC_VALUE_59="1" MPISPAWN_GENERIC_NAME_60=SLURM_JOB_NUM_NODES MPISPAWN_GENERIC_VALUE_60="128" MPISPAWN_GENERIC_NAME_61=SLURM_TASK_PID MPISPAWN_GENERIC_VALUE_61="31479" MPISPAWN_GENERIC_NAME_62=TACC_MPI_GETMODE MPISPAWN_GENERIC_VALUE_62="mvapich2_ssh" MPISPAWN_GENERIC_NAME_63=LANG MPISPAWN_GENERIC_VALUE_63="en_US.UTF-8" MPISPAWN_GENERIC_NAME_64=USER MPISPAWN_GENERIC_VALUE_64="tg825207" MPISPAWN_GENERIC_NAME_65=MIC_I_MPI_DAPL_PROVIDER MPISPAWN_GENERIC_VALUE_65="ofa-v2-mlx4_0-1u" MPISPAWN_GENERIC_NAME_66=G_BROKEN_FILENAMES MPISPAWN_GENERIC_VALUE_66="1" MPISPAWN_GENERIC_NAME_67=SLURM_LOCALID MPISPAWN_GENERIC_VALUE_67="0" MPISPAWN_GENERIC_NAME_68=SLURM_NPROCS MPISPAWN_GENERIC_VALUE_68="2048" MPISPAWN_GENERIC_NAME_69=SLURM_TACC_NNODES_SET MPISPAWN_GENERIC_VALUE_69="0" MPISPAWN_GENERIC_NAME_70=SLURM_TACC_CORES MPISPAWN_GENERIC_VALUE_70="2048" MPISPAWN_GENERIC_NAME_71=LMOD_DIR MPISPAWN_GENERIC_VALUE_71="/opt/apps/lmod/lmod/libexec" MPISPAWN_GENERIC_NAME_72=LMOD_SETTARG_CMD MPISPAWN_GENERIC_VALUE_72=":" MPISPAWN_GENERIC_NAME_73=OMP_NUM_THREADS MPISPAWN_GENERIC_VALUE_73="1" MPISPAWN_GENERIC_NAME_74=_ModuleTable004_ MPISPAWN_GENERIC_VALUE_74="Rk4iXT0iL29wdC9hcHBzL2djYzRfNy9tb2R1bGVmaWxlcy9tdmFwaWNoMi8xLjlhMiIsWyJkZWZhdWx0Il09MSxbImZ1bGxOYW1lIl09Im12YXBpY2gyLzEuOWEyIixbImxvYWRPcmRlciJdPTgscHJvcFQ9e30sWyJzaG9ydCJdPSJtdmFwaWNoMiIsWyJzdGF0dXMiXT0iYWN0aXZlIix9LHhhbHQ9e1siRk4iXT0iL29wdC9hcHBzL21vZHVsZWZpbGVzL3hhbHQvMC40LjQubHVhIixbImRlZmF1bHQiXT0xLFsiZnVsbE5hbWUiXT0ieGFsdC8wLjQuNCIsWyJsb2FkT3JkZXIiXT00LHByb3BUPXt9LFsic2hvcnQiXT0ieGFsdCIsWyJzdGF0dXMiXT0iYWN0aXZlIix9LH0sbXBhdGhBPXsiL29wdC9hcHBzL2djYzRfNy9tdmFwaWNoMl8xXzkvbW9kdWxlZmlsZXMiLCIvb3B0L2FwcHMv" MPISPAWN_GENERIC_NAME_75=__PERSONAL_PATH__ MPISPAWN_GENERIC_VALUE_75="1" MPISPAWN_GENERIC_NAME_76=LMOD_SYSTEM_DEFAULT_MODULES MPISPAWN_GENERIC_VALUE_76="TACC" MPISPAWN_GENERIC_NAME_77=SLURM_JOB_NAME MPISPAWN_GENERIC_VALUE_77="stampedeLargeScaleAllTS.sh" MPISPAWN_GENERIC_NAME_78=SLURM_SUBMIT_HOST MPISPAWN_GENERIC_VALUE_78="login4.stampede.tacc.utexas.edu" MPISPAWN_GENERIC_NAME_79=LD_LIBRARY_PATH MPISPAWN_GENERIC_VALUE_79="/opt/apps/gcc4_7/mvapich2/1.9/lib:/opt/apps/gcc4_7/mvapich2/1.9/lib/shared:/opt/apps/gcc/4.7.1/lib64:/opt/apps/gcc/4.7.1/lib:/opt/apps/xsede/gsi-openssh-5.7/lib64:/opt/apps/xsede/gsi-openssh-5.7/lib64" MPISPAWN_GENERIC_NAME_80=SLURM_QUEUE MPISPAWN_GENERIC_VALUE_80="normal" MPISPAWN_GENERIC_NAME_81=PKG_CONFIG_PATH MPISPAWN_GENERIC_VALUE_81="/opt/apps/gcc4_7/mvapich2/1.9/lib/pkgconfig" MPISPAWN_GENERIC_NAME_82=LMOD_PREPEND_BLOCK MPISPAWN_GENERIC_VALUE_82="normal" MPISPAWN_GENERIC_NAME_83=TACC_FAMILY_COMPILER MPISPAWN_GENERIC_VALUE_83="gcc" MPISPAWN_GENERIC_NAME_84=TACC_DOMAIN MPISPAWN_GENERIC_VALUE_84="stampede" MPISPAWN_GENERIC_NAME_85=MV2_HOMOGENEOUS_CLUSTER MPISPAWN_GENERIC_VALUE_85="1" MPISPAWN_GENERIC_NAME_86=LMOD_FAMILY_COMPILER MPISPAWN_GENERIC_VALUE_86="gcc" MPISPAWN_GENERIC_NAME_87=MV2_USE_UD_HYBRID MPISPAWN_GENERIC_VALUE_87="0" MPISPAWN_GENERIC_NAME_88=MPICH_HOME MPISPAWN_GENERIC_VALUE_88="/opt/apps/gcc4_7/mvapich2/1.9" MPISPAWN_GENERIC_NAME_89=SLURM_JOB_NODELIST MPISPAWN_GENERIC_VALUE_89="c468-601,c471-503,c472-[401-402],c473-[303-304,401,804,901],c474-[203-204,301,401,403-404],c478-303,c482-404,c483-[001-004,101,504,601,804,901],c484-[101,103-104,201,602,701],c485-502,c486-102,c490-[302,904],c491-004,c492-701,c493-[001,102,201],c494-[101,402-403,602,702-703,904],c495-[001-002,201],c497-[102,202,504,601-603],c498-[304,402,603],c501-[204,301,604,801],c502-104,c503-901,c504-[602,604,702,801],c505-[001,202-204,401,502-504,802,901-904],c506-[302,704],c507-[102,202,503,601,904],c508-[101,204,601,702-703,802,903-904],c509-[002,201,301,303,401,501-502],c510-[003,401,404,501,503],c511-[601-602,902-903],c536-[801,902],c537-902,c538-[001,003,104],c540-[104,201-202,804,901-904]" MPISPAWN_GENERIC_NAME_90=SLURM_PROCID MPISPAWN_GENERIC_VALUE_90="0" MPISPAWN_GENERIC_NAME_91=I_MPI_DAPL_PROVIDER MPISPAWN_GENERIC_VALUE_91="ofa-v2-mlx4_0-1u" MPISPAWN_GENERIC_NAME_92=GLOBUS_LOCATION MPISPAWN_GENERIC_VALUE_92="/opt/apps/xsede/gsi-openssh-5.7" MPISPAWN_GENERIC_NAME_93=LMOD_COLORIZE MPISPAWN_GENERIC_VALUE_93="yes" MPISPAWN_GENERIC_NAME_94=I_MPI_HYDRA_BRANCH_COUNT MPISPAWN_GENERIC_VALUE_94="6000" MPISPAWN_GENERIC_NAME_95=SLURM_NNODES MPISPAWN_GENERIC_VALUE_95="128" MPISPAWN_GENERIC_NAME_96=MV2_DEFAULT_TIME_OUT MPISPAWN_GENERIC_VALUE_96="23" MPISPAWN_GENERIC_NAME_97=SLURM_TACC_NCORES_SET MPISPAWN_GENERIC_VALUE_97="1" MPISPAWN_GENERIC_NAME_98=__Init_Default_Modules MPISPAWN_GENERIC_VALUE_98="1" MPISPAWN_GENERIC_NAME_99=BASH_ENV MPISPAWN_GENERIC_VALUE_99="/etc/tacc/tacc_functions" MPISPAWN_GENERIC_NAME_100=SLURM_TOPOLOGY_ADDR_PATTERN MPISPAWN_GENERIC_VALUE_100="node" MPISPAWN_GENERIC_NAME_101=MV2_USE_RING_STARTUP MPISPAWN_GENERIC_VALUE_101="0" MPISPAWN_GENERIC_NAME_102=TACC_XALT_BIN MPISPAWN_GENERIC_VALUE_102="/opt/apps/xalt/0.4.4/bin" MPISPAWN_GENERIC_NAME_103=X509_USER_PROXY MPISPAWN_GENERIC_VALUE_103="/tmp/x509up_p3084.file9zM3fu.1" MPISPAWN_GENERIC_NAME_104=TMPDIR MPISPAWN_GENERIC_VALUE_104="/tmp" MPISPAWN_GENERIC_NAME_105=LMOD_FULL_SETTARG_SUPPORT MPISPAWN_GENERIC_VALUE_105="no" MPISPAWN_ID=45 MPISPAWN_WORKING_DIR=/home1/03222/tg825207/parallelnbody MPISPAWN_MPIRUN_RANK_0=720 MPISPAWN_MPIRUN_RANK_1=721 MPISPAWN_MPIRUN_RANK_2=722 MPISPAWN_MPIRUN_RANK_3=723 MPISPAWN_MPIRUN_RANK_4=724 MPISPAWN_MPIRUN_RANK_5=725 MPISPAWN_MPIRUN_RANK_6=726 MPISPAWN_MPIRUN_RANK_7=727 MPISPAWN_MPIRUN_RANK_8=728 MPISPAWN_MPIRUN_RANK_9=729 MPISPAWN_MPIRUN_RANK_10=730 MPISPAWN_MPIRUN_RANK_11=731 MPISPAWN_MPIRUN_RANK_12=732 MPISPAWN_MPIRUN_RANK_13=733 MPISPAWN_MPIRUN_RANK_14=734 MPISPAWN_MPIRUN_RANK_15=735 /opt/apps/gcc4_7/mvapich2/1.9/bin/mpispawn 0
[c484-701.stampede.tacc.utexas.edu:mpispawn_31][error_sighandler] Caught error: Segmentation fault (signal 11)
[c483-601.stampede.tacc.utexas.edu:mpispawn_23][error_sighandler] Caught error: Segmentation fault (signal 11)
bash: line 1: 15824 Segmentation fault      /usr/bin/env LD_LIBRARY_PATH=/opt/apps/gcc4_7/mvapich2/1.9/lib:/opt/apps/gcc4_7/mvapich2/1.9/lib/shared:/opt/apps/gcc/4.7.1/lib64:/opt/apps/gcc/4.7.1/lib:/opt/apps/xsede/gsi-openssh-5.7/lib64:/opt/apps/xsede/gsi-openssh-5.7/lib64 MPISPAWN_MPIRUN_MPD=0 USE_LINEAR_SSH=1 MPISPAWN_MPIRUN_HOST=c468-601.stampede.tacc.utexas.edu MPISPAWN_MPIRUN_HOSTIP=129.114.76.165 MPIRUN_RSH_LAUNCH=1 MPISPAWN_CHECKIN_PORT=56630 MPISPAWN_MPIRUN_PORT=56630 MPISPAWN_NNODES=128 MPISPAWN_GLOBAL_NPROCS=2048 MPISPAWN_MPIRUN_ID=47451 MPISPAWN_ARGC=4 MPDMAN_KVS_TEMPLATE=kvs_437_c468-601.stampede.tacc.utexas.edu_47451 MPISPAWN_LOCAL_NPROCS=16 MPISPAWN_ARGV_0='/home1/03222/tg825207/parallelnbody/teamscatter' MPISPAWN_ARGV_1='256000' MPISPAWN_ARGV_2='-c' MPISPAWN_ARGV_3='2' MPISPAWN_ARGC=4 MPISPAWN_GENERIC_ENV_COUNT=106 MPISPAWN_GENERIC_NAME_0=TACC_ENVLEN MPISPAWN_GENERIC_VALUE_0=6578 MPISPAWN_GENERIC_NAME_1=SLURM_JOB_ID MPISPAWN_GENERIC_VALUE_1="4306264" MPISPAWN_GENERIC_NAME_2=SLURMD_NODENAME MPISPAWN_GENERIC_VALUE_2="c468-601" MPISPAWN_GENERIC_NAME_3=STOCKYARD MPISPAWN_GENERIC_VALUE_3="/work/03222/tg825207" MPISPAWN_GENERIC_NAME_4=LC_ALL MPISPAWN_GENERIC_VALUE_4="en_US.UTF-8" MPISPAWN_GENERIC_NAME_5=SLURM_CHECKPOINT_IMAGE_DIR MPISPAWN_GENERIC_VALUE_5="/home1/03222/tg825207/parallelnbody" MPISPAWN_GENERIC_NAME_6=LMOD_FAMILY_MPI MPISPAWN_GENERIC_VALUE_6="mvapich2" MPISPAWN_GENERIC_NAME_7=SLURM_JOBID MPISPAWN_GENERIC_VALUE_7="4306264" MPISPAWN_GENERIC_NAME_8=_ModuleTable001_ MPISPAWN_GENERIC_VALUE_8="X01vZHVsZVRhYmxlXz17WyJhY3RpdmVTaXplIl09OCxiYXNlTXBhdGhBPXsiL29wdC9hcHBzL3hzZWRlL21vZHVsZWZpbGVzIiwiL29wdC9hcHBzL21vZHVsZWZpbGVzIiwiL29wdC9tb2R1bGVmaWxlcyIsfSxbImNfcmVidWlsZFRpbWUiXT1mYWxzZSxbImNfc2hvcnRUaW1lIl09ZmFsc2UsZmFtaWx5PXtbIk1QSSJdPSJtdmFwaWNoMiIsWyJjb21waWxlciJdPSJnY2MiLH0saW5hY3RpdmU9e30sbVQ9e0xpbnV4PXtbIkZOIl09Ii9vcHQvbW9kdWxlZmlsZXMvTGludXgiLFsiZGVmYXVsdCJdPTAsWyJmdWxsTmFtZSJdPSJMaW51eCIsWyJsb2FkT3JkZXIiXT0yLHByb3BUPXt9LFsic2hvcnQiXT0iTGludXgiLFsic3RhdHVzIl09ImFjdGl2ZSIsfSxUQUNDPXtbIkZOIl09Ii9v" MPISPAWN_GENERIC_NAME_9=TACC_SYSTEM MPISPAWN_GENERIC_VALUE_9="stampede" MPISPAWN_GENERIC_NAME_10=WORK MPISPAWN_GENERIC_VALUE_10="/work/03222/tg825207" MPISPAWN_GENERIC_NAME_11=SLURM_GTIDS MPISPAWN_GENERIC_VALUE_11="0" MPISPAWN_GENERIC_NAME_12=GCC_LIB MPISPAWN_GENERIC_VALUE_12="/opt/apps/gcc/4.7.1/lib64" MPISPAWN_GENERIC_NAME_13=SLURM_PRIO_PROCESS MPISPAWN_GENERIC_VALUE_13="0" MPISPAWN_GENERIC_NAME_14=SCRATCH MPISPAWN_GENERIC_VALUE_14="/scratch/03222/tg825207" MPISPAWN_GENERIC_NAME_15=LOGNAME MPISPAWN_GENERIC_VALUE_15="tg825207" MPISPAWN_GENERIC_NAME_16=_ModuleTable002_ MPISPAWN_GENERIC_VALUE_16="cHQvbW9kdWxlZmlsZXMvVEFDQyIsWyJkZWZhdWx0Il09MCxbImZ1bGxOYW1lIl09IlRBQ0MiLFsibG9hZE9yZGVyIl09Nixwcm9wVD17fSxbInNob3J0Il09IlRBQ0MiLFsic3RhdHVzIl09ImFjdGl2ZSIsfSxbIlRBQ0MtcGF0aHMiXT17WyJGTiJdPSIvb3B0L21vZHVsZWZpbGVzL1RBQ0MtcGF0aHMiLFsiZGVmYXVsdCJdPTAsWyJmdWxsTmFtZSJdPSJUQUNDLXBhdGhzIixbImxvYWRPcmRlciJdPTEscHJvcFQ9e30sWyJzaG9ydCJdPSJUQUNDLXBhdGhzIixbInN0YXR1cyJdPSJhY3RpdmUiLH0sY2x1c3Rlcj17WyJGTiJdPSIvb3B0L21vZHVsZWZpbGVzL2NsdXN0ZXIiLFsiZGVmYXVsdCJdPTAsWyJmdWxsTmFtZSJdPSJjbHVzdGVyIixbImxvYWRPcmRlciJdPTUscHJvcFQ9" MPISPAWN_GENERIC_NAME_17=INPUTRC MPISPAWN_GENERIC_VALUE_17="/etc/inputrc" MPISPAWN_GENERIC_NAME_18=LMOD_CMD MPISPAWN_GENERIC_VALUE_18="/opt/apps/lmod/lmod/libexec/lmod" MPISPAWN_GENERIC_NAME_19=I_MPI_OFA_ADAPTER_NAME MPISPAWN_GENERIC_VALUE_19="mlx4_0" MPISPAWN_GENERIC_NAME_20=QTLIB MPISPAWN_GENERIC_VALUE_20="/usr/lib64/qt-3.3/lib" MPISPAWN_GENERIC_NAME_21=NODE_TASKS_PPN_INFO MPISPAWN_GENERIC_VALUE_21=""16,0_"" MPISPAWN_GENERIC_NAME_22=SLURM_CPUS_ON_NODE MPISPAWN_GENERIC_VALUE_22="16" MPISPAWN_GENERIC_NAME_23=PATH MPISPAWN_GENERIC_VALUE_23="/opt/apps/gcc4_7/mvapich2/1.9/bin:/opt/apps/gcc/4.7.1/bin:/opt/apps/xalt/0.4.4/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/opt/apps/xsede/gsi-openssh-5.7/bin:/usr/X11R6/bin:/opt/ofed/bin:/opt/ofed/sbin:." MPISPAWN_GENERIC_NAME_24=SLURM_TACC_JOBNAME MPISPAWN_GENERIC_VALUE_24="stampedeLargeScaleAllTS.sh" MPISPAWN_GENERIC_NAME_25=SLURM_NODEID MPISPAWN_GENERIC_VALUE_25="0" MPISPAWN_GENERIC_NAME_26=LMOD_PKG MPISPAWN_GENERIC_VALUE_26="/opt/apps/lmod/lmod" MPISPAWN_GENERIC_NAME_27=MIC_I_MPI_OFA_ADAPTER_NAME MPISPAWN_GENERIC_VALUE_27="mlx4_0" MPISPAWN_GENERIC_NAME_28=HISTSIZE MPISPAWN_GENERIC_VALUE_28="1000" MPISPAWN_GENERIC_NAME_29=SLURM_NTASKS MPISPAWN_GENERIC_VALUE_29="2048" MPISPAWN_GENERIC_NAME_30=SHLIB_PATH MPISPAWN_GENERIC_VALUE_30="/opt/apps/xsede/gsi-openssh-5.7/lib64" MPISPAWN_GENERIC_NAME_31=TACC_OVERRIDE_PROJECT MPISPAWN_GENERIC_VALUE_31="TG-ASC140020" MPISPAWN_GENERIC_NAME_32=_ModuleTable005_ MPISPAWN_GENERIC_VALUE_32="Z2NjNF83L21vZHVsZWZpbGVzIiwiL29wdC9hcHBzL3hzZWRlL21vZHVsZWZpbGVzIiwiL29wdC9hcHBzL21vZHVsZWZpbGVzIiwiL29wdC9tb2R1bGVmaWxlcyIsfSxbInN5c3RlbUJhc2VNUEFUSCJdPSIvb3B0L2FwcHMveHNlZGUvbW9kdWxlZmlsZXM6L29wdC9hcHBzL21vZHVsZWZpbGVzOi9vcHQvbW9kdWxlZmlsZXMiLFsidmVyc2lvbiJdPTIsfQ==" MPISPAWN_GENERIC_NAME_33=_ModuleTable_Sz_ MPISPAWN_GENERIC_VALUE_33="5" MPISPAWN_GENERIC_NAME_34=MAIL MPISPAWN_GENERIC_VALUE_34="/var/spool/mail/tg825207" MPISPAWN_GENERIC_NAME_35=MV2_USE_HUGEPAGES MPISPAWN_GENERIC_VALUE_35="0" MPISPAWN_GENERIC_NAME_36=SLURM_TOPOLOGY_ADDR MPISPAWN_GENERIC_VALUE_36="c468-601" MPISPAWN_GENERIC_NAME_37=TACC_XALT_DIR MPISPAWN_GENERIC_VALUE_37="/opt/apps/xalt/0.4.4/" MPISPAWN_GENERIC_NAME_38=QTDIR MPISPAWN_GENERIC_VALUE_38="/usr/lib64/qt-3.3" MPISPAWN_GENERIC_NAME_39=SYSTEM MPISPAWN_GENERIC_VALUE_39="linux" MPISPAWN_GENERIC_NAME_40=SLURM_TACC_RUNLIMIT_MINS MPISPAWN_GENERIC_VALUE_40="180" MPISPAWN_GENERIC_NAME_41=SLURM_SUBMIT_DIR MPISPAWN_GENERIC_VALUE_41="/home1/03222/tg825207/parallelnbody" MPISPAWN_GENERIC_NAME_42=LIBPATH MPISPAWN_GENERIC_VALUE_42="/opt/apps/xsede/gsi-openssh-5.7/lib64:/usr/lib:/lib" MPISPAWN_GENERIC_NAME_43=HOSTNAME MPISPAWN_GENERIC_VALUE_43="c468-601" MPISPAWN_GENERIC_NAME_44=LMOD_DEFAULT_MODULEPATH MPISPAWN_GENERIC_VALUE_44="/opt/apps/xsede/modulefiles:/opt/apps/modulefiles:/opt/modulefiles" MPISPAWN_GENERIC_NAME_45=ARCHIVE MPISPAWN_GENERIC_VALUE_45="/home/03222/tg825207" MPISPAWN_GENERIC_NAME_46=_ MPISPAWN_GENERIC_VALUE_46="/usr/local/bin/build_env.pl" MPISPAWN_GENERIC_NAME_47=__BASHRC_SOURCED__ MPISPAWN_GENERIC_VALUE_47="1" MPISPAWN_GENERIC_NAME_48=APPS MPISPAWN_GENERIC_VALUE_48="/opt/apps" MPISPAWN_GENERIC_NAME_49=SHELL MPISPAWN_GENERIC_VALUE_49="/bin/bash" MPISPAWN_GENERIC_NAME_50=ARMCI_OPENIB_DEVICE MPISPAWN_GENERIC_VALUE_50="mlx4_0" MPISPAWN_GENERIC_NAME_51=ENVIRONMENT MPISPAWN_GENERIC_VALUE_51="BATCH" MPISPAWN_GENERIC_NAME_52=TACC_FAMILY_MPI MPISPAWN_GENERIC_VALUE_52="mvapich2" MPISPAWN_GENERIC_NAME_53=MV2_IBA_HCA MPISPAWN_GENERIC_VALUE_53="mlx4_0" MPISPAWN_GENERIC_NAME_54=TACC_PUBLIC_MACHINE MPISPAWN_GENERIC_VALUE_54="1" MPISPAWN_GENERIC_NAME_55=SLURM_TACC_NODES MPISPAWN_GENERIC_VALUE_55="1" MPISPAWN_GENERIC_NAME_56=_ModuleTable003_ MPISPAWN_GENERIC_VALUE_56="e30sWyJzaG9ydCJdPSJjbHVzdGVyIixbInN0YXR1cyJdPSJhY3RpdmUiLH0sWyJjbHVzdGVyLXBhdGhzIl09e1siRk4iXT0iL29wdC9tb2R1bGVmaWxlcy9jbHVzdGVyLXBhdGhzIixbImRlZmF1bHQiXT0wLFsiZnVsbE5hbWUiXT0iY2x1c3Rlci1wYXRocyIsWyJsb2FkT3JkZXIiXT0zLHByb3BUPXt9LFsic2hvcnQiXT0iY2x1c3Rlci1wYXRocyIsWyJzdGF0dXMiXT0iYWN0aXZlIix9LGdjYz17WyJGTiJdPSIvb3B0L2FwcHMvbW9kdWxlZmlsZXMvZ2NjLzQuNy4xLmx1YSIsWyJkZWZhdWx0Il09MCxbImZ1bGxOYW1lIl09ImdjYy80LjcuMSIsWyJsb2FkT3JkZXIiXT03LHByb3BUPXt9LFsic2hvcnQiXT0iZ2NjIixbInN0YXR1cyJdPSJhY3RpdmUiLH0sbXZhcGljaDI9e1si" MPISPAWN_GENERIC_NAME_57=QTINC MPISPAWN_GENERIC_VALUE_57="/usr/lib64/qt-3.3/include" MPISPAWN_GENERIC_NAME_58=CVS_RSH MPISPAWN_GENERIC_VALUE_58="ssh" MPISPAWN_GENERIC_NAME_59=MV2_USE_OLD_BCAST MPISPAWN_GENERIC_VALUE_59="1" MPISPAWN_GENERIC_NAME_60=SLURM_JOB_NUM_NODES MPISPAWN_GENERIC_VALUE_60="128" MPISPAWN_GENERIC_NAME_61=SLURM_TASK_PID MPISPAWN_GENERIC_VALUE_61="31479" MPISPAWN_GENERIC_NAME_62=TACC_MPI_GETMODE MPISPAWN_GENERIC_VALUE_62="mvapich2_ssh" MPISPAWN_GENERIC_NAME_63=LANG MPISPAWN_GENERIC_VALUE_63="en_US.UTF-8" MPISPAWN_GENERIC_NAME_64=USER MPISPAWN_GENERIC_VALUE_64="tg825207" MPISPAWN_GENERIC_NAME_65=MIC_I_MPI_DAPL_PROVIDER MPISPAWN_GENERIC_VALUE_65="ofa-v2-mlx4_0-1u" MPISPAWN_GENERIC_NAME_66=G_BROKEN_FILENAMES MPISPAWN_GENERIC_VALUE_66="1" MPISPAWN_GENERIC_NAME_67=SLURM_LOCALID MPISPAWN_GENERIC_VALUE_67="0" MPISPAWN_GENERIC_NAME_68=SLURM_NPROCS MPISPAWN_GENERIC_VALUE_68="2048" MPISPAWN_GENERIC_NAME_69=SLURM_TACC_NNODES_SET MPISPAWN_GENERIC_VALUE_69="0" MPISPAWN_GENERIC_NAME_70=SLURM_TACC_CORES MPISPAWN_GENERIC_VALUE_70="2048" MPISPAWN_GENERIC_NAME_71=LMOD_DIR MPISPAWN_GENERIC_VALUE_71="/opt/apps/lmod/lmod/libexec" MPISPAWN_GENERIC_NAME_72=LMOD_SETTARG_CMD MPISPAWN_GENERIC_VALUE_72=":" MPISPAWN_GENERIC_NAME_73=OMP_NUM_THREADS MPISPAWN_GENERIC_VALUE_73="1" MPISPAWN_GENERIC_NAME_74=_ModuleTable004_ MPISPAWN_GENERIC_VALUE_74="Rk4iXT0iL29wdC9hcHBzL2djYzRfNy9tb2R1bGVmaWxlcy9tdmFwaWNoMi8xLjlhMiIsWyJkZWZhdWx0Il09MSxbImZ1bGxOYW1lIl09Im12YXBpY2gyLzEuOWEyIixbImxvYWRPcmRlciJdPTgscHJvcFQ9e30sWyJzaG9ydCJdPSJtdmFwaWNoMiIsWyJzdGF0dXMiXT0iYWN0aXZlIix9LHhhbHQ9e1siRk4iXT0iL29wdC9hcHBzL21vZHVsZWZpbGVzL3hhbHQvMC40LjQubHVhIixbImRlZmF1bHQiXT0xLFsiZnVsbE5hbWUiXT0ieGFsdC8wLjQuNCIsWyJsb2FkT3JkZXIiXT00LHByb3BUPXt9LFsic2hvcnQiXT0ieGFsdCIsWyJzdGF0dXMiXT0iYWN0aXZlIix9LH0sbXBhdGhBPXsiL29wdC9hcHBzL2djYzRfNy9tdmFwaWNoMl8xXzkvbW9kdWxlZmlsZXMiLCIvb3B0L2FwcHMv" MPISPAWN_GENERIC_NAME_75=__PERSONAL_PATH__ MPISPAWN_GENERIC_VALUE_75="1" MPISPAWN_GENERIC_NAME_76=LMOD_SYSTEM_DEFAULT_MODULES MPISPAWN_GENERIC_VALUE_76="TACC" MPISPAWN_GENERIC_NAME_77=SLURM_JOB_NAME MPISPAWN_GENERIC_VALUE_77="stampedeLargeScaleAllTS.sh" MPISPAWN_GENERIC_NAME_78=SLURM_SUBMIT_HOST MPISPAWN_GENERIC_VALUE_78="login4.stampede.tacc.utexas.edu" MPISPAWN_GENERIC_NAME_79=LD_LIBRARY_PATH MPISPAWN_GENERIC_VALUE_79="/opt/apps/gcc4_7/mvapich2/1.9/lib:/opt/apps/gcc4_7/mvapich2/1.9/lib/shared:/opt/apps/gcc/4.7.1/lib64:/opt/apps/gcc/4.7.1/lib:/opt/apps/xsede/gsi-openssh-5.7/lib64:/opt/apps/xsede/gsi-openssh-5.7/lib64" MPISPAWN_GENERIC_NAME_80=SLURM_QUEUE MPISPAWN_GENERIC_VALUE_80="normal" MPISPAWN_GENERIC_NAME_81=PKG_CONFIG_PATH MPISPAWN_GENERIC_VALUE_81="/opt/apps/gcc4_7/mvapich2/1.9/lib/pkgconfig" MPISPAWN_GENERIC_NAME_82=LMOD_PREPEND_BLOCK MPISPAWN_GENERIC_VALUE_82="normal" MPISPAWN_GENERIC_NAME_83=TACC_FAMILY_COMPILER MPISPAWN_GENERIC_VALUE_83="gcc" MPISPAWN_GENERIC_NAME_84=TACC_DOMAIN MPISPAWN_GENERIC_VALUE_84="stampede" MPISPAWN_GENERIC_NAME_85=MV2_HOMOGENEOUS_CLUSTER MPISPAWN_GENERIC_VALUE_85="1" MPISPAWN_GENERIC_NAME_86=LMOD_FAMILY_COMPILER MPISPAWN_GENERIC_VALUE_86="gcc" MPISPAWN_GENERIC_NAME_87=MV2_USE_UD_HYBRID MPISPAWN_GENERIC_VALUE_87="0" MPISPAWN_GENERIC_NAME_88=MPICH_HOME MPISPAWN_GENERIC_VALUE_88="/opt/apps/gcc4_7/mvapich2/1.9" MPISPAWN_GENERIC_NAME_89=SLURM_JOB_NODELIST MPISPAWN_GENERIC_VALUE_89="c468-601,c471-503,c472-[401-402],c473-[303-304,401,804,901],c474-[203-204,301,401,403-404],c478-303,c482-404,c483-[001-004,101,504,601,804,901],c484-[101,103-104,201,602,701],c485-502,c486-102,c490-[302,904],c491-004,c492-701,c493-[001,102,201],c494-[101,402-403,602,702-703,904],c495-[001-002,201],c497-[102,202,504,601-603],c498-[304,402,603],c501-[204,301,604,801],c502-104,c503-901,c504-[602,604,702,801],c505-[001,202-204,401,502-504,802,901-904],c506-[302,704],c507-[102,202,503,601,904],c508-[101,204,601,702-703,802,903-904],c509-[002,201,301,303,401,501-502],c510-[003,401,404,501,503],c511-[601-602,902-903],c536-[801,902],c537-902,c538-[001,003,104],c540-[104,201-202,804,901-904]" MPISPAWN_GENERIC_NAME_90=SLURM_PROCID MPISPAWN_GENERIC_VALUE_90="0" MPISPAWN_GENERIC_NAME_91=I_MPI_DAPL_PROVIDER MPISPAWN_GENERIC_VALUE_91="ofa-v2-mlx4_0-1u" MPISPAWN_GENERIC_NAME_92=GLOBUS_LOCATION MPISPAWN_GENERIC_VALUE_92="/opt/apps/xsede/gsi-openssh-5.7" MPISPAWN_GENERIC_NAME_93=LMOD_COLORIZE MPISPAWN_GENERIC_VALUE_93="yes" MPISPAWN_GENERIC_NAME_94=I_MPI_HYDRA_BRANCH_COUNT MPISPAWN_GENERIC_VALUE_94="6000" MPISPAWN_GENERIC_NAME_95=SLURM_NNODES MPISPAWN_GENERIC_VALUE_95="128" MPISPAWN_GENERIC_NAME_96=MV2_DEFAULT_TIME_OUT MPISPAWN_GENERIC_VALUE_96="23" MPISPAWN_GENERIC_NAME_97=SLURM_TACC_NCORES_SET MPISPAWN_GENERIC_VALUE_97="1" MPISPAWN_GENERIC_NAME_98=__Init_Default_Modules MPISPAWN_GENERIC_VALUE_98="1" MPISPAWN_GENERIC_NAME_99=BASH_ENV MPISPAWN_GENERIC_VALUE_99="/etc/tacc/tacc_functions" MPISPAWN_GENERIC_NAME_100=SLURM_TOPOLOGY_ADDR_PATTERN MPISPAWN_GENERIC_VALUE_100="node" MPISPAWN_GENERIC_NAME_101=MV2_USE_RING_STARTUP MPISPAWN_GENERIC_VALUE_101="0" MPISPAWN_GENERIC_NAME_102=TACC_XALT_BIN MPISPAWN_GENERIC_VALUE_102="/opt/apps/xalt/0.4.4/bin" MPISPAWN_GENERIC_NAME_103=X509_USER_PROXY MPISPAWN_GENERIC_VALUE_103="/tmp/x509up_p3084.file9zM3fu.1" MPISPAWN_GENERIC_NAME_104=TMPDIR MPISPAWN_GENERIC_VALUE_104="/tmp" MPISPAWN_GENERIC_NAME_105=LMOD_FULL_SETTARG_SUPPORT MPISPAWN_GENERIC_VALUE_105="no" MPISPAWN_ID=23 MPISPAWN_WORKING_DIR=/home1/03222/tg825207/parallelnbody MPISPAWN_MPIRUN_RANK_0=368 MPISPAWN_MPIRUN_RANK_1=369 MPISPAWN_MPIRUN_RANK_2=370 MPISPAWN_MPIRUN_RANK_3=371 MPISPAWN_MPIRUN_RANK_4=372 MPISPAWN_MPIRUN_RANK_5=373 MPISPAWN_MPIRUN_RANK_6=374 MPISPAWN_MPIRUN_RANK_7=375 MPISPAWN_MPIRUN_RANK_8=376 MPISPAWN_MPIRUN_RANK_9=377 MPISPAWN_MPIRUN_RANK_10=378 MPISPAWN_MPIRUN_RANK_11=379 MPISPAWN_MPIRUN_RANK_12=380 MPISPAWN_MPIRUN_RANK_13=381 MPISPAWN_MPIRUN_RANK_14=382 MPISPAWN_MPIRUN_RANK_15=383 /opt/apps/gcc4_7/mvapich2/1.9/bin/mpispawn 0
bash: line 1: 42878 Segmentation fault      /usr/bin/env LD_LIBRARY_PATH=/opt/apps/gcc4_7/mvapich2/1.9/lib:/opt/apps/gcc4_7/mvapich2/1.9/lib/shared:/opt/apps/gcc/4.7.1/lib64:/opt/apps/gcc/4.7.1/lib:/opt/apps/xsede/gsi-openssh-5.7/lib64:/opt/apps/xsede/gsi-openssh-5.7/lib64 MPISPAWN_MPIRUN_MPD=0 USE_LINEAR_SSH=1 MPISPAWN_MPIRUN_HOST=c468-601.stampede.tacc.utexas.edu MPISPAWN_MPIRUN_HOSTIP=129.114.76.165 MPIRUN_RSH_LAUNCH=1 MPISPAWN_CHECKIN_PORT=56630 MPISPAWN_MPIRUN_PORT=56630 MPISPAWN_NNODES=128 MPISPAWN_GLOBAL_NPROCS=2048 MPISPAWN_MPIRUN_ID=47451 MPISPAWN_ARGC=4 MPDMAN_KVS_TEMPLATE=kvs_437_c468-601.stampede.tacc.utexas.edu_47451 MPISPAWN_LOCAL_NPROCS=16 MPISPAWN_ARGV_0='/home1/03222/tg825207/parallelnbody/teamscatter' MPISPAWN_ARGV_1='256000' MPISPAWN_ARGV_2='-c' MPISPAWN_ARGV_3='2' MPISPAWN_ARGC=4 MPISPAWN_GENERIC_ENV_COUNT=106 MPISPAWN_GENERIC_NAME_0=TACC_ENVLEN MPISPAWN_GENERIC_VALUE_0=6578 MPISPAWN_GENERIC_NAME_1=SLURM_JOB_ID MPISPAWN_GENERIC_VALUE_1="4306264" MPISPAWN_GENERIC_NAME_2=SLURMD_NODENAME MPISPAWN_GENERIC_VALUE_2="c468-601" MPISPAWN_GENERIC_NAME_3=STOCKYARD MPISPAWN_GENERIC_VALUE_3="/work/03222/tg825207" MPISPAWN_GENERIC_NAME_4=LC_ALL MPISPAWN_GENERIC_VALUE_4="en_US.UTF-8" MPISPAWN_GENERIC_NAME_5=SLURM_CHECKPOINT_IMAGE_DIR MPISPAWN_GENERIC_VALUE_5="/home1/03222/tg825207/parallelnbody" MPISPAWN_GENERIC_NAME_6=LMOD_FAMILY_MPI MPISPAWN_GENERIC_VALUE_6="mvapich2" MPISPAWN_GENERIC_NAME_7=SLURM_JOBID MPISPAWN_GENERIC_VALUE_7="4306264" MPISPAWN_GENERIC_NAME_8=_ModuleTable001_ MPISPAWN_GENERIC_VALUE_8="X01vZHVsZVRhYmxlXz17WyJhY3RpdmVTaXplIl09OCxiYXNlTXBhdGhBPXsiL29wdC9hcHBzL3hzZWRlL21vZHVsZWZpbGVzIiwiL29wdC9hcHBzL21vZHVsZWZpbGVzIiwiL29wdC9tb2R1bGVmaWxlcyIsfSxbImNfcmVidWlsZFRpbWUiXT1mYWxzZSxbImNfc2hvcnRUaW1lIl09ZmFsc2UsZmFtaWx5PXtbIk1QSSJdPSJtdmFwaWNoMiIsWyJjb21waWxlciJdPSJnY2MiLH0saW5hY3RpdmU9e30sbVQ9e0xpbnV4PXtbIkZOIl09Ii9vcHQvbW9kdWxlZmlsZXMvTGludXgiLFsiZGVmYXVsdCJdPTAsWyJmdWxsTmFtZSJdPSJMaW51eCIsWyJsb2FkT3JkZXIiXT0yLHByb3BUPXt9LFsic2hvcnQiXT0iTGludXgiLFsic3RhdHVzIl09ImFjdGl2ZSIsfSxUQUNDPXtbIkZOIl09Ii9v" MPISPAWN_GENERIC_NAME_9=TACC_SYSTEM MPISPAWN_GENERIC_VALUE_9="stampede" MPISPAWN_GENERIC_NAME_10=WORK MPISPAWN_GENERIC_VALUE_10="/work/03222/tg825207" MPISPAWN_GENERIC_NAME_11=SLURM_GTIDS MPISPAWN_GENERIC_VALUE_11="0" MPISPAWN_GENERIC_NAME_12=GCC_LIB MPISPAWN_GENERIC_VALUE_12="/opt/apps/gcc/4.7.1/lib64" MPISPAWN_GENERIC_NAME_13=SLURM_PRIO_PROCESS MPISPAWN_GENERIC_VALUE_13="0" MPISPAWN_GENERIC_NAME_14=SCRATCH MPISPAWN_GENERIC_VALUE_14="/scratch/03222/tg825207" MPISPAWN_GENERIC_NAME_15=LOGNAME MPISPAWN_GENERIC_VALUE_15="tg825207" MPISPAWN_GENERIC_NAME_16=_ModuleTable002_ MPISPAWN_GENERIC_VALUE_16="cHQvbW9kdWxlZmlsZXMvVEFDQyIsWyJkZWZhdWx0Il09MCxbImZ1bGxOYW1lIl09IlRBQ0MiLFsibG9hZE9yZGVyIl09Nixwcm9wVD17fSxbInNob3J0Il09IlRBQ0MiLFsic3RhdHVzIl09ImFjdGl2ZSIsfSxbIlRBQ0MtcGF0aHMiXT17WyJGTiJdPSIvb3B0L21vZHVsZWZpbGVzL1RBQ0MtcGF0aHMiLFsiZGVmYXVsdCJdPTAsWyJmdWxsTmFtZSJdPSJUQUNDLXBhdGhzIixbImxvYWRPcmRlciJdPTEscHJvcFQ9e30sWyJzaG9ydCJdPSJUQUNDLXBhdGhzIixbInN0YXR1cyJdPSJhY3RpdmUiLH0sY2x1c3Rlcj17WyJGTiJdPSIvb3B0L21vZHVsZWZpbGVzL2NsdXN0ZXIiLFsiZGVmYXVsdCJdPTAsWyJmdWxsTmFtZSJdPSJjbHVzdGVyIixbImxvYWRPcmRlciJdPTUscHJvcFQ9" MPISPAWN_GENERIC_NAME_17=INPUTRC MPISPAWN_GENERIC_VALUE_17="/etc/inputrc" MPISPAWN_GENERIC_NAME_18=LMOD_CMD MPISPAWN_GENERIC_VALUE_18="/opt/apps/lmod/lmod/libexec/lmod" MPISPAWN_GENERIC_NAME_19=I_MPI_OFA_ADAPTER_NAME MPISPAWN_GENERIC_VALUE_19="mlx4_0" MPISPAWN_GENERIC_NAME_20=QTLIB MPISPAWN_GENERIC_VALUE_20="/usr/lib64/qt-3.3/lib" MPISPAWN_GENERIC_NAME_21=NODE_TASKS_PPN_INFO MPISPAWN_GENERIC_VALUE_21=""16,0_"" MPISPAWN_GENERIC_NAME_22=SLURM_CPUS_ON_NODE MPISPAWN_GENERIC_VALUE_22="16" MPISPAWN_GENERIC_NAME_23=PATH MPISPAWN_GENERIC_VALUE_23="/opt/apps/gcc4_7/mvapich2/1.9/bin:/opt/apps/gcc/4.7.1/bin:/opt/apps/xalt/0.4.4/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/opt/apps/xsede/gsi-openssh-5.7/bin:/usr/X11R6/bin:/opt/ofed/bin:/opt/ofed/sbin:." MPISPAWN_GENERIC_NAME_24=SLURM_TACC_JOBNAME MPISPAWN_GENERIC_VALUE_24="stampedeLargeScaleAllTS.sh" MPISPAWN_GENERIC_NAME_25=SLURM_NODEID MPISPAWN_GENERIC_VALUE_25="0" MPISPAWN_GENERIC_NAME_26=LMOD_PKG MPISPAWN_GENERIC_VALUE_26="/opt/apps/lmod/lmod" MPISPAWN_GENERIC_NAME_27=MIC_I_MPI_OFA_ADAPTER_NAME MPISPAWN_GENERIC_VALUE_27="mlx4_0" MPISPAWN_GENERIC_NAME_28=HISTSIZE MPISPAWN_GENERIC_VALUE_28="1000" MPISPAWN_GENERIC_NAME_29=SLURM_NTASKS MPISPAWN_GENERIC_VALUE_29="2048" MPISPAWN_GENERIC_NAME_30=SHLIB_PATH MPISPAWN_GENERIC_VALUE_30="/opt/apps/xsede/gsi-openssh-5.7/lib64" MPISPAWN_GENERIC_NAME_31=TACC_OVERRIDE_PROJECT MPISPAWN_GENERIC_VALUE_31="TG-ASC140020" MPISPAWN_GENERIC_NAME_32=_ModuleTable005_ MPISPAWN_GENERIC_VALUE_32="Z2NjNF83L21vZHVsZWZpbGVzIiwiL29wdC9hcHBzL3hzZWRlL21vZHVsZWZpbGVzIiwiL29wdC9hcHBzL21vZHVsZWZpbGVzIiwiL29wdC9tb2R1bGVmaWxlcyIsfSxbInN5c3RlbUJhc2VNUEFUSCJdPSIvb3B0L2FwcHMveHNlZGUvbW9kdWxlZmlsZXM6L29wdC9hcHBzL21vZHVsZWZpbGVzOi9vcHQvbW9kdWxlZmlsZXMiLFsidmVyc2lvbiJdPTIsfQ==" MPISPAWN_GENERIC_NAME_33=_ModuleTable_Sz_ MPISPAWN_GENERIC_VALUE_33="5" MPISPAWN_GENERIC_NAME_34=MAIL MPISPAWN_GENERIC_VALUE_34="/var/spool/mail/tg825207" MPISPAWN_GENERIC_NAME_35=MV2_USE_HUGEPAGES MPISPAWN_GENERIC_VALUE_35="0" MPISPAWN_GENERIC_NAME_36=SLURM_TOPOLOGY_ADDR MPISPAWN_GENERIC_VALUE_36="c468-601" MPISPAWN_GENERIC_NAME_37=TACC_XALT_DIR MPISPAWN_GENERIC_VALUE_37="/opt/apps/xalt/0.4.4/" MPISPAWN_GENERIC_NAME_38=QTDIR MPISPAWN_GENERIC_VALUE_38="/usr/lib64/qt-3.3" MPISPAWN_GENERIC_NAME_39=SYSTEM MPISPAWN_GENERIC_VALUE_39="linux" MPISPAWN_GENERIC_NAME_40=SLURM_TACC_RUNLIMIT_MINS MPISPAWN_GENERIC_VALUE_40="180" MPISPAWN_GENERIC_NAME_41=SLURM_SUBMIT_DIR MPISPAWN_GENERIC_VALUE_41="/home1/03222/tg825207/parallelnbody" MPISPAWN_GENERIC_NAME_42=LIBPATH MPISPAWN_GENERIC_VALUE_42="/opt/apps/xsede/gsi-openssh-5.7/lib64:/usr/lib:/lib" MPISPAWN_GENERIC_NAME_43=HOSTNAME MPISPAWN_GENERIC_VALUE_43="c468-601" MPISPAWN_GENERIC_NAME_44=LMOD_DEFAULT_MODULEPATH MPISPAWN_GENERIC_VALUE_44="/opt/apps/xsede/modulefiles:/opt/apps/modulefiles:/opt/modulefiles" MPISPAWN_GENERIC_NAME_45=ARCHIVE MPISPAWN_GENERIC_VALUE_45="/home/03222/tg825207" MPISPAWN_GENERIC_NAME_46=_ MPISPAWN_GENERIC_VALUE_46="/usr/local/bin/build_env.pl" MPISPAWN_GENERIC_NAME_47=__BASHRC_SOURCED__ MPISPAWN_GENERIC_VALUE_47="1" MPISPAWN_GENERIC_NAME_48=APPS MPISPAWN_GENERIC_VALUE_48="/opt/apps" MPISPAWN_GENERIC_NAME_49=SHELL MPISPAWN_GENERIC_VALUE_49="/bin/bash" MPISPAWN_GENERIC_NAME_50=ARMCI_OPENIB_DEVICE MPISPAWN_GENERIC_VALUE_50="mlx4_0" MPISPAWN_GENERIC_NAME_51=ENVIRONMENT MPISPAWN_GENERIC_VALUE_51="BATCH" MPISPAWN_GENERIC_NAME_52=TACC_FAMILY_MPI MPISPAWN_GENERIC_VALUE_52="mvapich2" MPISPAWN_GENERIC_NAME_53=MV2_IBA_HCA MPISPAWN_GENERIC_VALUE_53="mlx4_0" MPISPAWN_GENERIC_NAME_54=TACC_PUBLIC_MACHINE MPISPAWN_GENERIC_VALUE_54="1" MPISPAWN_GENERIC_NAME_55=SLURM_TACC_NODES MPISPAWN_GENERIC_VALUE_55="1" MPISPAWN_GENERIC_NAME_56=_ModuleTable003_ MPISPAWN_GENERIC_VALUE_56="e30sWyJzaG9ydCJdPSJjbHVzdGVyIixbInN0YXR1cyJdPSJhY3RpdmUiLH0sWyJjbHVzdGVyLXBhdGhzIl09e1siRk4iXT0iL29wdC9tb2R1bGVmaWxlcy9jbHVzdGVyLXBhdGhzIixbImRlZmF1bHQiXT0wLFsiZnVsbE5hbWUiXT0iY2x1c3Rlci1wYXRocyIsWyJsb2FkT3JkZXIiXT0zLHByb3BUPXt9LFsic2hvcnQiXT0iY2x1c3Rlci1wYXRocyIsWyJzdGF0dXMiXT0iYWN0aXZlIix9LGdjYz17WyJGTiJdPSIvb3B0L2FwcHMvbW9kdWxlZmlsZXMvZ2NjLzQuNy4xLmx1YSIsWyJkZWZhdWx0Il09MCxbImZ1bGxOYW1lIl09ImdjYy80LjcuMSIsWyJsb2FkT3JkZXIiXT03LHByb3BUPXt9LFsic2hvcnQiXT0iZ2NjIixbInN0YXR1cyJdPSJhY3RpdmUiLH0sbXZhcGljaDI9e1si" MPISPAWN_GENERIC_NAME_57=QTINC MPISPAWN_GENERIC_VALUE_57="/usr/lib64/qt-3.3/include" MPISPAWN_GENERIC_NAME_58=CVS_RSH MPISPAWN_GENERIC_VALUE_58="ssh" MPISPAWN_GENERIC_NAME_59=MV2_USE_OLD_BCAST MPISPAWN_GENERIC_VALUE_59="1" MPISPAWN_GENERIC_NAME_60=SLURM_JOB_NUM_NODES MPISPAWN_GENERIC_VALUE_60="128" MPISPAWN_GENERIC_NAME_61=SLURM_TASK_PID MPISPAWN_GENERIC_VALUE_61="31479" MPISPAWN_GENERIC_NAME_62=TACC_MPI_GETMODE MPISPAWN_GENERIC_VALUE_62="mvapich2_ssh" MPISPAWN_GENERIC_NAME_63=LANG MPISPAWN_GENERIC_VALUE_63="en_US.UTF-8" MPISPAWN_GENERIC_NAME_64=USER MPISPAWN_GENERIC_VALUE_64="tg825207" MPISPAWN_GENERIC_NAME_65=MIC_I_MPI_DAPL_PROVIDER MPISPAWN_GENERIC_VALUE_65="ofa-v2-mlx4_0-1u" MPISPAWN_GENERIC_NAME_66=G_BROKEN_FILENAMES MPISPAWN_GENERIC_VALUE_66="1" MPISPAWN_GENERIC_NAME_67=SLURM_LOCALID MPISPAWN_GENERIC_VALUE_67="0" MPISPAWN_GENERIC_NAME_68=SLURM_NPROCS MPISPAWN_GENERIC_VALUE_68="2048" MPISPAWN_GENERIC_NAME_69=SLURM_TACC_NNODES_SET MPISPAWN_GENERIC_VALUE_69="0" MPISPAWN_GENERIC_NAME_70=SLURM_TACC_CORES MPISPAWN_GENERIC_VALUE_70="2048" MPISPAWN_GENERIC_NAME_71=LMOD_DIR MPISPAWN_GENERIC_VALUE_71="/opt/apps/lmod/lmod/libexec" MPISPAWN_GENERIC_NAME_72=LMOD_SETTARG_CMD MPISPAWN_GENERIC_VALUE_72=":" MPISPAWN_GENERIC_NAME_73=OMP_NUM_THREADS MPISPAWN_GENERIC_VALUE_73="1" MPISPAWN_GENERIC_NAME_74=_ModuleTable004_ MPISPAWN_GENERIC_VALUE_74="Rk4iXT0iL29wdC9hcHBzL2djYzRfNy9tb2R1bGVmaWxlcy9tdmFwaWNoMi8xLjlhMiIsWyJkZWZhdWx0Il09MSxbImZ1bGxOYW1lIl09Im12YXBpY2gyLzEuOWEyIixbImxvYWRPcmRlciJdPTgscHJvcFQ9e30sWyJzaG9ydCJdPSJtdmFwaWNoMiIsWyJzdGF0dXMiXT0iYWN0aXZlIix9LHhhbHQ9e1siRk4iXT0iL29wdC9hcHBzL21vZHVsZWZpbGVzL3hhbHQvMC40LjQubHVhIixbImRlZmF1bHQiXT0xLFsiZnVsbE5hbWUiXT0ieGFsdC8wLjQuNCIsWyJsb2FkT3JkZXIiXT00LHByb3BUPXt9LFsic2hvcnQiXT0ieGFsdCIsWyJzdGF0dXMiXT0iYWN0aXZlIix9LH0sbXBhdGhBPXsiL29wdC9hcHBzL2djYzRfNy9tdmFwaWNoMl8xXzkvbW9kdWxlZmlsZXMiLCIvb3B0L2FwcHMv" MPISPAWN_GENERIC_NAME_75=__PERSONAL_PATH__ MPISPAWN_GENERIC_VALUE_75="1" MPISPAWN_GENERIC_NAME_76=LMOD_SYSTEM_DEFAULT_MODULES MPISPAWN_GENERIC_VALUE_76="TACC" MPISPAWN_GENERIC_NAME_77=SLURM_JOB_NAME MPISPAWN_GENERIC_VALUE_77="stampedeLargeScaleAllTS.sh" MPISPAWN_GENERIC_NAME_78=SLURM_SUBMIT_HOST MPISPAWN_GENERIC_VALUE_78="login4.stampede.tacc.utexas.edu" MPISPAWN_GENERIC_NAME_79=LD_LIBRARY_PATH MPISPAWN_GENERIC_VALUE_79="/opt/apps/gcc4_7/mvapich2/1.9/lib:/opt/apps/gcc4_7/mvapich2/1.9/lib/shared:/opt/apps/gcc/4.7.1/lib64:/opt/apps/gcc/4.7.1/lib:/opt/apps/xsede/gsi-openssh-5.7/lib64:/opt/apps/xsede/gsi-openssh-5.7/lib64" MPISPAWN_GENERIC_NAME_80=SLURM_QUEUE MPISPAWN_GENERIC_VALUE_80="normal" MPISPAWN_GENERIC_NAME_81=PKG_CONFIG_PATH MPISPAWN_GENERIC_VALUE_81="/opt/apps/gcc4_7/mvapich2/1.9/lib/pkgconfig" MPISPAWN_GENERIC_NAME_82=LMOD_PREPEND_BLOCK MPISPAWN_GENERIC_VALUE_82="normal" MPISPAWN_GENERIC_NAME_83=TACC_FAMILY_COMPILER MPISPAWN_GENERIC_VALUE_83="gcc" MPISPAWN_GENERIC_NAME_84=TACC_DOMAIN MPISPAWN_GENERIC_VALUE_84="stampede" MPISPAWN_GENERIC_NAME_85=MV2_HOMOGENEOUS_CLUSTER MPISPAWN_GENERIC_VALUE_85="1" MPISPAWN_GENERIC_NAME_86=LMOD_FAMILY_COMPILER MPISPAWN_GENERIC_VALUE_86="gcc" MPISPAWN_GENERIC_NAME_87=MV2_USE_UD_HYBRID MPISPAWN_GENERIC_VALUE_87="0" MPISPAWN_GENERIC_NAME_88=MPICH_HOME MPISPAWN_GENERIC_VALUE_88="/opt/apps/gcc4_7/mvapich2/1.9" MPISPAWN_GENERIC_NAME_89=SLURM_JOB_NODELIST MPISPAWN_GENERIC_VALUE_89="c468-601,c471-503,c472-[401-402],c473-[303-304,401,804,901],c474-[203-204,301,401,403-404],c478-303,c482-404,c483-[001-004,101,504,601,804,901],c484-[101,103-104,201,602,701],c485-502,c486-102,c490-[302,904],c491-004,c492-701,c493-[001,102,201],c494-[101,402-403,602,702-703,904],c495-[001-002,201],c497-[102,202,504,601-603],c498-[304,402,603],c501-[204,301,604,801],c502-104,c503-901,c504-[602,604,702,801],c505-[001,202-204,401,502-504,802,901-904],c506-[302,704],c507-[102,202,503,601,904],c508-[101,204,601,702-703,802,903-904],c509-[002,201,301,303,401,501-502],c510-[003,401,404,501,503],c511-[601-602,902-903],c536-[801,902],c537-902,c538-[001,003,104],c540-[104,201-202,804,901-904]" MPISPAWN_GENERIC_NAME_90=SLURM_PROCID MPISPAWN_GENERIC_VALUE_90="0" MPISPAWN_GENERIC_NAME_91=I_MPI_DAPL_PROVIDER MPISPAWN_GENERIC_VALUE_91="ofa-v2-mlx4_0-1u" MPISPAWN_GENERIC_NAME_92=GLOBUS_LOCATION MPISPAWN_GENERIC_VALUE_92="/opt/apps/xsede/gsi-openssh-5.7" MPISPAWN_GENERIC_NAME_93=LMOD_COLORIZE MPISPAWN_GENERIC_VALUE_93="yes" MPISPAWN_GENERIC_NAME_94=I_MPI_HYDRA_BRANCH_COUNT MPISPAWN_GENERIC_VALUE_94="6000" MPISPAWN_GENERIC_NAME_95=SLURM_NNODES MPISPAWN_GENERIC_VALUE_95="128" MPISPAWN_GENERIC_NAME_96=MV2_DEFAULT_TIME_OUT MPISPAWN_GENERIC_VALUE_96="23" MPISPAWN_GENERIC_NAME_97=SLURM_TACC_NCORES_SET MPISPAWN_GENERIC_VALUE_97="1" MPISPAWN_GENERIC_NAME_98=__Init_Default_Modules MPISPAWN_GENERIC_VALUE_98="1" MPISPAWN_GENERIC_NAME_99=BASH_ENV MPISPAWN_GENERIC_VALUE_99="/etc/tacc/tacc_functions" MPISPAWN_GENERIC_NAME_100=SLURM_TOPOLOGY_ADDR_PATTERN MPISPAWN_GENERIC_VALUE_100="node" MPISPAWN_GENERIC_NAME_101=MV2_USE_RING_STARTUP MPISPAWN_GENERIC_VALUE_101="0" MPISPAWN_GENERIC_NAME_102=TACC_XALT_BIN MPISPAWN_GENERIC_VALUE_102="/opt/apps/xalt/0.4.4/bin" MPISPAWN_GENERIC_NAME_103=X509_USER_PROXY MPISPAWN_GENERIC_VALUE_103="/tmp/x509up_p3084.file9zM3fu.1" MPISPAWN_GENERIC_NAME_104=TMPDIR MPISPAWN_GENERIC_VALUE_104="/tmp" MPISPAWN_GENERIC_NAME_105=LMOD_FULL_SETTARG_SUPPORT MPISPAWN_GENERIC_VALUE_105="no" MPISPAWN_ID=31 MPISPAWN_WORKING_DIR=/home1/03222/tg825207/parallelnbody MPISPAWN_MPIRUN_RANK_0=496 MPISPAWN_MPIRUN_RANK_1=497 MPISPAWN_MPIRUN_RANK_2=498 MPISPAWN_MPIRUN_RANK_3=499 MPISPAWN_MPIRUN_RANK_4=500 MPISPAWN_MPIRUN_RANK_5=501 MPISPAWN_MPIRUN_RANK_6=502 MPISPAWN_MPIRUN_RANK_7=503 MPISPAWN_MPIRUN_RANK_8=504 MPISPAWN_MPIRUN_RANK_9=505 MPISPAWN_MPIRUN_RANK_10=506 MPISPAWN_MPIRUN_RANK_11=507 MPISPAWN_MPIRUN_RANK_12=508 MPISPAWN_MPIRUN_RANK_13=509 MPISPAWN_MPIRUN_RANK_14=510 MPISPAWN_MPIRUN_RANK_15=511 /opt/apps/gcc4_7/mvapich2/1.9/bin/mpispawn 0
[c491-004.stampede.tacc.utexas.edu:mpispawn_36][error_sighandler] Caught error: Segmentation fault (signal 11)
bash: line 1: 117223 Segmentation fault      /usr/bin/env LD_LIBRARY_PATH=/opt/apps/gcc4_7/mvapich2/1.9/lib:/opt/apps/gcc4_7/mvapich2/1.9/lib/shared:/opt/apps/gcc/4.7.1/lib64:/opt/apps/gcc/4.7.1/lib:/opt/apps/xsede/gsi-openssh-5.7/lib64:/opt/apps/xsede/gsi-openssh-5.7/lib64 MPISPAWN_MPIRUN_MPD=0 USE_LINEAR_SSH=1 MPISPAWN_MPIRUN_HOST=c468-601.stampede.tacc.utexas.edu MPISPAWN_MPIRUN_HOSTIP=129.114.76.165 MPIRUN_RSH_LAUNCH=1 MPISPAWN_CHECKIN_PORT=56630 MPISPAWN_MPIRUN_PORT=56630 MPISPAWN_NNODES=128 MPISPAWN_GLOBAL_NPROCS=2048 MPISPAWN_MPIRUN_ID=47451 MPISPAWN_ARGC=4 MPDMAN_KVS_TEMPLATE=kvs_437_c468-601.stampede.tacc.utexas.edu_47451 MPISPAWN_LOCAL_NPROCS=16 MPISPAWN_ARGV_0='/home1/03222/tg825207/parallelnbody/teamscatter' MPISPAWN_ARGV_1='256000' MPISPAWN_ARGV_2='-c' MPISPAWN_ARGV_3='2' MPISPAWN_ARGC=4 MPISPAWN_GENERIC_ENV_COUNT=106 MPISPAWN_GENERIC_NAME_0=TACC_ENVLEN MPISPAWN_GENERIC_VALUE_0=6578 MPISPAWN_GENERIC_NAME_1=SLURM_JOB_ID MPISPAWN_GENERIC_VALUE_1="4306264" MPISPAWN_GENERIC_NAME_2=SLURMD_NODENAME MPISPAWN_GENERIC_VALUE_2="c468-601" MPISPAWN_GENERIC_NAME_3=STOCKYARD MPISPAWN_GENERIC_VALUE_3="/work/03222/tg825207" MPISPAWN_GENERIC_NAME_4=LC_ALL MPISPAWN_GENERIC_VALUE_4="en_US.UTF-8" MPISPAWN_GENERIC_NAME_5=SLURM_CHECKPOINT_IMAGE_DIR MPISPAWN_GENERIC_VALUE_5="/home1/03222/tg825207/parallelnbody" MPISPAWN_GENERIC_NAME_6=LMOD_FAMILY_MPI MPISPAWN_GENERIC_VALUE_6="mvapich2" MPISPAWN_GENERIC_NAME_7=SLURM_JOBID MPISPAWN_GENERIC_VALUE_7="4306264" MPISPAWN_GENERIC_NAME_8=_ModuleTable001_ MPISPAWN_GENERIC_VALUE_8="X01vZHVsZVRhYmxlXz17WyJhY3RpdmVTaXplIl09OCxiYXNlTXBhdGhBPXsiL29wdC9hcHBzL3hzZWRlL21vZHVsZWZpbGVzIiwiL29wdC9hcHBzL21vZHVsZWZpbGVzIiwiL29wdC9tb2R1bGVmaWxlcyIsfSxbImNfcmVidWlsZFRpbWUiXT1mYWxzZSxbImNfc2hvcnRUaW1lIl09ZmFsc2UsZmFtaWx5PXtbIk1QSSJdPSJtdmFwaWNoMiIsWyJjb21waWxlciJdPSJnY2MiLH0saW5hY3RpdmU9e30sbVQ9e0xpbnV4PXtbIkZOIl09Ii9vcHQvbW9kdWxlZmlsZXMvTGludXgiLFsiZGVmYXVsdCJdPTAsWyJmdWxsTmFtZSJdPSJMaW51eCIsWyJsb2FkT3JkZXIiXT0yLHByb3BUPXt9LFsic2hvcnQiXT0iTGludXgiLFsic3RhdHVzIl09ImFjdGl2ZSIsfSxUQUNDPXtbIkZOIl09Ii9v" MPISPAWN_GENERIC_NAME_9=TACC_SYSTEM MPISPAWN_GENERIC_VALUE_9="stampede" MPISPAWN_GENERIC_NAME_10=WORK MPISPAWN_GENERIC_VALUE_10="/work/03222/tg825207" MPISPAWN_GENERIC_NAME_11=SLURM_GTIDS MPISPAWN_GENERIC_VALUE_11="0" MPISPAWN_GENERIC_NAME_12=GCC_LIB MPISPAWN_GENERIC_VALUE_12="/opt/apps/gcc/4.7.1/lib64" MPISPAWN_GENERIC_NAME_13=SLURM_PRIO_PROCESS MPISPAWN_GENERIC_VALUE_13="0" MPISPAWN_GENERIC_NAME_14=SCRATCH MPISPAWN_GENERIC_VALUE_14="/scratch/03222/tg825207" MPISPAWN_GENERIC_NAME_15=LOGNAME MPISPAWN_GENERIC_VALUE_15="tg825207" MPISPAWN_GENERIC_NAME_16=_ModuleTable002_ MPISPAWN_GENERIC_VALUE_16="cHQvbW9kdWxlZmlsZXMvVEFDQyIsWyJkZWZhdWx0Il09MCxbImZ1bGxOYW1lIl09IlRBQ0MiLFsibG9hZE9yZGVyIl09Nixwcm9wVD17fSxbInNob3J0Il09IlRBQ0MiLFsic3RhdHVzIl09ImFjdGl2ZSIsfSxbIlRBQ0MtcGF0aHMiXT17WyJGTiJdPSIvb3B0L21vZHVsZWZpbGVzL1RBQ0MtcGF0aHMiLFsiZGVmYXVsdCJdPTAsWyJmdWxsTmFtZSJdPSJUQUNDLXBhdGhzIixbImxvYWRPcmRlciJdPTEscHJvcFQ9e30sWyJzaG9ydCJdPSJUQUNDLXBhdGhzIixbInN0YXR1cyJdPSJhY3RpdmUiLH0sY2x1c3Rlcj17WyJGTiJdPSIvb3B0L21vZHVsZWZpbGVzL2NsdXN0ZXIiLFsiZGVmYXVsdCJdPTAsWyJmdWxsTmFtZSJdPSJjbHVzdGVyIixbImxvYWRPcmRlciJdPTUscHJvcFQ9" MPISPAWN_GENERIC_NAME_17=INPUTRC MPISPAWN_GENERIC_VALUE_17="/etc/inputrc" MPISPAWN_GENERIC_NAME_18=LMOD_CMD MPISPAWN_GENERIC_VALUE_18="/opt/apps/lmod/lmod/libexec/lmod" MPISPAWN_GENERIC_NAME_19=I_MPI_OFA_ADAPTER_NAME MPISPAWN_GENERIC_VALUE_19="mlx4_0" MPISPAWN_GENERIC_NAME_20=QTLIB MPISPAWN_GENERIC_VALUE_20="/usr/lib64/qt-3.3/lib" MPISPAWN_GENERIC_NAME_21=NODE_TASKS_PPN_INFO MPISPAWN_GENERIC_VALUE_21=""16,0_"" MPISPAWN_GENERIC_NAME_22=SLURM_CPUS_ON_NODE MPISPAWN_GENERIC_VALUE_22="16" MPISPAWN_GENERIC_NAME_23=PATH MPISPAWN_GENERIC_VALUE_23="/opt/apps/gcc4_7/mvapich2/1.9/bin:/opt/apps/gcc/4.7.1/bin:/opt/apps/xalt/0.4.4/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/opt/apps/xsede/gsi-openssh-5.7/bin:/usr/X11R6/bin:/opt/ofed/bin:/opt/ofed/sbin:." MPISPAWN_GENERIC_NAME_24=SLURM_TACC_JOBNAME MPISPAWN_GENERIC_VALUE_24="stampedeLargeScaleAllTS.sh" MPISPAWN_GENERIC_NAME_25=SLURM_NODEID MPISPAWN_GENERIC_VALUE_25="0" MPISPAWN_GENERIC_NAME_26=LMOD_PKG MPISPAWN_GENERIC_VALUE_26="/opt/apps/lmod/lmod" MPISPAWN_GENERIC_NAME_27=MIC_I_MPI_OFA_ADAPTER_NAME MPISPAWN_GENERIC_VALUE_27="mlx4_0" MPISPAWN_GENERIC_NAME_28=HISTSIZE MPISPAWN_GENERIC_VALUE_28="1000" MPISPAWN_GENERIC_NAME_29=SLURM_NTASKS MPISPAWN_GENERIC_VALUE_29="2048" MPISPAWN_GENERIC_NAME_30=SHLIB_PATH MPISPAWN_GENERIC_VALUE_30="/opt/apps/xsede/gsi-openssh-5.7/lib64" MPISPAWN_GENERIC_NAME_31=TACC_OVERRIDE_PROJECT MPISPAWN_GENERIC_VALUE_31="TG-ASC140020" MPISPAWN_GENERIC_NAME_32=_ModuleTable005_ MPISPAWN_GENERIC_VALUE_32="Z2NjNF83L21vZHVsZWZpbGVzIiwiL29wdC9hcHBzL3hzZWRlL21vZHVsZWZpbGVzIiwiL29wdC9hcHBzL21vZHVsZWZpbGVzIiwiL29wdC9tb2R1bGVmaWxlcyIsfSxbInN5c3RlbUJhc2VNUEFUSCJdPSIvb3B0L2FwcHMveHNlZGUvbW9kdWxlZmlsZXM6L29wdC9hcHBzL21vZHVsZWZpbGVzOi9vcHQvbW9kdWxlZmlsZXMiLFsidmVyc2lvbiJdPTIsfQ==" MPISPAWN_GENERIC_NAME_33=_ModuleTable_Sz_ MPISPAWN_GENERIC_VALUE_33="5" MPISPAWN_GENERIC_NAME_34=MAIL MPISPAWN_GENERIC_VALUE_34="/var/spool/mail/tg825207" MPISPAWN_GENERIC_NAME_35=MV2_USE_HUGEPAGES MPISPAWN_GENERIC_VALUE_35="0" MPISPAWN_GENERIC_NAME_36=SLURM_TOPOLOGY_ADDR MPISPAWN_GENERIC_VALUE_36="c468-601" MPISPAWN_GENERIC_NAME_37=TACC_XALT_DIR MPISPAWN_GENERIC_VALUE_37="/opt/apps/xalt/0.4.4/" MPISPAWN_GENERIC_NAME_38=QTDIR MPISPAWN_GENERIC_VALUE_38="/usr/lib64/qt-3.3" MPISPAWN_GENERIC_NAME_39=SYSTEM MPISPAWN_GENERIC_VALUE_39="linux" MPISPAWN_GENERIC_NAME_40=SLURM_TACC_RUNLIMIT_MINS MPISPAWN_GENERIC_VALUE_40="180" MPISPAWN_GENERIC_NAME_41=SLURM_SUBMIT_DIR MPISPAWN_GENERIC_VALUE_41="/home1/03222/tg825207/parallelnbody" MPISPAWN_GENERIC_NAME_42=LIBPATH MPISPAWN_GENERIC_VALUE_42="/opt/apps/xsede/gsi-openssh-5.7/lib64:/usr/lib:/lib" MPISPAWN_GENERIC_NAME_43=HOSTNAME MPISPAWN_GENERIC_VALUE_43="c468-601" MPISPAWN_GENERIC_NAME_44=LMOD_DEFAULT_MODULEPATH MPISPAWN_GENERIC_VALUE_44="/opt/apps/xsede/modulefiles:/opt/apps/modulefiles:/opt/modulefiles" MPISPAWN_GENERIC_NAME_45=ARCHIVE MPISPAWN_GENERIC_VALUE_45="/home/03222/tg825207" MPISPAWN_GENERIC_NAME_46=_ MPISPAWN_GENERIC_VALUE_46="/usr/local/bin/build_env.pl" MPISPAWN_GENERIC_NAME_47=__BASHRC_SOURCED__ MPISPAWN_GENERIC_VALUE_47="1" MPISPAWN_GENERIC_NAME_48=APPS MPISPAWN_GENERIC_VALUE_48="/opt/apps" MPISPAWN_GENERIC_NAME_49=SHELL MPISPAWN_GENERIC_VALUE_49="/bin/bash" MPISPAWN_GENERIC_NAME_50=ARMCI_OPENIB_DEVICE MPISPAWN_GENERIC_VALUE_50="mlx4_0" MPISPAWN_GENERIC_NAME_51=ENVIRONMENT MPISPAWN_GENERIC_VALUE_51="BATCH" MPISPAWN_GENERIC_NAME_52=TACC_FAMILY_MPI MPISPAWN_GENERIC_VALUE_52="mvapich2" MPISPAWN_GENERIC_NAME_53=MV2_IBA_HCA MPISPAWN_GENERIC_VALUE_53="mlx4_0" MPISPAWN_GENERIC_NAME_54=TACC_PUBLIC_MACHINE MPISPAWN_GENERIC_VALUE_54="1" MPISPAWN_GENERIC_NAME_55=SLURM_TACC_NODES MPISPAWN_GENERIC_VALUE_55="1" MPISPAWN_GENERIC_NAME_56=_ModuleTable003_ MPISPAWN_GENERIC_VALUE_56="e30sWyJzaG9ydCJdPSJjbHVzdGVyIixbInN0YXR1cyJdPSJhY3RpdmUiLH0sWyJjbHVzdGVyLXBhdGhzIl09e1siRk4iXT0iL29wdC9tb2R1bGVmaWxlcy9jbHVzdGVyLXBhdGhzIixbImRlZmF1bHQiXT0wLFsiZnVsbE5hbWUiXT0iY2x1c3Rlci1wYXRocyIsWyJsb2FkT3JkZXIiXT0zLHByb3BUPXt9LFsic2hvcnQiXT0iY2x1c3Rlci1wYXRocyIsWyJzdGF0dXMiXT0iYWN0aXZlIix9LGdjYz17WyJGTiJdPSIvb3B0L2FwcHMvbW9kdWxlZmlsZXMvZ2NjLzQuNy4xLmx1YSIsWyJkZWZhdWx0Il09MCxbImZ1bGxOYW1lIl09ImdjYy80LjcuMSIsWyJsb2FkT3JkZXIiXT03LHByb3BUPXt9LFsic2hvcnQiXT0iZ2NjIixbInN0YXR1cyJdPSJhY3RpdmUiLH0sbXZhcGljaDI9e1si" MPISPAWN_GENERIC_NAME_57=QTINC MPISPAWN_GENERIC_VALUE_57="/usr/lib64/qt-3.3/include" MPISPAWN_GENERIC_NAME_58=CVS_RSH MPISPAWN_GENERIC_VALUE_58="ssh" MPISPAWN_GENERIC_NAME_59=MV2_USE_OLD_BCAST MPISPAWN_GENERIC_VALUE_59="1" MPISPAWN_GENERIC_NAME_60=SLURM_JOB_NUM_NODES MPISPAWN_GENERIC_VALUE_60="128" MPISPAWN_GENERIC_NAME_61=SLURM_TASK_PID MPISPAWN_GENERIC_VALUE_61="31479" MPISPAWN_GENERIC_NAME_62=TACC_MPI_GETMODE MPISPAWN_GENERIC_VALUE_62="mvapich2_ssh" MPISPAWN_GENERIC_NAME_63=LANG MPISPAWN_GENERIC_VALUE_63="en_US.UTF-8" MPISPAWN_GENERIC_NAME_64=USER MPISPAWN_GENERIC_VALUE_64="tg825207" MPISPAWN_GENERIC_NAME_65=MIC_I_MPI_DAPL_PROVIDER MPISPAWN_GENERIC_VALUE_65="ofa-v2-mlx4_0-1u" MPISPAWN_GENERIC_NAME_66=G_BROKEN_FILENAMES MPISPAWN_GENERIC_VALUE_66="1" MPISPAWN_GENERIC_NAME_67=SLURM_LOCALID MPISPAWN_GENERIC_VALUE_67="0" MPISPAWN_GENERIC_NAME_68=SLURM_NPROCS MPISPAWN_GENERIC_VALUE_68="2048" MPISPAWN_GENERIC_NAME_69=SLURM_TACC_NNODES_SET MPISPAWN_GENERIC_VALUE_69="0" MPISPAWN_GENERIC_NAME_70=SLURM_TACC_CORES MPISPAWN_GENERIC_VALUE_70="2048" MPISPAWN_GENERIC_NAME_71=LMOD_DIR MPISPAWN_GENERIC_VALUE_71="/opt/apps/lmod/lmod/libexec" MPISPAWN_GENERIC_NAME_72=LMOD_SETTARG_CMD MPISPAWN_GENERIC_VALUE_72=":" MPISPAWN_GENERIC_NAME_73=OMP_NUM_THREADS MPISPAWN_GENERIC_VALUE_73="1" MPISPAWN_GENERIC_NAME_74=_ModuleTable004_ MPISPAWN_GENERIC_VALUE_74="Rk4iXT0iL29wdC9hcHBzL2djYzRfNy9tb2R1bGVmaWxlcy9tdmFwaWNoMi8xLjlhMiIsWyJkZWZhdWx0Il09MSxbImZ1bGxOYW1lIl09Im12YXBpY2gyLzEuOWEyIixbImxvYWRPcmRlciJdPTgscHJvcFQ9e30sWyJzaG9ydCJdPSJtdmFwaWNoMiIsWyJzdGF0dXMiXT0iYWN0aXZlIix9LHhhbHQ9e1siRk4iXT0iL29wdC9hcHBzL21vZHVsZWZpbGVzL3hhbHQvMC40LjQubHVhIixbImRlZmF1bHQiXT0xLFsiZnVsbE5hbWUiXT0ieGFsdC8wLjQuNCIsWyJsb2FkT3JkZXIiXT00LHByb3BUPXt9LFsic2hvcnQiXT0ieGFsdCIsWyJzdGF0dXMiXT0iYWN0aXZlIix9LH0sbXBhdGhBPXsiL29wdC9hcHBzL2djYzRfNy9tdmFwaWNoMl8xXzkvbW9kdWxlZmlsZXMiLCIvb3B0L2FwcHMv" MPISPAWN_GENERIC_NAME_75=__PERSONAL_PATH__ MPISPAWN_GENERIC_VALUE_75="1" MPISPAWN_GENERIC_NAME_76=LMOD_SYSTEM_DEFAULT_MODULES MPISPAWN_GENERIC_VALUE_76="TACC" MPISPAWN_GENERIC_NAME_77=SLURM_JOB_NAME MPISPAWN_GENERIC_VALUE_77="stampedeLargeScaleAllTS.sh" MPISPAWN_GENERIC_NAME_78=SLURM_SUBMIT_HOST MPISPAWN_GENERIC_VALUE_78="login4.stampede.tacc.utexas.edu" MPISPAWN_GENERIC_NAME_79=LD_LIBRARY_PATH MPISPAWN_GENERIC_VALUE_79="/opt/apps/gcc4_7/mvapich2/1.9/lib:/opt/apps/gcc4_7/mvapich2/1.9/lib/shared:/opt/apps/gcc/4.7.1/lib64:/opt/apps/gcc/4.7.1/lib:/opt/apps/xsede/gsi-openssh-5.7/lib64:/opt/apps/xsede/gsi-openssh-5.7/lib64" MPISPAWN_GENERIC_NAME_80=SLURM_QUEUE MPISPAWN_GENERIC_VALUE_80="normal" MPISPAWN_GENERIC_NAME_81=PKG_CONFIG_PATH MPISPAWN_GENERIC_VALUE_81="/opt/apps/gcc4_7/mvapich2/1.9/lib/pkgconfig" MPISPAWN_GENERIC_NAME_82=LMOD_PREPEND_BLOCK MPISPAWN_GENERIC_VALUE_82="normal" MPISPAWN_GENERIC_NAME_83=TACC_FAMILY_COMPILER MPISPAWN_GENERIC_VALUE_83="gcc" MPISPAWN_GENERIC_NAME_84=TACC_DOMAIN MPISPAWN_GENERIC_VALUE_84="stampede" MPISPAWN_GENERIC_NAME_85=MV2_HOMOGENEOUS_CLUSTER MPISPAWN_GENERIC_VALUE_85="1" MPISPAWN_GENERIC_NAME_86=LMOD_FAMILY_COMPILER MPISPAWN_GENERIC_VALUE_86="gcc" MPISPAWN_GENERIC_NAME_87=MV2_USE_UD_HYBRID MPISPAWN_GENERIC_VALUE_87="0" MPISPAWN_GENERIC_NAME_88=MPICH_HOME MPISPAWN_GENERIC_VALUE_88="/opt/apps/gcc4_7/mvapich2/1.9" MPISPAWN_GENERIC_NAME_89=SLURM_JOB_NODELIST MPISPAWN_GENERIC_VALUE_89="c468-601,c471-503,c472-[401-402],c473-[303-304,401,804,901],c474-[203-204,301,401,403-404],c478-303,c482-404,c483-[001-004,101,504,601,804,901],c484-[101,103-104,201,602,701],c485-502,c486-102,c490-[302,904],c491-004,c492-701,c493-[001,102,201],c494-[101,402-403,602,702-703,904],c495-[001-002,201],c497-[102,202,504,601-603],c498-[304,402,603],c501-[204,301,604,801],c502-104,c503-901,c504-[602,604,702,801],c505-[001,202-204,401,502-504,802,901-904],c506-[302,704],c507-[102,202,503,601,904],c508-[101,204,601,702-703,802,903-904],c509-[002,201,301,303,401,501-502],c510-[003,401,404,501,503],c511-[601-602,902-903],c536-[801,902],c537-902,c538-[001,003,104],c540-[104,201-202,804,901-904]" MPISPAWN_GENERIC_NAME_90=SLURM_PROCID MPISPAWN_GENERIC_VALUE_90="0" MPISPAWN_GENERIC_NAME_91=I_MPI_DAPL_PROVIDER MPISPAWN_GENERIC_VALUE_91="ofa-v2-mlx4_0-1u" MPISPAWN_GENERIC_NAME_92=GLOBUS_LOCATION MPISPAWN_GENERIC_VALUE_92="/opt/apps/xsede/gsi-openssh-5.7" MPISPAWN_GENERIC_NAME_93=LMOD_COLORIZE MPISPAWN_GENERIC_VALUE_93="yes" MPISPAWN_GENERIC_NAME_94=I_MPI_HYDRA_BRANCH_COUNT MPISPAWN_GENERIC_VALUE_94="6000" MPISPAWN_GENERIC_NAME_95=SLURM_NNODES MPISPAWN_GENERIC_VALUE_95="128" MPISPAWN_GENERIC_NAME_96=MV2_DEFAULT_TIME_OUT MPISPAWN_GENERIC_VALUE_96="23" MPISPAWN_GENERIC_NAME_97=SLURM_TACC_NCORES_SET MPISPAWN_GENERIC_VALUE_97="1" MPISPAWN_GENERIC_NAME_98=__Init_Default_Modules MPISPAWN_GENERIC_VALUE_98="1" MPISPAWN_GENERIC_NAME_99=BASH_ENV MPISPAWN_GENERIC_VALUE_99="/etc/tacc/tacc_functions" MPISPAWN_GENERIC_NAME_100=SLURM_TOPOLOGY_ADDR_PATTERN MPISPAWN_GENERIC_VALUE_100="node" MPISPAWN_GENERIC_NAME_101=MV2_USE_RING_STARTUP MPISPAWN_GENERIC_VALUE_101="0" MPISPAWN_GENERIC_NAME_102=TACC_XALT_BIN MPISPAWN_GENERIC_VALUE_102="/opt/apps/xalt/0.4.4/bin" MPISPAWN_GENERIC_NAME_103=X509_USER_PROXY MPISPAWN_GENERIC_VALUE_103="/tmp/x509up_p3084.file9zM3fu.1" MPISPAWN_GENERIC_NAME_104=TMPDIR MPISPAWN_GENERIC_VALUE_104="/tmp" MPISPAWN_GENERIC_NAME_105=LMOD_FULL_SETTARG_SUPPORT MPISPAWN_GENERIC_VALUE_105="no" MPISPAWN_ID=36 MPISPAWN_WORKING_DIR=/home1/03222/tg825207/parallelnbody MPISPAWN_MPIRUN_RANK_0=576 MPISPAWN_MPIRUN_RANK_1=577 MPISPAWN_MPIRUN_RANK_2=578 MPISPAWN_MPIRUN_RANK_3=579 MPISPAWN_MPIRUN_RANK_4=580 MPISPAWN_MPIRUN_RANK_5=581 MPISPAWN_MPIRUN_RANK_6=582 MPISPAWN_MPIRUN_RANK_7=583 MPISPAWN_MPIRUN_RANK_8=584 MPISPAWN_MPIRUN_RANK_9=585 MPISPAWN_MPIRUN_RANK_10=586 MPISPAWN_MPIRUN_RANK_11=587 MPISPAWN_MPIRUN_RANK_12=588 MPISPAWN_MPIRUN_RANK_13=589 MPISPAWN_MPIRUN_RANK_14=590 MPISPAWN_MPIRUN_RANK_15=591 /opt/apps/gcc4_7/mvapich2/1.9/bin/mpispawn 0
